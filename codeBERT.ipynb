{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOrVdRqPJe3xW1Va1+N/ydw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bye23mj/BERT/blob/main/codeBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(sys.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWbaizxZ7WOH",
        "outputId": "9e13031d-d4eb-42e6-e9f7-1adc2a5e3a34"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.7.15 (default, Oct 12 2022, 19:14:55) \n",
            "[GCC 7.5.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "# This is the TPU initialization code that has to be at the beginning.\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
        "\n",
        "strategy = tf.distribute.experimental.TPUStrategy(resolver)"
      ],
      "metadata": {
        "id": "JEpCvYAtaTsI",
        "outputId": "3db97d5c-09cf-43dc-9b23-866034af881e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All devices:  [LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:0', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:1', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:2', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:3', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:4', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:5', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:6', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cloud-tpu-client==0.10 torch==1.12.1 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-1.12-cp37-cp37m-linux_x86_64.whl"
      ],
      "metadata": {
        "id": "KdztFTwqokY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Scn3q9AuHT6_"
      },
      "outputs": [],
      "source": [
        "#!pip install torch==1.12.1\n",
        "!pip install transformers==4.23.1\n",
        "!pip install filelock"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown"
      ],
      "metadata": {
        "id": "jiurv6eYHYB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tree_sitter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubZSLi3jTHeC",
        "outputId": "eb9c3de9-4141-49b9-d86e-6d53b1c7a8c4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tree_sitter\n",
            "  Downloading tree_sitter-0.20.1.tar.gz (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 5.3 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: tree-sitter\n",
            "  Building wheel for tree-sitter (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tree-sitter: filename=tree_sitter-0.20.1-cp37-cp37m-linux_x86_64.whl size=329980 sha256=00297bb4c44d485de5a548e7ef3bcb9f6a2213d83a90ce2d22d9f3245247a398\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/a1/61/ec5131dac9344581d9e0364bc231721138ebd1318fba66982c\n",
            "Successfully built tree-sitter\n",
            "Installing collected packages: tree-sitter\n",
            "Successfully installed tree-sitter-0.20.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mkdir data data/code2nl"
      ],
      "metadata": {
        "id": "6Wz4GkSzHiIH"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mkdir data model/java"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFJzWGePOrns",
        "outputId": "a0490854-0d41-4be6-b4b3-1a7293c9b86e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n",
            "mkdir: cannot create directory ‘model/java’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cd data/code2nl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fP2RUj_IHkFH",
        "outputId": "9a7b79be-2764-49c2-9616-a7fc65381d05"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/data/code2nl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!gdown https://drive.google.com/uc?id=1rd2Tc6oUWBo7JouwexW3ksQ0PaOhUr6h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N30VlLnvHl1r",
        "outputId": "f70bf4af-4c71-4aa4-c563-a5ad33431cf9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1rd2Tc6oUWBo7JouwexW3ksQ0PaOhUr6h\n",
            "To: /content/data/code2nl/Cleaned_CodeSearchNet.zip\n",
            "100% 381M/381M [00:01<00:00, 191MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!unzip Cleaned_CodeSearchNet.zip"
      ],
      "metadata": {
        "id": "JWyv0lNuHql3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rm Cleaned_CodeSearchNet.zip"
      ],
      "metadata": {
        "id": "Bp1Z9PnOHs51"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cd ../.."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvUXWCeTHvy9",
        "outputId": "fe2c0f20-42fd-4347-c947-bb648d4c267c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/microsoft/CodeBERT.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPKrmo1oH122",
        "outputId": "cd7f567a-0db5-42d7-fef9-b3a88465104f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CodeBERT'...\n",
            "remote: Enumerating objects: 583, done.\u001b[K\n",
            "remote: Counting objects: 100% (97/97), done.\u001b[K\n",
            "remote: Compressing objects: 100% (55/55), done.\u001b[K\n",
            "remote: Total 583 (delta 45), reused 92 (delta 42), pack-reused 486\u001b[K\n",
            "Receiving objects: 100% (583/583), 49.56 MiB | 60.20 MiB/s, done.\n",
            "Resolving deltas: 100% (284/284), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/CodeBERT/GraphCodeBERT/codesearch"
      ],
      "metadata": {
        "id": "bety53O3JUgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip dataset.zip"
      ],
      "metadata": {
        "id": "Yqae--D9PQoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/CodeBERT/GraphCodeBERT/codesearch/dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZO5R25QiPVzd",
        "outputId": "d127c1be-f947-4f63-a869-0c20ab9bab8f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CodeBERT/GraphCodeBERT/codesearch/dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cd /content/CodeBERT/GraphCodeBERT/codesearch/dataset/java"
      ],
      "metadata": {
        "id": "4zHRP0bLX1jS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!bash run.sh "
      ],
      "metadata": {
        "id": "zxzoqvqbParA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/CodeBERT/GraphCodeBERT/codesearch/parser"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L09imSRsRpwg",
        "outputId": "dbbcea6f-afdb-4218-e238-55ed3ea7f92a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CodeBERT/GraphCodeBERT/codesearch/parser\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash build.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GipTwBl7R30w",
        "outputId": "4a88834a-e656-44eb-cf6d-e1bc72a374db"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'tree-sitter-go' already exists and is not an empty directory.\n",
            "fatal: destination path 'tree-sitter-javascript' already exists and is not an empty directory.\n",
            "fatal: destination path 'tree-sitter-python' already exists and is not an empty directory.\n",
            "fatal: destination path 'tree-sitter-ruby' already exists and is not an empty directory.\n",
            "fatal: destination path 'tree-sitter-php' already exists and is not an empty directory.\n",
            "fatal: destination path 'tree-sitter-java' already exists and is not an empty directory.\n",
            "fatal: destination path 'tree-sitter-c-sharp' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mkdir -p ./saved_models/java"
      ],
      "metadata": {
        "id": "cEiMbSg6dEHx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/CodeBERT/GraphCodeBERT/codesearch/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rfDCWoSeEIw",
        "outputId": "c61afed4-9482-4e16-82d4-1c4e3458e8bf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CodeBERT/GraphCodeBERT/codesearch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run.py \\\n",
        "    --output_dir=parser/saved_models/java \\\n",
        "    --config_name=microsoft/graphcodebert-base \\\n",
        "    --model_name_or_path=microsoft/graphcodebert-base \\\n",
        "    --tokenizer_name=microsoft/graphcodebert-base \\\n",
        "    --lang=java \\\n",
        "    --do_train \\\n",
        "    --train_data_file=dataset/java/train.jsonl \\\n",
        "    --eval_data_file=dataset/java/valid.jsonl \\\n",
        "    --test_data_file=dataset/java/test.jsonl \\\n",
        "    --codebase_file=dataset/java/codebase.jsonl \\\n",
        "    --num_train_epochs 10 \\\n",
        "    --code_length 256 \\\n",
        "    --data_flow_length 64 \\\n",
        "    --nl_length 128 \\\n",
        "    --train_batch_size 32 \\\n",
        "    --eval_batch_size 64 \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --seed 123456 2>&1| tee parser/saved_models/java/train.log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrY978GaH3xF",
        "outputId": "3d9944c5-8a9d-4bf9-bae3-2b9d89a651e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-01 04:21:14.658083: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n",
            "All devices:  [LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:0', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:1', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:2', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:3', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:4', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:5', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:6', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU')]\n",
            "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "device: %s, n_gpu: %s cpu\n",
            "***** Running training *****\n",
            "  Num examples = %d 164923\n",
            "  Num Epochs = %d 10\n",
            "  Instantaneous batch size per GPU = %d 32\n",
            "  Total train batch size  = %d 32\n",
            "  Total optimization steps = %d 51540\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "epoch 0 step 1 loss 3.32073\n",
            "epoch 0 step 2 loss 3.46876\n",
            "epoch 0 step 3 loss 8.23877\n",
            "epoch 0 step 4 loss 5.58799\n",
            "epoch 0 step 5 loss 6.54063\n",
            "epoch 0 step 6 loss 2.35405\n",
            "epoch 0 step 7 loss 2.91988\n",
            "epoch 0 step 8 loss 2.40846\n",
            "epoch 0 step 9 loss 2.49582\n",
            "epoch 0 step 10 loss 1.74544\n",
            "epoch 0 step 11 loss 1.4946\n",
            "epoch 0 step 12 loss 1.69941\n",
            "epoch 0 step 13 loss 1.12913\n",
            "epoch 0 step 14 loss 1.11747\n",
            "epoch 0 step 15 loss 1.12162\n",
            "epoch 0 step 16 loss 1.13341\n",
            "epoch 0 step 17 loss 0.3658\n",
            "epoch 0 step 18 loss 1.0491\n",
            "epoch 0 step 19 loss 0.7791\n",
            "epoch 0 step 20 loss 0.64126\n",
            "epoch 0 step 21 loss 0.48125\n",
            "epoch 0 step 22 loss 0.40639\n",
            "epoch 0 step 23 loss 0.65323\n",
            "epoch 0 step 24 loss 0.68587\n",
            "epoch 0 step 25 loss 0.19671\n",
            "epoch 0 step 26 loss 0.67213\n",
            "epoch 0 step 27 loss 0.39913\n",
            "epoch 0 step 28 loss 0.52526\n",
            "epoch 0 step 29 loss 0.50066\n",
            "epoch 0 step 30 loss 0.38619\n",
            "epoch 0 step 31 loss 0.28481\n",
            "epoch 0 step 32 loss 0.88455\n",
            "epoch 0 step 33 loss 0.49942\n",
            "epoch 0 step 34 loss 0.46377\n",
            "epoch 0 step 35 loss 0.38392\n",
            "epoch 0 step 36 loss 0.59465\n",
            "epoch 0 step 37 loss 0.35067\n",
            "epoch 0 step 38 loss 0.35267\n",
            "epoch 0 step 39 loss 0.06578\n",
            "epoch 0 step 40 loss 0.49286\n",
            "epoch 0 step 41 loss 0.37784\n",
            "epoch 0 step 42 loss 0.28431\n",
            "epoch 0 step 43 loss 0.28544\n",
            "epoch 0 step 44 loss 0.32927\n",
            "epoch 0 step 45 loss 0.24537\n",
            "epoch 0 step 46 loss 0.13117\n",
            "epoch 0 step 47 loss 0.05938\n",
            "epoch 0 step 48 loss 0.43124\n",
            "epoch 0 step 49 loss 0.26879\n",
            "epoch 0 step 50 loss 0.25798\n",
            "epoch 0 step 51 loss 0.33825\n",
            "epoch 0 step 52 loss 0.14261\n",
            "epoch 0 step 53 loss 0.15028\n",
            "epoch 0 step 54 loss 0.09377\n",
            "epoch 0 step 55 loss 0.01477\n",
            "epoch 0 step 56 loss 0.77054\n",
            "epoch 0 step 57 loss 0.00935\n",
            "epoch 0 step 58 loss 0.01793\n",
            "epoch 0 step 59 loss 0.31697\n",
            "epoch 0 step 60 loss 0.29663\n",
            "epoch 0 step 61 loss 0.41378\n",
            "epoch 0 step 62 loss 0.19994\n",
            "epoch 0 step 63 loss 0.20397\n",
            "epoch 0 step 64 loss 0.0872\n",
            "epoch 0 step 65 loss 0.66378\n",
            "epoch 0 step 66 loss 0.24731\n",
            "epoch 0 step 67 loss 0.02509\n",
            "epoch 0 step 68 loss 0.09762\n",
            "epoch 0 step 69 loss 0.59438\n",
            "epoch 0 step 70 loss 0.0303\n",
            "epoch 0 step 71 loss 0.59567\n",
            "epoch 0 step 72 loss 0.34463\n",
            "epoch 0 step 73 loss 0.26225\n",
            "epoch 0 step 74 loss 0.15318\n",
            "epoch 0 step 75 loss 0.51364\n",
            "epoch 0 step 76 loss 0.06303\n",
            "epoch 0 step 77 loss 0.00642\n",
            "epoch 0 step 78 loss 0.01047\n",
            "epoch 0 step 79 loss 0.38244\n",
            "epoch 0 step 80 loss 0.04643\n",
            "epoch 0 step 81 loss 0.01257\n",
            "epoch 0 step 82 loss 0.32313\n",
            "epoch 0 step 83 loss 0.12705\n",
            "epoch 0 step 84 loss 0.00891\n",
            "epoch 0 step 85 loss 0.22966\n",
            "epoch 0 step 86 loss 0.11207\n",
            "epoch 0 step 87 loss 0.70348\n",
            "epoch 0 step 88 loss 0.47064\n",
            "epoch 0 step 89 loss 0.11959\n",
            "epoch 0 step 90 loss 0.02693\n",
            "epoch 0 step 91 loss 0.78616\n",
            "epoch 0 step 92 loss 0.13416\n",
            "epoch 0 step 93 loss 0.02364\n",
            "epoch 0 step 94 loss 0.55638\n",
            "epoch 0 step 95 loss 0.02083\n",
            "epoch 0 step 96 loss 0.01427\n",
            "epoch 0 step 97 loss 0.08312\n",
            "epoch 0 step 98 loss 0.50455\n",
            "epoch 0 step 99 loss 0.36738\n",
            "epoch 0 step 100 loss 0.44607\n",
            "epoch 0 step 101 loss 0.35591\n",
            "epoch 0 step 102 loss 0.60091\n",
            "epoch 0 step 103 loss 0.00431\n",
            "epoch 0 step 104 loss 0.25292\n",
            "epoch 0 step 105 loss 0.60484\n",
            "epoch 0 step 106 loss 0.12023\n",
            "epoch 0 step 107 loss 0.1062\n",
            "epoch 0 step 108 loss 0.36475\n",
            "epoch 0 step 109 loss 0.22758\n",
            "epoch 0 step 110 loss 0.00168\n",
            "epoch 0 step 111 loss 0.00995\n",
            "epoch 0 step 112 loss 0.31265\n",
            "epoch 0 step 113 loss 0.00302\n",
            "epoch 0 step 114 loss 0.30212\n",
            "epoch 0 step 115 loss 0.16104\n",
            "epoch 0 step 116 loss 0.08691\n",
            "epoch 0 step 117 loss 0.16511\n",
            "epoch 0 step 118 loss 0.18043\n",
            "epoch 0 step 119 loss 0.04523\n",
            "epoch 0 step 120 loss 0.50013\n",
            "epoch 0 step 121 loss 0.33057\n",
            "epoch 0 step 122 loss 0.54773\n",
            "epoch 0 step 123 loss 0.20578\n",
            "epoch 0 step 124 loss 0.08829\n",
            "epoch 0 step 125 loss 0.1311\n",
            "epoch 0 step 126 loss 0.51666\n",
            "epoch 0 step 127 loss 0.25838\n",
            "epoch 0 step 128 loss 0.27446\n",
            "epoch 0 step 129 loss 0.60468\n",
            "epoch 0 step 130 loss 0.19714\n",
            "epoch 0 step 131 loss 0.13974\n",
            "epoch 0 step 132 loss 0.17836\n",
            "epoch 0 step 133 loss 0.12518\n",
            "epoch 0 step 134 loss 0.17267\n",
            "epoch 0 step 135 loss 0.0906\n",
            "epoch 0 step 136 loss 0.71407\n",
            "epoch 0 step 137 loss 0.39517\n",
            "epoch 0 step 138 loss 0.00173\n",
            "epoch 0 step 139 loss 0.04659\n",
            "epoch 0 step 140 loss 0.37705\n",
            "epoch 0 step 141 loss 0.1896\n",
            "epoch 0 step 142 loss 0.2765\n",
            "epoch 0 step 143 loss 0.15527\n",
            "epoch 0 step 144 loss 0.41927\n",
            "epoch 0 step 145 loss 0.13108\n",
            "epoch 0 step 146 loss 0.16275\n",
            "epoch 0 step 147 loss 0.41914\n",
            "epoch 0 step 148 loss 0.09049\n",
            "epoch 0 step 149 loss 0.34826\n",
            "epoch 0 step 150 loss 0.06788\n",
            "epoch 0 step 151 loss 0.18701\n",
            "epoch 0 step 152 loss 0.02037\n",
            "epoch 0 step 153 loss 0.76254\n",
            "epoch 0 step 154 loss 0.05361\n",
            "epoch 0 step 155 loss 0.03947\n",
            "epoch 0 step 156 loss 0.00849\n",
            "epoch 0 step 157 loss 0.17734\n",
            "epoch 0 step 158 loss 0.38309\n",
            "epoch 0 step 159 loss 0.67363\n",
            "epoch 0 step 160 loss 0.07726\n",
            "epoch 0 step 161 loss 0.25614\n",
            "epoch 0 step 162 loss 0.07358\n",
            "epoch 0 step 163 loss 0.21711\n",
            "epoch 0 step 164 loss 0.06011\n",
            "epoch 0 step 165 loss 0.45007\n",
            "epoch 0 step 166 loss 0.02659\n",
            "epoch 0 step 167 loss 0.02878\n",
            "epoch 0 step 168 loss 0.16705\n",
            "epoch 0 step 169 loss 0.00114\n",
            "epoch 0 step 170 loss 0.22742\n",
            "epoch 0 step 171 loss 0.16918\n",
            "epoch 0 step 172 loss 0.1624\n",
            "epoch 0 step 173 loss 0.15458\n",
            "epoch 0 step 174 loss 0.39323\n",
            "epoch 0 step 175 loss 0.42399\n",
            "epoch 0 step 176 loss 0.28015\n",
            "epoch 0 step 177 loss 0.17452\n",
            "epoch 0 step 178 loss 0.1483\n",
            "epoch 0 step 179 loss 0.35779\n",
            "epoch 0 step 180 loss 0.61579\n",
            "epoch 0 step 181 loss 0.00226\n",
            "epoch 0 step 182 loss 0.60448\n",
            "epoch 0 step 183 loss 0.10196\n",
            "epoch 0 step 184 loss 0.12472\n",
            "epoch 0 step 185 loss 0.01143\n",
            "epoch 0 step 186 loss 0.18736\n",
            "epoch 0 step 187 loss 0.14714\n",
            "epoch 0 step 188 loss 0.35777\n",
            "epoch 0 step 189 loss 0.11842\n",
            "epoch 0 step 190 loss 0.19715\n",
            "epoch 0 step 191 loss 0.16904\n",
            "epoch 0 step 192 loss 0.0399\n",
            "epoch 0 step 193 loss 0.01605\n",
            "epoch 0 step 194 loss 0.0704\n",
            "epoch 0 step 195 loss 0.04701\n",
            "epoch 0 step 196 loss 0.16548\n",
            "epoch 0 step 197 loss 0.08422\n",
            "epoch 0 step 198 loss 0.14441\n",
            "epoch 0 step 199 loss 0.26019\n",
            "epoch 0 step 200 loss 0.1296\n",
            "epoch 0 step 201 loss 0.09603\n",
            "epoch 0 step 202 loss 0.21364\n",
            "epoch 0 step 203 loss 0.06393\n",
            "epoch 0 step 204 loss 0.23496\n",
            "epoch 0 step 205 loss 0.03023\n",
            "epoch 0 step 206 loss 0.00794\n",
            "epoch 0 step 207 loss 0.43532\n",
            "epoch 0 step 208 loss 0.03909\n",
            "epoch 0 step 209 loss 0.00446\n",
            "epoch 0 step 210 loss 0.16856\n",
            "epoch 0 step 211 loss 0.36156\n",
            "epoch 0 step 212 loss 0.082\n",
            "epoch 0 step 213 loss 0.08604\n",
            "epoch 0 step 214 loss 0.04145\n",
            "epoch 0 step 215 loss 0.18674\n",
            "epoch 0 step 216 loss 0.05979\n",
            "epoch 0 step 217 loss 0.07935\n",
            "epoch 0 step 218 loss 0.12297\n",
            "epoch 0 step 219 loss 0.0602\n",
            "epoch 0 step 220 loss 0.47134\n",
            "epoch 0 step 221 loss 0.28889\n",
            "epoch 0 step 222 loss 0.14093\n",
            "epoch 0 step 223 loss 0.20525\n",
            "epoch 0 step 224 loss 0.23072\n",
            "epoch 0 step 225 loss 0.00433\n",
            "epoch 0 step 226 loss 0.27763\n",
            "epoch 0 step 227 loss 0.15682\n",
            "epoch 0 step 228 loss 0.09771\n",
            "epoch 0 step 229 loss 0.02163\n",
            "epoch 0 step 230 loss 0.15664\n",
            "epoch 0 step 231 loss 0.19561\n",
            "epoch 0 step 232 loss 0.02665\n",
            "epoch 0 step 233 loss 0.016\n",
            "epoch 0 step 234 loss 0.00136\n",
            "epoch 0 step 235 loss 0.07091\n",
            "epoch 0 step 236 loss 0.9239\n",
            "epoch 0 step 237 loss 0.35103\n",
            "epoch 0 step 238 loss 0.13639\n",
            "epoch 0 step 239 loss 0.15143\n",
            "epoch 0 step 240 loss 0.10008\n",
            "epoch 0 step 241 loss 0.06583\n",
            "epoch 0 step 242 loss 0.00267\n",
            "epoch 0 step 243 loss 0.11132\n",
            "epoch 0 step 244 loss 0.11769\n",
            "epoch 0 step 245 loss 0.00174\n",
            "epoch 0 step 246 loss 0.0486\n",
            "epoch 0 step 247 loss 0.13995\n",
            "epoch 0 step 248 loss 0.36015\n",
            "epoch 0 step 249 loss 0.19221\n",
            "epoch 0 step 250 loss 0.01848\n",
            "epoch 0 step 251 loss 0.39694\n",
            "epoch 0 step 252 loss 0.04163\n",
            "epoch 0 step 253 loss 0.00079\n",
            "epoch 0 step 254 loss 0.30682\n",
            "epoch 0 step 255 loss 0.00741\n",
            "epoch 0 step 256 loss 0.13614\n",
            "epoch 0 step 257 loss 0.06148\n",
            "epoch 0 step 258 loss 0.14107\n",
            "epoch 0 step 259 loss 0.14527\n",
            "epoch 0 step 260 loss 0.01041\n",
            "epoch 0 step 261 loss 0.10335\n",
            "epoch 0 step 262 loss 0.00243\n",
            "epoch 0 step 263 loss 0.72881\n",
            "epoch 0 step 264 loss 0.04711\n",
            "epoch 0 step 265 loss 0.00431\n",
            "epoch 0 step 266 loss 0.44353\n",
            "epoch 0 step 267 loss 0.03062\n",
            "epoch 0 step 268 loss 0.42282\n",
            "epoch 0 step 269 loss 0.25892\n",
            "epoch 0 step 270 loss 0.02243\n",
            "epoch 0 step 271 loss 0.04701\n",
            "epoch 0 step 272 loss 0.24235\n",
            "epoch 0 step 273 loss 0.08886\n",
            "epoch 0 step 274 loss 0.10061\n",
            "epoch 0 step 275 loss 0.22572\n",
            "epoch 0 step 276 loss 0.19907\n",
            "epoch 0 step 277 loss 0.03836\n",
            "epoch 0 step 278 loss 0.11794\n",
            "epoch 0 step 279 loss 0.14413\n",
            "epoch 0 step 280 loss 0.08659\n",
            "epoch 0 step 281 loss 0.00395\n",
            "epoch 0 step 282 loss 0.15037\n",
            "epoch 0 step 283 loss 0.01524\n",
            "epoch 0 step 284 loss 0.04173\n",
            "epoch 0 step 285 loss 0.00531\n",
            "epoch 0 step 286 loss 0.07887\n",
            "epoch 0 step 287 loss 0.38356\n",
            "epoch 0 step 288 loss 0.05526\n",
            "epoch 0 step 289 loss 0.22844\n",
            "epoch 0 step 290 loss 0.00464\n",
            "epoch 0 step 291 loss 0.03222\n",
            "epoch 0 step 292 loss 0.07743\n",
            "epoch 0 step 293 loss 0.15712\n",
            "epoch 0 step 294 loss 0.06643\n",
            "epoch 0 step 295 loss 0.34373\n",
            "epoch 0 step 296 loss 0.00462\n",
            "epoch 0 step 297 loss 0.17495\n",
            "epoch 0 step 298 loss 0.25662\n",
            "epoch 0 step 299 loss 0.01959\n",
            "epoch 0 step 300 loss 0.28795\n",
            "epoch 0 step 301 loss 0.10422\n",
            "epoch 0 step 302 loss 0.06082\n",
            "epoch 0 step 303 loss 0.0006\n",
            "epoch 0 step 304 loss 0.00111\n",
            "epoch 0 step 305 loss 0.00131\n",
            "epoch 0 step 306 loss 0.623\n",
            "epoch 0 step 307 loss 0.16824\n",
            "epoch 0 step 308 loss 0.00108\n",
            "epoch 0 step 309 loss 0.45405\n",
            "epoch 0 step 310 loss 0.35723\n",
            "epoch 0 step 311 loss 0.16084\n",
            "epoch 0 step 312 loss 0.18113\n",
            "epoch 0 step 313 loss 0.04914\n",
            "epoch 0 step 314 loss 0.00124\n",
            "epoch 0 step 315 loss 0.01287\n",
            "epoch 0 step 316 loss 0.58374\n",
            "epoch 0 step 317 loss 0.06275\n",
            "epoch 0 step 318 loss 0.23709\n",
            "epoch 0 step 319 loss 0.17757\n",
            "epoch 0 step 320 loss 0.00452\n",
            "epoch 0 step 321 loss 0.26847\n",
            "epoch 0 step 322 loss 0.26439\n",
            "epoch 0 step 323 loss 0.20724\n",
            "epoch 0 step 324 loss 0.03299\n",
            "epoch 0 step 325 loss 0.09881\n",
            "epoch 0 step 326 loss 0.04356\n",
            "epoch 0 step 327 loss 0.28095\n",
            "epoch 0 step 328 loss 0.39502\n",
            "epoch 0 step 329 loss 0.00088\n",
            "epoch 0 step 330 loss 0.2403\n",
            "epoch 0 step 331 loss 0.14828\n",
            "epoch 0 step 332 loss 0.01916\n",
            "epoch 0 step 333 loss 0.01085\n",
            "epoch 0 step 334 loss 0.24559\n",
            "epoch 0 step 335 loss 0.4212\n",
            "epoch 0 step 336 loss 0.08727\n",
            "epoch 0 step 337 loss 0.0036\n",
            "epoch 0 step 338 loss 0.04732\n",
            "epoch 0 step 339 loss 0.19074\n",
            "epoch 0 step 340 loss 0.37951\n",
            "epoch 0 step 341 loss 0.43888\n",
            "epoch 0 step 342 loss 0.32906\n",
            "epoch 0 step 343 loss 0.15143\n",
            "epoch 0 step 344 loss 0.06504\n",
            "epoch 0 step 345 loss 4e-05\n",
            "epoch 0 step 346 loss 0.18822\n",
            "epoch 0 step 347 loss 0.20928\n",
            "epoch 0 step 348 loss 0.44892\n",
            "epoch 0 step 349 loss 0.025\n",
            "epoch 0 step 350 loss 0.09164\n",
            "epoch 0 step 351 loss 0.0807\n",
            "epoch 0 step 352 loss 0.00064\n",
            "epoch 0 step 353 loss 0.00955\n",
            "epoch 0 step 354 loss 0.22242\n",
            "epoch 0 step 355 loss 0.23435\n",
            "epoch 0 step 356 loss 0.26894\n",
            "epoch 0 step 357 loss 0.00036\n",
            "epoch 0 step 358 loss 0.28765\n",
            "epoch 0 step 359 loss 0.10766\n",
            "epoch 0 step 360 loss 0.02167\n",
            "epoch 0 step 361 loss 0.53915\n",
            "epoch 0 step 362 loss 0.08939\n",
            "epoch 0 step 363 loss 0.004\n",
            "epoch 0 step 364 loss 0.82676\n",
            "epoch 0 step 365 loss 0.20736\n",
            "epoch 0 step 366 loss 0.07306\n",
            "epoch 0 step 367 loss 0.31801\n",
            "epoch 0 step 368 loss 0.20862\n",
            "epoch 0 step 369 loss 0.04863\n",
            "epoch 0 step 370 loss 0.32555\n",
            "epoch 0 step 371 loss 0.0913\n",
            "epoch 0 step 372 loss 0.03922\n",
            "epoch 0 step 373 loss 0.03953\n",
            "epoch 0 step 374 loss 0.12346\n",
            "epoch 0 step 375 loss 0.00534\n",
            "epoch 0 step 376 loss 0.11254\n",
            "epoch 0 step 377 loss 0.65402\n",
            "epoch 0 step 378 loss 0.15652\n",
            "epoch 0 step 379 loss 0.0163\n",
            "epoch 0 step 380 loss 0.00023\n",
            "epoch 0 step 381 loss 0.05327\n",
            "epoch 0 step 382 loss 0.32741\n",
            "epoch 0 step 383 loss 0.26496\n",
            "epoch 0 step 384 loss 0.00199\n",
            "epoch 0 step 385 loss 0.3024\n",
            "epoch 0 step 386 loss 0.26519\n",
            "epoch 0 step 387 loss 0.1865\n",
            "epoch 0 step 388 loss 0.00315\n",
            "epoch 0 step 389 loss 0.37008\n",
            "epoch 0 step 390 loss 0.01095\n",
            "epoch 0 step 391 loss 0.00084\n",
            "epoch 0 step 392 loss 0.06429\n",
            "epoch 0 step 393 loss 0.0568\n",
            "epoch 0 step 394 loss 0.10522\n",
            "epoch 0 step 395 loss 0.05631\n",
            "epoch 0 step 396 loss 0.00922\n",
            "epoch 0 step 397 loss 0.06228\n",
            "epoch 0 step 398 loss 0.42339\n",
            "epoch 0 step 399 loss 0.06445\n",
            "epoch 0 step 400 loss 0.00072\n",
            "epoch 0 step 401 loss 0.13423\n",
            "epoch 0 step 402 loss 0.05504\n",
            "epoch 0 step 403 loss 0.33878\n",
            "epoch 0 step 404 loss 0.36417\n",
            "epoch 0 step 405 loss 0.00313\n",
            "epoch 0 step 406 loss 0.02495\n",
            "epoch 0 step 407 loss 0.68072\n",
            "epoch 0 step 408 loss 0.43142\n",
            "epoch 0 step 409 loss 0.15922\n",
            "epoch 0 step 410 loss 0.4986\n",
            "epoch 0 step 411 loss 0.07654\n",
            "epoch 0 step 412 loss 0.05731\n",
            "epoch 0 step 413 loss 0.44277\n",
            "epoch 0 step 414 loss 0.46523\n",
            "epoch 0 step 415 loss 0.02559\n",
            "epoch 0 step 416 loss 0.11948\n",
            "epoch 0 step 417 loss 0.1221\n",
            "epoch 0 step 418 loss 0.0604\n",
            "epoch 0 step 419 loss 0.17534\n",
            "epoch 0 step 420 loss 0.01952\n",
            "epoch 0 step 421 loss 0.23001\n",
            "epoch 0 step 422 loss 0.06209\n",
            "epoch 0 step 423 loss 0.00447\n",
            "epoch 0 step 424 loss 0.40796\n",
            "epoch 0 step 425 loss 0.00805\n",
            "epoch 0 step 426 loss 0.00166\n",
            "epoch 0 step 427 loss 0.0014\n",
            "epoch 0 step 428 loss 0.04933\n",
            "epoch 0 step 429 loss 0.22311\n",
            "epoch 0 step 430 loss 0.01668\n",
            "epoch 0 step 431 loss 0.19116\n",
            "epoch 0 step 432 loss 0.03303\n",
            "epoch 0 step 433 loss 0.37759\n",
            "epoch 0 step 434 loss 0.21484\n",
            "epoch 0 step 435 loss 0.00059\n",
            "epoch 0 step 436 loss 0.30318\n",
            "epoch 0 step 437 loss 0.08303\n",
            "epoch 0 step 438 loss 0.03742\n",
            "epoch 0 step 439 loss 0.20735\n",
            "epoch 0 step 440 loss 0.00301\n",
            "epoch 0 step 441 loss 0.14096\n",
            "epoch 0 step 442 loss 0.02857\n",
            "epoch 0 step 443 loss 0.00026\n",
            "epoch 0 step 444 loss 0.04814\n",
            "epoch 0 step 445 loss 0.26817\n",
            "epoch 0 step 446 loss 0.1027\n",
            "epoch 0 step 447 loss 0.01971\n",
            "epoch 0 step 448 loss 0.03499\n",
            "epoch 0 step 449 loss 0.04386\n",
            "epoch 0 step 450 loss 0.11809\n",
            "epoch 0 step 451 loss 0.03056\n",
            "epoch 0 step 452 loss 0.07206\n",
            "epoch 0 step 453 loss 0.01229\n",
            "epoch 0 step 454 loss 0.14774\n",
            "epoch 0 step 455 loss 0.0162\n",
            "epoch 0 step 456 loss 0.15311\n",
            "epoch 0 step 457 loss 0.04556\n",
            "epoch 0 step 458 loss 0.02007\n",
            "epoch 0 step 459 loss 0.13953\n",
            "epoch 0 step 460 loss 0.70445\n",
            "epoch 0 step 461 loss 0.00045\n",
            "epoch 0 step 462 loss 0.03125\n",
            "epoch 0 step 463 loss 0.16209\n",
            "epoch 0 step 464 loss 0.00269\n",
            "epoch 0 step 465 loss 0.07318\n",
            "epoch 0 step 466 loss 0.15671\n",
            "epoch 0 step 467 loss 0.17259\n",
            "epoch 0 step 468 loss 0.44409\n",
            "epoch 0 step 469 loss 0.05808\n",
            "epoch 0 step 470 loss 0.11165\n",
            "epoch 0 step 471 loss 0.33847\n",
            "epoch 0 step 472 loss 0.40194\n",
            "epoch 0 step 473 loss 0.16251\n",
            "epoch 0 step 474 loss 0.17144\n",
            "epoch 0 step 475 loss 0.03472\n",
            "epoch 0 step 476 loss 0.00139\n",
            "epoch 0 step 477 loss 0.03727\n",
            "epoch 0 step 478 loss 0.11007\n",
            "epoch 0 step 479 loss 0.46621\n",
            "epoch 0 step 480 loss 0.15861\n",
            "epoch 0 step 481 loss 0.01138\n",
            "epoch 0 step 482 loss 0.29513\n",
            "epoch 0 step 483 loss 0.2924\n",
            "epoch 0 step 484 loss 0.23938\n",
            "epoch 0 step 485 loss 0.06845\n",
            "epoch 0 step 486 loss 0.29701\n",
            "epoch 0 step 487 loss 0.28986\n",
            "epoch 0 step 488 loss 0.05133\n",
            "epoch 0 step 489 loss 0.09339\n",
            "epoch 0 step 490 loss 0.21787\n",
            "epoch 0 step 491 loss 0.14597\n",
            "epoch 0 step 492 loss 0.00958\n",
            "epoch 0 step 493 loss 0.25254\n",
            "epoch 0 step 494 loss 0.30733\n",
            "epoch 0 step 495 loss 0.06295\n",
            "epoch 0 step 496 loss 0.08742\n",
            "epoch 0 step 497 loss 0.17197\n",
            "epoch 0 step 498 loss 0.00027\n",
            "epoch 0 step 499 loss 0.07432\n",
            "epoch 0 step 500 loss 0.03946\n",
            "epoch 0 step 501 loss 0.01663\n",
            "epoch 0 step 502 loss 0.19368\n",
            "epoch 0 step 503 loss 0.02975\n",
            "epoch 0 step 504 loss 0.0047\n",
            "epoch 0 step 505 loss 0.05375\n",
            "epoch 0 step 506 loss 0.47397\n",
            "epoch 0 step 507 loss 0.08821\n",
            "epoch 0 step 508 loss 0.25704\n",
            "epoch 0 step 509 loss 0.08867\n",
            "epoch 0 step 510 loss 0.70523\n",
            "epoch 0 step 511 loss 0.02072\n",
            "epoch 0 step 512 loss 0.1509\n",
            "epoch 0 step 513 loss 0.00834\n",
            "epoch 0 step 514 loss 0.1214\n",
            "epoch 0 step 515 loss 0.10789\n",
            "epoch 0 step 516 loss 0.15441\n",
            "epoch 0 step 517 loss 0.15959\n",
            "epoch 0 step 518 loss 0.00748\n",
            "epoch 0 step 519 loss 0.0207\n",
            "epoch 0 step 520 loss 0.03437\n",
            "epoch 0 step 521 loss 0.13879\n",
            "epoch 0 step 522 loss 0.12787\n",
            "epoch 0 step 523 loss 0.00068\n",
            "epoch 0 step 524 loss 0.07983\n",
            "epoch 0 step 525 loss 0.39552\n",
            "epoch 0 step 526 loss 0.30923\n",
            "epoch 0 step 527 loss 0.00906\n",
            "epoch 0 step 528 loss 0.0003\n",
            "epoch 0 step 529 loss 0.10462\n",
            "epoch 0 step 530 loss 0.01117\n",
            "epoch 0 step 531 loss 0.04704\n",
            "epoch 0 step 532 loss 0.05375\n",
            "epoch 0 step 533 loss 0.28155\n",
            "epoch 0 step 534 loss 0.25581\n",
            "epoch 0 step 535 loss 0.09613\n",
            "epoch 0 step 536 loss 0.00107\n",
            "epoch 0 step 537 loss 0.23023\n",
            "epoch 0 step 538 loss 0.12697\n",
            "epoch 0 step 539 loss 0.00281\n",
            "epoch 0 step 540 loss 0.00094\n",
            "epoch 0 step 541 loss 0.06905\n",
            "epoch 0 step 542 loss 0.21462\n",
            "epoch 0 step 543 loss 0.05304\n",
            "epoch 0 step 544 loss 0.44165\n",
            "epoch 0 step 545 loss 0.00539\n",
            "epoch 0 step 546 loss 0.01631\n",
            "epoch 0 step 547 loss 0.21804\n",
            "epoch 0 step 548 loss 0.30003\n",
            "epoch 0 step 549 loss 0.00922\n",
            "epoch 0 step 550 loss 0.00957\n",
            "epoch 0 step 551 loss 0.12629\n",
            "epoch 0 step 552 loss 0.17937\n",
            "epoch 0 step 553 loss 0.05597\n",
            "epoch 0 step 554 loss 0.17562\n",
            "epoch 0 step 555 loss 0.06342\n",
            "epoch 0 step 556 loss 0.14953\n",
            "epoch 0 step 557 loss 0.00081\n",
            "epoch 0 step 558 loss 0.01562\n",
            "epoch 0 step 559 loss 0.00457\n",
            "epoch 0 step 560 loss 0.33459\n",
            "epoch 0 step 561 loss 0.05155\n",
            "epoch 0 step 562 loss 0.1236\n",
            "epoch 0 step 563 loss 0.2108\n",
            "epoch 0 step 564 loss 0.29577\n",
            "epoch 0 step 565 loss 0.02243\n",
            "epoch 0 step 566 loss 0.25022\n",
            "epoch 0 step 567 loss 0.00875\n",
            "epoch 0 step 568 loss 0.00027\n",
            "epoch 0 step 569 loss 0.0036\n",
            "epoch 0 step 570 loss 0.26909\n",
            "epoch 0 step 571 loss 0.64238\n",
            "epoch 0 step 572 loss 0.34662\n",
            "epoch 0 step 573 loss 0.1352\n",
            "epoch 0 step 574 loss 0.00195\n",
            "epoch 0 step 575 loss 0.46514\n",
            "epoch 0 step 576 loss 0.00184\n",
            "epoch 0 step 577 loss 0.04773\n",
            "epoch 0 step 578 loss 0.00132\n",
            "epoch 0 step 579 loss 0.20053\n",
            "epoch 0 step 580 loss 0.41348\n",
            "epoch 0 step 581 loss 0.06066\n",
            "epoch 0 step 582 loss 0.36024\n",
            "epoch 0 step 583 loss 0.02132\n",
            "epoch 0 step 584 loss 0.00138\n",
            "epoch 0 step 585 loss 0.15277\n",
            "epoch 0 step 586 loss 0.00643\n",
            "epoch 0 step 587 loss 0.23258\n",
            "epoch 0 step 588 loss 0.69998\n",
            "epoch 0 step 589 loss 0.163\n",
            "epoch 0 step 590 loss 0.23137\n",
            "epoch 0 step 591 loss 0.01582\n",
            "epoch 0 step 592 loss 0.02051\n",
            "epoch 0 step 593 loss 0.38477\n",
            "epoch 0 step 594 loss 0.3677\n",
            "epoch 0 step 595 loss 0.20451\n",
            "epoch 0 step 596 loss 0.0029\n",
            "epoch 0 step 597 loss 0.03763\n",
            "epoch 0 step 598 loss 0.11998\n",
            "epoch 0 step 599 loss 0.00461\n",
            "epoch 0 step 600 loss 0.65569\n",
            "epoch 0 step 601 loss 0.02413\n",
            "epoch 0 step 602 loss 0.62852\n",
            "epoch 0 step 603 loss 0.09366\n",
            "epoch 0 step 604 loss 0.06158\n",
            "epoch 0 step 605 loss 0.10082\n",
            "epoch 0 step 606 loss 0.45555\n",
            "epoch 0 step 607 loss 0.1638\n",
            "epoch 0 step 608 loss 0.00874\n",
            "epoch 0 step 609 loss 0.24459\n",
            "epoch 0 step 610 loss 0.16282\n",
            "epoch 0 step 611 loss 0.33363\n",
            "epoch 0 step 612 loss 0.1152\n",
            "epoch 0 step 613 loss 0.01152\n",
            "epoch 0 step 614 loss 0.15875\n",
            "epoch 0 step 615 loss 0.46266\n",
            "epoch 0 step 616 loss 0.17375\n",
            "epoch 0 step 617 loss 0.03989\n",
            "epoch 0 step 618 loss 0.11862\n",
            "epoch 0 step 619 loss 0.0929\n",
            "epoch 0 step 620 loss 0.10481\n",
            "epoch 0 step 621 loss 0.07235\n",
            "epoch 0 step 622 loss 0.10636\n",
            "epoch 0 step 623 loss 0.03488\n",
            "epoch 0 step 624 loss 0.09802\n",
            "epoch 0 step 625 loss 0.5201\n",
            "epoch 0 step 626 loss 0.07047\n",
            "epoch 0 step 627 loss 0.2558\n",
            "epoch 0 step 628 loss 0.21837\n",
            "epoch 0 step 629 loss 0.09975\n",
            "epoch 0 step 630 loss 0.21075\n",
            "epoch 0 step 631 loss 0.06324\n",
            "epoch 0 step 632 loss 0.12521\n",
            "epoch 0 step 633 loss 0.39869\n",
            "epoch 0 step 634 loss 0.42819\n",
            "epoch 0 step 635 loss 0.04719\n",
            "epoch 0 step 636 loss 0.05357\n",
            "epoch 0 step 637 loss 0.12258\n",
            "epoch 0 step 638 loss 0.06092\n",
            "epoch 0 step 639 loss 0.0244\n",
            "epoch 0 step 640 loss 0.02771\n",
            "epoch 0 step 641 loss 0.01664\n",
            "epoch 0 step 642 loss 0.0086\n",
            "epoch 0 step 643 loss 0.00241\n",
            "epoch 0 step 644 loss 0.13157\n",
            "epoch 0 step 645 loss 0.16851\n",
            "epoch 0 step 646 loss 0.12547\n",
            "epoch 0 step 647 loss 0.00783\n",
            "epoch 0 step 648 loss 0.09878\n",
            "epoch 0 step 649 loss 0.09644\n",
            "epoch 0 step 650 loss 0.04222\n",
            "epoch 0 step 651 loss 0.2517\n",
            "epoch 0 step 652 loss 0.00696\n",
            "epoch 0 step 653 loss 0.19564\n",
            "epoch 0 step 654 loss 0.00922\n",
            "epoch 0 step 655 loss 0.0006\n",
            "epoch 0 step 656 loss 0.05634\n",
            "epoch 0 step 657 loss 0.03129\n",
            "epoch 0 step 658 loss 0.03272\n",
            "epoch 0 step 659 loss 0.22665\n",
            "epoch 0 step 660 loss 0.23466\n",
            "epoch 0 step 661 loss 0.03521\n",
            "epoch 0 step 662 loss 0.05793\n",
            "epoch 0 step 663 loss 0.15394\n",
            "epoch 0 step 664 loss 0.28057\n",
            "epoch 0 step 665 loss 0.35883\n",
            "epoch 0 step 666 loss 0.02251\n",
            "epoch 0 step 667 loss 0.3383\n",
            "epoch 0 step 668 loss 0.00321\n",
            "epoch 0 step 669 loss 0.00791\n",
            "epoch 0 step 670 loss 0.00893\n",
            "epoch 0 step 671 loss 0.00563\n",
            "epoch 0 step 672 loss 0.02866\n",
            "epoch 0 step 673 loss 0.158\n",
            "epoch 0 step 674 loss 0.00344\n",
            "epoch 0 step 675 loss 0.00437\n",
            "epoch 0 step 676 loss 0.41011\n",
            "epoch 0 step 677 loss 0.00328\n",
            "epoch 0 step 678 loss 0.00079\n",
            "epoch 0 step 679 loss 0.01789\n",
            "epoch 0 step 680 loss 0.00885\n",
            "epoch 0 step 681 loss 0.04803\n",
            "epoch 0 step 682 loss 0.24167\n",
            "epoch 0 step 683 loss 0.0003\n",
            "epoch 0 step 684 loss 0.0232\n",
            "epoch 0 step 685 loss 0.03264\n",
            "epoch 0 step 686 loss 0.09822\n",
            "epoch 0 step 687 loss 0.0028\n",
            "epoch 0 step 688 loss 0.02554\n",
            "epoch 0 step 689 loss 0.11007\n",
            "epoch 0 step 690 loss 0.19245\n",
            "epoch 0 step 691 loss 0.01205\n",
            "epoch 0 step 692 loss 1e-05\n",
            "epoch 0 step 693 loss 0.0031\n",
            "epoch 0 step 694 loss 0.00198\n",
            "epoch 0 step 695 loss 0.38954\n",
            "epoch 0 step 696 loss 0.0038\n",
            "epoch 0 step 697 loss 0.00522\n",
            "epoch 0 step 698 loss 0.00024\n",
            "epoch 0 step 699 loss 0.0\n",
            "epoch 0 step 700 loss 0.01176\n",
            "epoch 0 step 701 loss 0.01934\n",
            "epoch 0 step 702 loss 7e-05\n",
            "epoch 0 step 703 loss 0.53823\n",
            "epoch 0 step 704 loss 0.0834\n",
            "epoch 0 step 705 loss 6e-05\n",
            "epoch 0 step 706 loss 0.03118\n",
            "epoch 0 step 707 loss 0.43262\n",
            "epoch 0 step 708 loss 0.56104\n",
            "epoch 0 step 709 loss 0.40725\n",
            "epoch 0 step 710 loss 0.27215\n",
            "epoch 0 step 711 loss 0.00597\n",
            "epoch 0 step 712 loss 0.30008\n",
            "epoch 0 step 713 loss 0.03317\n",
            "epoch 0 step 714 loss 0.12946\n",
            "epoch 0 step 715 loss 0.29618\n",
            "epoch 0 step 716 loss 0.34574\n",
            "epoch 0 step 717 loss 0.10131\n",
            "epoch 0 step 718 loss 0.46832\n",
            "epoch 0 step 719 loss 0.15889\n",
            "epoch 0 step 720 loss 0.09847\n",
            "epoch 0 step 721 loss 0.42745\n",
            "epoch 0 step 722 loss 0.09208\n",
            "epoch 0 step 723 loss 0.07849\n",
            "epoch 0 step 724 loss 0.00445\n",
            "epoch 0 step 725 loss 0.05752\n",
            "epoch 0 step 726 loss 0.3187\n",
            "epoch 0 step 727 loss 0.19733\n",
            "epoch 0 step 728 loss 0.19037\n",
            "epoch 0 step 729 loss 0.13896\n",
            "epoch 0 step 730 loss 0.20591\n",
            "epoch 0 step 731 loss 0.37153\n",
            "epoch 0 step 732 loss 0.03537\n",
            "epoch 0 step 733 loss 0.02692\n",
            "epoch 0 step 734 loss 0.16681\n",
            "epoch 0 step 735 loss 0.00216\n",
            "epoch 0 step 736 loss 0.0722\n",
            "epoch 0 step 737 loss 0.50512\n",
            "epoch 0 step 738 loss 0.06736\n",
            "epoch 0 step 739 loss 0.22136\n",
            "epoch 0 step 740 loss 0.14258\n",
            "epoch 0 step 741 loss 0.27411\n",
            "epoch 0 step 742 loss 0.09235\n",
            "epoch 0 step 743 loss 0.02063\n",
            "epoch 0 step 744 loss 0.2388\n",
            "epoch 0 step 745 loss 0.01752\n",
            "epoch 0 step 746 loss 0.12196\n",
            "epoch 0 step 747 loss 0.16918\n",
            "epoch 0 step 748 loss 0.13624\n",
            "epoch 0 step 749 loss 0.54488\n",
            "epoch 0 step 750 loss 0.14201\n",
            "epoch 0 step 751 loss 0.04533\n",
            "epoch 0 step 752 loss 0.02079\n",
            "epoch 0 step 753 loss 0.10574\n",
            "epoch 0 step 754 loss 0.05532\n",
            "epoch 0 step 755 loss 0.32216\n",
            "epoch 0 step 756 loss 0.00384\n",
            "epoch 0 step 757 loss 0.39337\n",
            "epoch 0 step 758 loss 0.73921\n",
            "epoch 0 step 759 loss 0.48809\n",
            "epoch 0 step 760 loss 0.09736\n",
            "epoch 0 step 761 loss 0.0003\n",
            "epoch 0 step 762 loss 0.10077\n",
            "epoch 0 step 763 loss 0.00557\n",
            "epoch 0 step 764 loss 0.184\n",
            "epoch 0 step 765 loss 0.13227\n",
            "epoch 0 step 766 loss 0.00108\n",
            "epoch 0 step 767 loss 0.10106\n",
            "epoch 0 step 768 loss 0.2358\n",
            "epoch 0 step 769 loss 0.85435\n",
            "epoch 0 step 770 loss 0.03903\n",
            "epoch 0 step 771 loss 0.02362\n",
            "epoch 0 step 772 loss 0.02789\n",
            "epoch 0 step 773 loss 0.17849\n",
            "epoch 0 step 774 loss 0.46851\n",
            "epoch 0 step 775 loss 0.14386\n",
            "epoch 0 step 776 loss 0.1658\n",
            "epoch 0 step 777 loss 0.29065\n",
            "epoch 0 step 778 loss 0.01104\n",
            "epoch 0 step 779 loss 0.12622\n",
            "epoch 0 step 780 loss 0.09069\n",
            "epoch 0 step 781 loss 0.1787\n",
            "epoch 0 step 782 loss 0.00414\n",
            "epoch 0 step 783 loss 0.01397\n",
            "epoch 0 step 784 loss 0.46563\n",
            "epoch 0 step 785 loss 0.10772\n",
            "epoch 0 step 786 loss 0.1697\n",
            "epoch 0 step 787 loss 0.17109\n",
            "epoch 0 step 788 loss 0.00595\n",
            "epoch 0 step 789 loss 0.00388\n",
            "epoch 0 step 790 loss 0.00929\n",
            "epoch 0 step 791 loss 0.08378\n",
            "epoch 0 step 792 loss 0.22458\n",
            "epoch 0 step 793 loss 0.20108\n",
            "epoch 0 step 794 loss 0.03193\n",
            "epoch 0 step 795 loss 0.20407\n",
            "epoch 0 step 796 loss 0.19904\n",
            "epoch 0 step 797 loss 0.03519\n",
            "epoch 0 step 798 loss 0.25007\n",
            "epoch 0 step 799 loss 0.32664\n",
            "epoch 0 step 800 loss 0.07357\n",
            "epoch 0 step 801 loss 0.00135\n",
            "epoch 0 step 802 loss 0.01322\n",
            "epoch 0 step 803 loss 0.40551\n",
            "epoch 0 step 804 loss 0.17272\n",
            "epoch 0 step 805 loss 0.25808\n",
            "epoch 0 step 806 loss 0.31423\n",
            "epoch 0 step 807 loss 0.00197\n",
            "epoch 0 step 808 loss 0.03723\n",
            "epoch 0 step 809 loss 0.10653\n",
            "epoch 0 step 810 loss 0.01648\n",
            "epoch 0 step 811 loss 0.07367\n",
            "epoch 0 step 812 loss 0.41631\n",
            "epoch 0 step 813 loss 0.00378\n",
            "epoch 0 step 814 loss 0.00069\n",
            "epoch 0 step 815 loss 0.17067\n",
            "epoch 0 step 816 loss 0.01421\n",
            "epoch 0 step 817 loss 0.12664\n",
            "epoch 0 step 818 loss 0.10869\n",
            "epoch 0 step 819 loss 0.00855\n",
            "epoch 0 step 820 loss 0.1848\n",
            "epoch 0 step 821 loss 0.21804\n",
            "epoch 0 step 822 loss 0.18156\n",
            "epoch 0 step 823 loss 0.29595\n",
            "epoch 0 step 824 loss 0.09292\n",
            "epoch 0 step 825 loss 0.02741\n",
            "epoch 0 step 826 loss 0.32827\n",
            "epoch 0 step 827 loss 0.08102\n",
            "epoch 0 step 828 loss 0.09197\n",
            "epoch 0 step 829 loss 0.00322\n",
            "epoch 0 step 830 loss 0.23443\n",
            "epoch 0 step 831 loss 0.19239\n",
            "epoch 0 step 832 loss 0.0615\n",
            "epoch 0 step 833 loss 0.05411\n",
            "epoch 0 step 834 loss 0.02073\n",
            "epoch 0 step 835 loss 0.33406\n",
            "epoch 0 step 836 loss 0.01248\n",
            "epoch 0 step 837 loss 0.02668\n",
            "epoch 0 step 838 loss 0.01776\n",
            "epoch 0 step 839 loss 0.53384\n",
            "epoch 0 step 840 loss 0.05407\n",
            "epoch 0 step 841 loss 0.00875\n",
            "epoch 0 step 842 loss 0.02398\n",
            "epoch 0 step 843 loss 0.00098\n",
            "epoch 0 step 844 loss 0.27106\n",
            "epoch 0 step 845 loss 0.00029\n",
            "epoch 0 step 846 loss 0.10116\n",
            "epoch 0 step 847 loss 0.28498\n",
            "epoch 0 step 848 loss 0.12448\n",
            "epoch 0 step 849 loss 0.03215\n",
            "epoch 0 step 850 loss 0.12159\n",
            "epoch 0 step 851 loss 0.01343\n",
            "epoch 0 step 852 loss 0.02004\n",
            "epoch 0 step 853 loss 0.35034\n",
            "epoch 0 step 854 loss 0.2214\n",
            "epoch 0 step 855 loss 0.16836\n",
            "epoch 0 step 856 loss 0.1327\n",
            "epoch 0 step 857 loss 0.13363\n",
            "epoch 0 step 858 loss 0.1654\n",
            "epoch 0 step 859 loss 0.11967\n",
            "epoch 0 step 860 loss 0.19625\n",
            "epoch 0 step 861 loss 0.60588\n",
            "epoch 0 step 862 loss 0.50682\n",
            "epoch 0 step 863 loss 0.09683\n",
            "epoch 0 step 864 loss 0.00035\n",
            "epoch 0 step 865 loss 0.00042\n",
            "epoch 0 step 866 loss 0.1847\n",
            "epoch 0 step 867 loss 0.0371\n",
            "epoch 0 step 868 loss 0.15886\n",
            "epoch 0 step 869 loss 0.50808\n",
            "epoch 0 step 870 loss 0.16111\n",
            "epoch 0 step 871 loss 0.06727\n",
            "epoch 0 step 872 loss 0.01426\n",
            "epoch 0 step 873 loss 0.10544\n",
            "epoch 0 step 874 loss 0.06773\n",
            "epoch 0 step 875 loss 0.01948\n",
            "epoch 0 step 876 loss 0.43939\n",
            "epoch 0 step 877 loss 0.15345\n",
            "epoch 0 step 878 loss 0.20394\n",
            "epoch 0 step 879 loss 0.44352\n",
            "epoch 0 step 880 loss 0.17872\n",
            "epoch 0 step 881 loss 0.11377\n",
            "epoch 0 step 882 loss 0.09542\n",
            "epoch 0 step 883 loss 0.00015\n",
            "epoch 0 step 884 loss 0.21289\n",
            "epoch 0 step 885 loss 0.28751\n",
            "epoch 0 step 886 loss 0.01691\n",
            "epoch 0 step 887 loss 0.08858\n",
            "epoch 0 step 888 loss 0.00253\n",
            "epoch 0 step 889 loss 0.23659\n",
            "epoch 0 step 890 loss 0.0546\n",
            "epoch 0 step 891 loss 0.52084\n",
            "epoch 0 step 892 loss 0.24191\n",
            "epoch 0 step 893 loss 0.0001\n",
            "epoch 0 step 894 loss 0.24557\n",
            "epoch 0 step 895 loss 0.12556\n",
            "epoch 0 step 896 loss 0.09197\n",
            "epoch 0 step 897 loss 0.15026\n",
            "epoch 0 step 898 loss 0.08582\n",
            "epoch 0 step 899 loss 0.00105\n",
            "epoch 0 step 900 loss 0.15144\n",
            "epoch 0 step 901 loss 0.16374\n",
            "epoch 0 step 902 loss 0.25179\n",
            "epoch 0 step 903 loss 0.02934\n",
            "epoch 0 step 904 loss 0.44096\n",
            "epoch 0 step 905 loss 0.07811\n",
            "epoch 0 step 906 loss 0.31043\n",
            "epoch 0 step 907 loss 0.02907\n",
            "epoch 0 step 908 loss 0.13403\n",
            "epoch 0 step 909 loss 0.00341\n",
            "epoch 0 step 910 loss 0.34442\n",
            "epoch 0 step 911 loss 0.15012\n",
            "epoch 0 step 912 loss 0.02892\n",
            "epoch 0 step 913 loss 0.48781\n",
            "epoch 0 step 914 loss 0.23473\n",
            "epoch 0 step 915 loss 0.17258\n",
            "epoch 0 step 916 loss 0.00386\n",
            "epoch 0 step 917 loss 0.07814\n",
            "epoch 0 step 918 loss 0.00459\n",
            "epoch 0 step 919 loss 0.1565\n",
            "epoch 0 step 920 loss 0.08312\n",
            "epoch 0 step 921 loss 0.38437\n",
            "epoch 0 step 922 loss 0.00085\n",
            "epoch 0 step 923 loss 0.23326\n",
            "epoch 0 step 924 loss 0.01254\n",
            "epoch 0 step 925 loss 0.38396\n",
            "epoch 0 step 926 loss 0.08429\n",
            "epoch 0 step 927 loss 0.30879\n",
            "epoch 0 step 928 loss 0.00754\n",
            "epoch 0 step 929 loss 0.18971\n",
            "epoch 0 step 930 loss 0.15296\n",
            "epoch 0 step 931 loss 0.03421\n",
            "epoch 0 step 932 loss 0.03604\n",
            "epoch 0 step 933 loss 0.01623\n",
            "epoch 0 step 934 loss 0.19052\n",
            "epoch 0 step 935 loss 0.22804\n",
            "epoch 0 step 936 loss 0.00168\n",
            "epoch 0 step 937 loss 0.00056\n",
            "epoch 0 step 938 loss 0.05154\n",
            "epoch 0 step 939 loss 0.15426\n",
            "epoch 0 step 940 loss 0.22815\n",
            "epoch 0 step 941 loss 0.14624\n",
            "epoch 0 step 942 loss 0.19914\n",
            "epoch 0 step 943 loss 0.07598\n",
            "epoch 0 step 944 loss 0.00605\n",
            "epoch 0 step 945 loss 0.10206\n",
            "epoch 0 step 946 loss 0.04249\n",
            "epoch 0 step 947 loss 0.01545\n",
            "epoch 0 step 948 loss 0.13179\n",
            "epoch 0 step 949 loss 0.05974\n",
            "epoch 0 step 950 loss 0.42151\n",
            "epoch 0 step 951 loss 0.32505\n",
            "epoch 0 step 952 loss 0.11079\n",
            "epoch 0 step 953 loss 0.28659\n",
            "epoch 0 step 954 loss 0.00951\n",
            "epoch 0 step 955 loss 0.12553\n",
            "epoch 0 step 956 loss 0.09496\n",
            "epoch 0 step 957 loss 0.01415\n",
            "epoch 0 step 958 loss 0.0503\n",
            "epoch 0 step 959 loss 0.19927\n",
            "epoch 0 step 960 loss 0.31331\n",
            "epoch 0 step 961 loss 0.27909\n",
            "epoch 0 step 962 loss 0.66763\n",
            "epoch 0 step 963 loss 0.00122\n",
            "epoch 0 step 964 loss 0.0716\n",
            "epoch 0 step 965 loss 0.01383\n",
            "epoch 0 step 966 loss 0.12194\n",
            "epoch 0 step 967 loss 0.65663\n",
            "epoch 0 step 968 loss 0.52259\n",
            "epoch 0 step 969 loss 0.38455\n",
            "epoch 0 step 970 loss 0.16345\n",
            "epoch 0 step 971 loss 0.00878\n",
            "epoch 0 step 972 loss 0.80918\n",
            "epoch 0 step 973 loss 0.00366\n",
            "epoch 0 step 974 loss 0.32178\n",
            "epoch 0 step 975 loss 0.18445\n",
            "epoch 0 step 976 loss 0.12797\n",
            "epoch 0 step 977 loss 0.02672\n",
            "epoch 0 step 978 loss 0.19161\n",
            "epoch 0 step 979 loss 0.02235\n",
            "epoch 0 step 980 loss 0.00785\n",
            "epoch 0 step 981 loss 0.18624\n",
            "epoch 0 step 982 loss 0.02097\n",
            "epoch 0 step 983 loss 0.4525\n",
            "epoch 0 step 984 loss 0.10838\n",
            "epoch 0 step 985 loss 0.14261\n",
            "epoch 0 step 986 loss 0.00052\n",
            "epoch 0 step 987 loss 0.37706\n",
            "epoch 0 step 988 loss 0.04701\n",
            "epoch 0 step 989 loss 0.01441\n",
            "epoch 0 step 990 loss 0.17541\n",
            "epoch 0 step 991 loss 0.74127\n",
            "epoch 0 step 992 loss 0.10458\n",
            "epoch 0 step 993 loss 0.08015\n",
            "epoch 0 step 994 loss 0.06746\n",
            "epoch 0 step 995 loss 0.27974\n",
            "epoch 0 step 996 loss 0.15522\n",
            "epoch 0 step 997 loss 0.00238\n",
            "epoch 0 step 998 loss 0.0451\n",
            "epoch 0 step 999 loss 0.02194\n",
            "epoch 0 step 1000 loss 0.26139\n",
            "epoch 0 step 1001 loss 0.00118\n",
            "epoch 0 step 1002 loss 0.19832\n",
            "epoch 0 step 1003 loss 0.05191\n",
            "epoch 0 step 1004 loss 0.09225\n",
            "epoch 0 step 1005 loss 0.10667\n",
            "epoch 0 step 1006 loss 0.17399\n",
            "epoch 0 step 1007 loss 0.0013\n",
            "epoch 0 step 1008 loss 0.12797\n",
            "epoch 0 step 1009 loss 0.00456\n",
            "epoch 0 step 1010 loss 0.02217\n",
            "epoch 0 step 1011 loss 0.00744\n",
            "epoch 0 step 1012 loss 0.20327\n",
            "epoch 0 step 1013 loss 0.11676\n",
            "epoch 0 step 1014 loss 0.16941\n",
            "epoch 0 step 1015 loss 0.14991\n",
            "epoch 0 step 1016 loss 0.37751\n",
            "epoch 0 step 1017 loss 0.08993\n",
            "epoch 0 step 1018 loss 0.1147\n",
            "epoch 0 step 1019 loss 0.00421\n",
            "epoch 0 step 1020 loss 0.17032\n",
            "epoch 0 step 1021 loss 0.05843\n",
            "epoch 0 step 1022 loss 0.60628\n",
            "epoch 0 step 1023 loss 0.09159\n",
            "epoch 0 step 1024 loss 0.02142\n",
            "epoch 0 step 1025 loss 0.15692\n",
            "epoch 0 step 1026 loss 0.09584\n",
            "epoch 0 step 1027 loss 0.08763\n",
            "epoch 0 step 1028 loss 0.31156\n",
            "epoch 0 step 1029 loss 0.06846\n",
            "epoch 0 step 1030 loss 0.09966\n",
            "epoch 0 step 1031 loss 0.01078\n",
            "epoch 0 step 1032 loss 0.23409\n",
            "epoch 0 step 1033 loss 0.44326\n",
            "epoch 0 step 1034 loss 0.00768\n",
            "epoch 0 step 1035 loss 0.45833\n",
            "epoch 0 step 1036 loss 0.15683\n",
            "epoch 0 step 1037 loss 0.13607\n",
            "epoch 0 step 1038 loss 0.11596\n",
            "epoch 0 step 1039 loss 0.09395\n",
            "epoch 0 step 1040 loss 0.0788\n",
            "epoch 0 step 1041 loss 0.19314\n",
            "epoch 0 step 1042 loss 0.03056\n",
            "epoch 0 step 1043 loss 0.33676\n",
            "epoch 0 step 1044 loss 0.15513\n",
            "epoch 0 step 1045 loss 0.00685\n",
            "epoch 0 step 1046 loss 0.00566\n",
            "epoch 0 step 1047 loss 0.26823\n",
            "epoch 0 step 1048 loss 0.0909\n",
            "epoch 0 step 1049 loss 0.16779\n",
            "epoch 0 step 1050 loss 0.06621\n",
            "epoch 0 step 1051 loss 0.1245\n",
            "epoch 0 step 1052 loss 0.29174\n",
            "epoch 0 step 1053 loss 0.22229\n",
            "epoch 0 step 1054 loss 0.03694\n",
            "epoch 0 step 1055 loss 0.04938\n",
            "epoch 0 step 1056 loss 0.04832\n",
            "epoch 0 step 1057 loss 0.00286\n",
            "epoch 0 step 1058 loss 0.10209\n",
            "epoch 0 step 1059 loss 0.03884\n",
            "epoch 0 step 1060 loss 0.20147\n",
            "epoch 0 step 1061 loss 0.00027\n",
            "epoch 0 step 1062 loss 0.11108\n",
            "epoch 0 step 1063 loss 0.18405\n",
            "epoch 0 step 1064 loss 0.03133\n",
            "epoch 0 step 1065 loss 0.10095\n",
            "epoch 0 step 1066 loss 0.00772\n",
            "epoch 0 step 1067 loss 0.1169\n",
            "epoch 0 step 1068 loss 0.29776\n",
            "epoch 0 step 1069 loss 0.21485\n",
            "epoch 0 step 1070 loss 0.15283\n",
            "epoch 0 step 1071 loss 0.00226\n",
            "epoch 0 step 1072 loss 0.01601\n",
            "epoch 0 step 1073 loss 0.00659\n",
            "epoch 0 step 1074 loss 4e-05\n",
            "epoch 0 step 1075 loss 0.1036\n",
            "epoch 0 step 1076 loss 0.04036\n",
            "epoch 0 step 1077 loss 0.27544\n",
            "epoch 0 step 1078 loss 0.68185\n",
            "epoch 0 step 1079 loss 0.01709\n",
            "epoch 0 step 1080 loss 0.19371\n",
            "epoch 0 step 1081 loss 0.02834\n",
            "epoch 0 step 1082 loss 0.26396\n",
            "epoch 0 step 1083 loss 0.05046\n",
            "epoch 0 step 1084 loss 0.00221\n",
            "epoch 0 step 1085 loss 0.10579\n",
            "epoch 0 step 1086 loss 0.25459\n",
            "epoch 0 step 1087 loss 0.00283\n",
            "epoch 0 step 1088 loss 0.10447\n",
            "epoch 0 step 1089 loss 0.00105\n",
            "epoch 0 step 1090 loss 0.1064\n",
            "epoch 0 step 1091 loss 0.04337\n",
            "epoch 0 step 1092 loss 0.00574\n",
            "epoch 0 step 1093 loss 0.31777\n",
            "epoch 0 step 1094 loss 0.02328\n",
            "epoch 0 step 1095 loss 0.00347\n",
            "epoch 0 step 1096 loss 0.13608\n",
            "epoch 0 step 1097 loss 0.00925\n",
            "epoch 0 step 1098 loss 0.08299\n",
            "epoch 0 step 1099 loss 0.1963\n",
            "epoch 0 step 1100 loss 0.02341\n",
            "epoch 0 step 1101 loss 0.04582\n",
            "epoch 0 step 1102 loss 0.00068\n",
            "epoch 0 step 1103 loss 0.20533\n",
            "epoch 0 step 1104 loss 0.09588\n",
            "epoch 0 step 1105 loss 0.41509\n",
            "epoch 0 step 1106 loss 0.08954\n",
            "epoch 0 step 1107 loss 0.00016\n",
            "epoch 0 step 1108 loss 0.00654\n",
            "epoch 0 step 1109 loss 0.13298\n",
            "epoch 0 step 1110 loss 0.2153\n",
            "epoch 0 step 1111 loss 0.03356\n",
            "epoch 0 step 1112 loss 0.06661\n",
            "epoch 0 step 1113 loss 0.24839\n",
            "epoch 0 step 1114 loss 0.08699\n",
            "epoch 0 step 1115 loss 0.00082\n",
            "epoch 0 step 1116 loss 0.06615\n",
            "epoch 0 step 1117 loss 0.01127\n",
            "epoch 0 step 1118 loss 0.10214\n",
            "epoch 0 step 1119 loss 0.00411\n",
            "epoch 0 step 1120 loss 0.11684\n",
            "epoch 0 step 1121 loss 0.01439\n",
            "epoch 0 step 1122 loss 0.03898\n",
            "epoch 0 step 1123 loss 0.24617\n",
            "epoch 0 step 1124 loss 0.23573\n",
            "epoch 0 step 1125 loss 0.01048\n",
            "epoch 0 step 1126 loss 0.39705\n",
            "epoch 0 step 1127 loss 0.01534\n",
            "epoch 0 step 1128 loss 0.11757\n",
            "epoch 0 step 1129 loss 0.03271\n",
            "epoch 0 step 1130 loss 0.17353\n",
            "epoch 0 step 1131 loss 0.191\n",
            "epoch 0 step 1132 loss 0.42919\n",
            "epoch 0 step 1133 loss 0.30647\n",
            "epoch 0 step 1134 loss 0.35187\n",
            "epoch 0 step 1135 loss 0.27863\n",
            "epoch 0 step 1136 loss 0.21422\n",
            "epoch 0 step 1137 loss 0.53619\n",
            "epoch 0 step 1138 loss 7e-05\n",
            "epoch 0 step 1139 loss 0.1392\n",
            "epoch 0 step 1140 loss 0.29774\n",
            "epoch 0 step 1141 loss 0.25949\n",
            "epoch 0 step 1142 loss 0.38704\n",
            "epoch 0 step 1143 loss 0.27624\n",
            "epoch 0 step 1144 loss 0.02195\n",
            "epoch 0 step 1145 loss 0.07903\n",
            "epoch 0 step 1146 loss 0.25167\n",
            "epoch 0 step 1147 loss 0.01878\n",
            "epoch 0 step 1148 loss 0.06541\n",
            "epoch 0 step 1149 loss 0.10062\n",
            "epoch 0 step 1150 loss 0.0113\n",
            "epoch 0 step 1151 loss 0.09953\n",
            "epoch 0 step 1152 loss 0.14936\n",
            "epoch 0 step 1153 loss 0.20109\n",
            "epoch 0 step 1154 loss 0.00249\n",
            "epoch 0 step 1155 loss 0.00076\n",
            "epoch 0 step 1156 loss 0.01729\n",
            "epoch 0 step 1157 loss 0.2029\n",
            "epoch 0 step 1158 loss 0.07589\n",
            "epoch 0 step 1159 loss 0.19283\n",
            "epoch 0 step 1160 loss 0.11767\n",
            "epoch 0 step 1161 loss 0.0953\n",
            "epoch 0 step 1162 loss 0.1016\n",
            "epoch 0 step 1163 loss 0.03984\n",
            "epoch 0 step 1164 loss 0.0148\n",
            "epoch 0 step 1165 loss 0.54876\n",
            "epoch 0 step 1166 loss 0.46662\n",
            "epoch 0 step 1167 loss 0.06158\n",
            "epoch 0 step 1168 loss 0.01668\n",
            "epoch 0 step 1169 loss 0.20718\n",
            "epoch 0 step 1170 loss 0.00043\n",
            "epoch 0 step 1171 loss 0.31849\n",
            "epoch 0 step 1172 loss 0.01501\n",
            "epoch 0 step 1173 loss 0.07566\n",
            "epoch 0 step 1174 loss 0.14101\n",
            "epoch 0 step 1175 loss 0.36263\n",
            "epoch 0 step 1176 loss 0.16526\n",
            "epoch 0 step 1177 loss 0.11902\n",
            "epoch 0 step 1178 loss 0.12681\n",
            "epoch 0 step 1179 loss 0.08007\n",
            "epoch 0 step 1180 loss 0.04895\n",
            "epoch 0 step 1181 loss 0.28726\n",
            "epoch 0 step 1182 loss 0.31501\n",
            "epoch 0 step 1183 loss 0.02592\n",
            "epoch 0 step 1184 loss 0.75591\n",
            "epoch 0 step 1185 loss 0.13685\n",
            "epoch 0 step 1186 loss 0.42285\n",
            "epoch 0 step 1187 loss 0.44857\n",
            "epoch 0 step 1188 loss 0.09003\n",
            "epoch 0 step 1189 loss 0.43109\n",
            "epoch 0 step 1190 loss 0.20696\n",
            "epoch 0 step 1191 loss 0.00034\n",
            "epoch 0 step 1192 loss 0.33784\n",
            "epoch 0 step 1193 loss 0.19535\n",
            "epoch 0 step 1194 loss 0.01314\n",
            "epoch 0 step 1195 loss 0.07045\n",
            "epoch 0 step 1196 loss 0.10965\n",
            "epoch 0 step 1197 loss 0.10314\n",
            "epoch 0 step 1198 loss 0.14477\n",
            "epoch 0 step 1199 loss 0.03486\n",
            "epoch 0 step 1200 loss 0.03297\n",
            "epoch 0 step 1201 loss 0.09594\n",
            "epoch 0 step 1202 loss 0.2437\n",
            "epoch 0 step 1203 loss 0.11426\n",
            "epoch 0 step 1204 loss 0.38443\n",
            "epoch 0 step 1205 loss 0.01802\n",
            "epoch 0 step 1206 loss 0.00211\n",
            "epoch 0 step 1207 loss 0.26143\n",
            "epoch 0 step 1208 loss 0.01693\n",
            "epoch 0 step 1209 loss 0.18529\n",
            "epoch 0 step 1210 loss 0.00248\n",
            "epoch 0 step 1211 loss 0.15349\n",
            "epoch 0 step 1212 loss 0.03103\n",
            "epoch 0 step 1213 loss 0.43917\n",
            "epoch 0 step 1214 loss 0.1882\n",
            "epoch 0 step 1215 loss 0.1339\n",
            "epoch 0 step 1216 loss 0.20801\n",
            "epoch 0 step 1217 loss 0.10322\n",
            "epoch 0 step 1218 loss 0.00032\n",
            "epoch 0 step 1219 loss 0.0009\n",
            "epoch 0 step 1220 loss 0.43208\n",
            "epoch 0 step 1221 loss 0.18187\n",
            "epoch 0 step 1222 loss 0.18133\n",
            "epoch 0 step 1223 loss 0.33476\n",
            "epoch 0 step 1224 loss 0.02365\n",
            "epoch 0 step 1225 loss 0.00184\n",
            "epoch 0 step 1226 loss 0.11985\n",
            "epoch 0 step 1227 loss 0.01159\n",
            "epoch 0 step 1228 loss 0.14097\n",
            "epoch 0 step 1229 loss 0.02463\n",
            "epoch 0 step 1230 loss 0.15477\n",
            "epoch 0 step 1231 loss 0.00687\n",
            "epoch 0 step 1232 loss 0.01109\n",
            "epoch 0 step 1233 loss 0.00275\n",
            "epoch 0 step 1234 loss 0.00155\n",
            "epoch 0 step 1235 loss 0.01332\n",
            "epoch 0 step 1236 loss 0.02383\n",
            "epoch 0 step 1237 loss 0.02364\n",
            "epoch 0 step 1238 loss 0.2153\n",
            "epoch 0 step 1239 loss 0.00829\n",
            "epoch 0 step 1240 loss 0.05276\n",
            "epoch 0 step 1241 loss 0.01369\n",
            "epoch 0 step 1242 loss 0.66899\n",
            "epoch 0 step 1243 loss 0.43229\n",
            "epoch 0 step 1244 loss 0.34426\n",
            "epoch 0 step 1245 loss 0.05078\n",
            "epoch 0 step 1246 loss 0.22564\n",
            "epoch 0 step 1247 loss 0.39053\n",
            "epoch 0 step 1248 loss 0.12332\n",
            "epoch 0 step 1249 loss 0.00502\n",
            "epoch 0 step 1250 loss 0.17502\n",
            "epoch 0 step 1251 loss 0.00523\n",
            "epoch 0 step 1252 loss 0.05831\n",
            "epoch 0 step 1253 loss 0.32536\n",
            "epoch 0 step 1254 loss 0.08052\n",
            "epoch 0 step 1255 loss 0.25595\n",
            "epoch 0 step 1256 loss 0.2558\n",
            "epoch 0 step 1257 loss 0.00112\n",
            "epoch 0 step 1258 loss 0.20333\n",
            "epoch 0 step 1259 loss 0.0004\n",
            "epoch 0 step 1260 loss 0.26797\n",
            "epoch 0 step 1261 loss 0.19848\n",
            "epoch 0 step 1262 loss 0.15821\n",
            "epoch 0 step 1263 loss 0.07003\n",
            "epoch 0 step 1264 loss 0.13149\n",
            "epoch 0 step 1265 loss 0.00948\n",
            "epoch 0 step 1266 loss 0.45584\n",
            "epoch 0 step 1267 loss 0.21015\n",
            "epoch 0 step 1268 loss 0.00039\n",
            "epoch 0 step 1269 loss 0.18476\n",
            "epoch 0 step 1270 loss 0.3219\n",
            "epoch 0 step 1271 loss 0.2566\n",
            "epoch 0 step 1272 loss 0.00298\n",
            "epoch 0 step 1273 loss 0.09587\n",
            "epoch 0 step 1274 loss 0.20318\n",
            "epoch 0 step 1275 loss 0.20519\n",
            "epoch 0 step 1276 loss 0.0081\n",
            "epoch 0 step 1277 loss 0.00081\n",
            "epoch 0 step 1278 loss 0.06965\n",
            "epoch 0 step 1279 loss 0.24883\n",
            "epoch 0 step 1280 loss 0.0055\n",
            "epoch 0 step 1281 loss 0.06287\n",
            "epoch 0 step 1282 loss 0.01778\n",
            "epoch 0 step 1283 loss 0.01437\n",
            "epoch 0 step 1284 loss 0.00232\n",
            "epoch 0 step 1285 loss 0.12648\n",
            "epoch 0 step 1286 loss 0.04322\n",
            "epoch 0 step 1287 loss 0.03292\n",
            "epoch 0 step 1288 loss 0.02153\n",
            "epoch 0 step 1289 loss 0.04247\n",
            "epoch 0 step 1290 loss 0.00323\n",
            "epoch 0 step 1291 loss 0.24331\n",
            "epoch 0 step 1292 loss 0.19277\n",
            "epoch 0 step 1293 loss 0.00928\n",
            "epoch 0 step 1294 loss 0.26289\n",
            "epoch 0 step 1295 loss 0.18842\n",
            "epoch 0 step 1296 loss 0.0224\n",
            "epoch 0 step 1297 loss 0.00533\n",
            "epoch 0 step 1298 loss 0.34734\n",
            "epoch 0 step 1299 loss 0.24431\n",
            "epoch 0 step 1300 loss 0.0025\n",
            "epoch 0 step 1301 loss 0.12419\n",
            "epoch 0 step 1302 loss 0.26043\n",
            "epoch 0 step 1303 loss 0.2255\n",
            "epoch 0 step 1304 loss 0.31656\n",
            "epoch 0 step 1305 loss 0.38454\n",
            "epoch 0 step 1306 loss 0.07493\n",
            "epoch 0 step 1307 loss 0.22345\n",
            "epoch 0 step 1308 loss 0.09605\n",
            "epoch 0 step 1309 loss 0.04687\n",
            "epoch 0 step 1310 loss 0.0372\n",
            "epoch 0 step 1311 loss 0.39653\n",
            "epoch 0 step 1312 loss 0.18536\n",
            "epoch 0 step 1313 loss 0.04349\n",
            "epoch 0 step 1314 loss 0.128\n",
            "epoch 0 step 1315 loss 0.01241\n",
            "epoch 0 step 1316 loss 0.01307\n",
            "epoch 0 step 1317 loss 0.11194\n",
            "epoch 0 step 1318 loss 0.13159\n",
            "epoch 0 step 1319 loss 0.03518\n",
            "epoch 0 step 1320 loss 0.01057\n",
            "epoch 0 step 1321 loss 0.00352\n",
            "epoch 0 step 1322 loss 0.00375\n",
            "epoch 0 step 1323 loss 0.17764\n",
            "epoch 0 step 1324 loss 0.00744\n",
            "epoch 0 step 1325 loss 0.10834\n",
            "epoch 0 step 1326 loss 0.00894\n",
            "epoch 0 step 1327 loss 0.04038\n",
            "epoch 0 step 1328 loss 0.13806\n",
            "epoch 0 step 1329 loss 0.28156\n",
            "epoch 0 step 1330 loss 0.32398\n",
            "epoch 0 step 1331 loss 0.01592\n",
            "epoch 0 step 1332 loss 0.01682\n",
            "epoch 0 step 1333 loss 0.33359\n",
            "epoch 0 step 1334 loss 0.16112\n",
            "epoch 0 step 1335 loss 0.20151\n",
            "epoch 0 step 1336 loss 0.04681\n",
            "epoch 0 step 1337 loss 0.13137\n",
            "epoch 0 step 1338 loss 0.14086\n",
            "epoch 0 step 1339 loss 0.11777\n",
            "epoch 0 step 1340 loss 0.28379\n",
            "epoch 0 step 1341 loss 0.0078\n",
            "epoch 0 step 1342 loss 0.00058\n",
            "epoch 0 step 1343 loss 0.02695\n",
            "epoch 0 step 1344 loss 0.07693\n",
            "epoch 0 step 1345 loss 0.47637\n",
            "epoch 0 step 1346 loss 0.00284\n",
            "epoch 0 step 1347 loss 0.00361\n",
            "epoch 0 step 1348 loss 0.0086\n",
            "epoch 0 step 1349 loss 0.26974\n",
            "epoch 0 step 1350 loss 0.20266\n",
            "epoch 0 step 1351 loss 0.00036\n",
            "epoch 0 step 1352 loss 0.25068\n",
            "epoch 0 step 1353 loss 0.04039\n",
            "epoch 0 step 1354 loss 0.34725\n",
            "epoch 0 step 1355 loss 0.02928\n",
            "epoch 0 step 1356 loss 0.00245\n",
            "epoch 0 step 1357 loss 0.06224\n",
            "epoch 0 step 1358 loss 0.05771\n",
            "epoch 0 step 1359 loss 0.00034\n",
            "epoch 0 step 1360 loss 0.12809\n",
            "epoch 0 step 1361 loss 0.15027\n",
            "epoch 0 step 1362 loss 0.16471\n",
            "epoch 0 step 1363 loss 0.00586\n",
            "epoch 0 step 1364 loss 0.34761\n",
            "epoch 0 step 1365 loss 0.33342\n",
            "epoch 0 step 1366 loss 0.03471\n",
            "epoch 0 step 1367 loss 0.01435\n",
            "epoch 0 step 1368 loss 0.0029\n",
            "epoch 0 step 1369 loss 0.07766\n",
            "epoch 0 step 1370 loss 0.01296\n",
            "epoch 0 step 1371 loss 0.16218\n",
            "epoch 0 step 1372 loss 0.31201\n",
            "epoch 0 step 1373 loss 0.0008\n",
            "epoch 0 step 1374 loss 0.02303\n",
            "epoch 0 step 1375 loss 0.11138\n",
            "epoch 0 step 1376 loss 0.09093\n",
            "epoch 0 step 1377 loss 0.14381\n",
            "epoch 0 step 1378 loss 0.16245\n",
            "epoch 0 step 1379 loss 0.03434\n",
            "epoch 0 step 1380 loss 0.17631\n",
            "epoch 0 step 1381 loss 0.00279\n",
            "epoch 0 step 1382 loss 0.40357\n",
            "epoch 0 step 1383 loss 0.22668\n",
            "epoch 0 step 1384 loss 0.22972\n",
            "epoch 0 step 1385 loss 0.11218\n",
            "epoch 0 step 1386 loss 0.00114\n",
            "epoch 0 step 1387 loss 0.00957\n",
            "epoch 0 step 1388 loss 0.02081\n",
            "epoch 0 step 1389 loss 0.09078\n",
            "epoch 0 step 1390 loss 0.23136\n",
            "epoch 0 step 1391 loss 0.04796\n",
            "epoch 0 step 1392 loss 0.0021\n",
            "epoch 0 step 1393 loss 0.10311\n",
            "epoch 0 step 1394 loss 0.03272\n",
            "epoch 0 step 1395 loss 0.4513\n",
            "epoch 0 step 1396 loss 0.31455\n",
            "epoch 0 step 1397 loss 0.24227\n",
            "epoch 0 step 1398 loss 0.04321\n",
            "epoch 0 step 1399 loss 0.0426\n",
            "epoch 0 step 1400 loss 0.19145\n",
            "epoch 0 step 1401 loss 0.20153\n",
            "epoch 0 step 1402 loss 0.1896\n",
            "epoch 0 step 1403 loss 0.01497\n",
            "epoch 0 step 1404 loss 0.04213\n",
            "epoch 0 step 1405 loss 0.02942\n",
            "epoch 0 step 1406 loss 0.05696\n",
            "epoch 0 step 1407 loss 0.11816\n",
            "epoch 0 step 1408 loss 0.0002\n",
            "epoch 0 step 1409 loss 0.27994\n",
            "epoch 0 step 1410 loss 0.17915\n",
            "epoch 0 step 1411 loss 0.00384\n",
            "epoch 0 step 1412 loss 0.22917\n",
            "epoch 0 step 1413 loss 0.00061\n",
            "epoch 0 step 1414 loss 0.67534\n",
            "epoch 0 step 1415 loss 0.02193\n",
            "epoch 0 step 1416 loss 0.06759\n",
            "epoch 0 step 1417 loss 0.00432\n",
            "epoch 0 step 1418 loss 0.02219\n",
            "epoch 0 step 1419 loss 0.38786\n",
            "epoch 0 step 1420 loss 0.01831\n",
            "epoch 0 step 1421 loss 0.03005\n",
            "epoch 0 step 1422 loss 0.0819\n",
            "epoch 0 step 1423 loss 0.00105\n",
            "epoch 0 step 1424 loss 0.41957\n",
            "epoch 0 step 1425 loss 0.10041\n",
            "epoch 0 step 1426 loss 0.25296\n",
            "epoch 0 step 1427 loss 0.17602\n",
            "epoch 0 step 1428 loss 0.48195\n",
            "epoch 0 step 1429 loss 0.1537\n",
            "epoch 0 step 1430 loss 0.05406\n",
            "epoch 0 step 1431 loss 0.00229\n",
            "epoch 0 step 1432 loss 0.17045\n",
            "epoch 0 step 1433 loss 0.15708\n",
            "epoch 0 step 1434 loss 0.08383\n",
            "epoch 0 step 1435 loss 0.24841\n",
            "epoch 0 step 1436 loss 0.05878\n",
            "epoch 0 step 1437 loss 0.91004\n",
            "epoch 0 step 1438 loss 0.06732\n",
            "epoch 0 step 1439 loss 0.00918\n",
            "epoch 0 step 1440 loss 0.25097\n",
            "epoch 0 step 1441 loss 0.18461\n",
            "epoch 0 step 1442 loss 0.08228\n",
            "epoch 0 step 1443 loss 0.04426\n",
            "epoch 0 step 1444 loss 0.22802\n",
            "epoch 0 step 1445 loss 0.01312\n",
            "epoch 0 step 1446 loss 0.06878\n",
            "epoch 0 step 1447 loss 0.33806\n",
            "epoch 0 step 1448 loss 0.21218\n",
            "epoch 0 step 1449 loss 0.30181\n",
            "epoch 0 step 1450 loss 0.0719\n",
            "epoch 0 step 1451 loss 0.09777\n",
            "epoch 0 step 1452 loss 0.00079\n",
            "epoch 0 step 1453 loss 0.14107\n",
            "epoch 0 step 1454 loss 0.03732\n",
            "epoch 0 step 1455 loss 0.00968\n",
            "epoch 0 step 1456 loss 0.02815\n",
            "epoch 0 step 1457 loss 0.17161\n",
            "epoch 0 step 1458 loss 0.10766\n",
            "epoch 0 step 1459 loss 0.27753\n",
            "epoch 0 step 1460 loss 0.0887\n",
            "epoch 0 step 1461 loss 0.10581\n",
            "epoch 0 step 1462 loss 0.28207\n",
            "epoch 0 step 1463 loss 0.00816\n",
            "epoch 0 step 1464 loss 0.34968\n",
            "epoch 0 step 1465 loss 0.29456\n",
            "epoch 0 step 1466 loss 0.076\n",
            "epoch 0 step 1467 loss 0.12412\n",
            "epoch 0 step 1468 loss 0.01296\n",
            "epoch 0 step 1469 loss 0.00419\n",
            "epoch 0 step 1470 loss 0.64292\n",
            "epoch 0 step 1471 loss 0.45103\n",
            "epoch 0 step 1472 loss 0.00326\n",
            "epoch 0 step 1473 loss 0.42852\n",
            "epoch 0 step 1474 loss 0.15396\n",
            "epoch 0 step 1475 loss 0.14143\n",
            "epoch 0 step 1476 loss 0.25308\n",
            "epoch 0 step 1477 loss 0.00085\n",
            "epoch 0 step 1478 loss 0.18236\n",
            "epoch 0 step 1479 loss 5e-05\n",
            "epoch 0 step 1480 loss 0.21414\n",
            "epoch 0 step 1481 loss 0.02923\n",
            "epoch 0 step 1482 loss 0.03019\n",
            "epoch 0 step 1483 loss 0.1596\n",
            "epoch 0 step 1484 loss 0.1745\n",
            "epoch 0 step 1485 loss 0.08218\n",
            "epoch 0 step 1486 loss 0.26572\n",
            "epoch 0 step 1487 loss 0.14657\n",
            "epoch 0 step 1488 loss 0.01046\n",
            "epoch 0 step 1489 loss 0.06281\n",
            "epoch 0 step 1490 loss 0.05303\n",
            "epoch 0 step 1491 loss 0.09996\n",
            "epoch 0 step 1492 loss 0.6297\n",
            "epoch 0 step 1493 loss 0.05286\n",
            "epoch 0 step 1494 loss 0.00607\n",
            "epoch 0 step 1495 loss 0.00787\n",
            "epoch 0 step 1496 loss 0.00245\n",
            "epoch 0 step 1497 loss 0.19196\n",
            "epoch 0 step 1498 loss 0.11523\n",
            "epoch 0 step 1499 loss 0.11724\n",
            "epoch 0 step 1500 loss 0.00026\n",
            "epoch 0 step 1501 loss 0.07936\n",
            "epoch 0 step 1502 loss 0.02986\n",
            "epoch 0 step 1503 loss 0.24759\n",
            "epoch 0 step 1504 loss 0.1218\n",
            "epoch 0 step 1505 loss 0.07906\n",
            "epoch 0 step 1506 loss 0.08215\n",
            "epoch 0 step 1507 loss 0.20723\n",
            "epoch 0 step 1508 loss 0.16257\n",
            "epoch 0 step 1509 loss 0.21117\n",
            "epoch 0 step 1510 loss 0.05065\n",
            "epoch 0 step 1511 loss 0.15252\n",
            "epoch 0 step 1512 loss 0.21946\n",
            "epoch 0 step 1513 loss 0.02267\n",
            "epoch 0 step 1514 loss 0.07429\n",
            "epoch 0 step 1515 loss 0.20823\n",
            "epoch 0 step 1516 loss 0.36911\n",
            "epoch 0 step 1517 loss 0.23365\n",
            "epoch 0 step 1518 loss 0.01678\n",
            "epoch 0 step 1519 loss 0.33271\n",
            "epoch 0 step 1520 loss 0.01876\n",
            "epoch 0 step 1521 loss 0.01662\n",
            "epoch 0 step 1522 loss 0.27168\n",
            "epoch 0 step 1523 loss 0.13695\n",
            "epoch 0 step 1524 loss 0.13465\n",
            "epoch 0 step 1525 loss 0.16804\n",
            "epoch 0 step 1526 loss 0.07268\n",
            "epoch 0 step 1527 loss 0.00441\n",
            "epoch 0 step 1528 loss 0.07347\n",
            "epoch 0 step 1529 loss 0.00066\n",
            "epoch 0 step 1530 loss 0.38974\n",
            "epoch 0 step 1531 loss 0.11291\n",
            "epoch 0 step 1532 loss 0.00834\n",
            "epoch 0 step 1533 loss 0.11501\n",
            "epoch 0 step 1534 loss 0.00172\n",
            "epoch 0 step 1535 loss 0.27188\n",
            "epoch 0 step 1536 loss 0.13637\n",
            "epoch 0 step 1537 loss 0.01588\n",
            "epoch 0 step 1538 loss 0.00171\n",
            "epoch 0 step 1539 loss 0.14083\n",
            "epoch 0 step 1540 loss 0.0595\n",
            "epoch 0 step 1541 loss 0.01077\n",
            "epoch 0 step 1542 loss 0.08493\n",
            "epoch 0 step 1543 loss 0.51246\n",
            "epoch 0 step 1544 loss 0.07348\n",
            "epoch 0 step 1545 loss 0.29298\n",
            "epoch 0 step 1546 loss 0.00056\n",
            "epoch 0 step 1547 loss 0.19888\n",
            "epoch 0 step 1548 loss 0.10063\n",
            "epoch 0 step 1549 loss 0.10726\n",
            "epoch 0 step 1550 loss 0.21068\n",
            "epoch 0 step 1551 loss 0.00178\n",
            "epoch 0 step 1552 loss 0.03286\n",
            "epoch 0 step 1553 loss 0.22662\n",
            "epoch 0 step 1554 loss 0.03601\n",
            "epoch 0 step 1555 loss 0.00256\n",
            "epoch 0 step 1556 loss 0.00071\n",
            "epoch 0 step 1557 loss 0.0215\n",
            "epoch 0 step 1558 loss 0.20546\n",
            "epoch 0 step 1559 loss 0.32849\n",
            "epoch 0 step 1560 loss 0.12582\n",
            "epoch 0 step 1561 loss 0.02295\n",
            "epoch 0 step 1562 loss 0.3468\n",
            "epoch 0 step 1563 loss 0.01036\n",
            "epoch 0 step 1564 loss 0.11519\n",
            "epoch 0 step 1565 loss 0.0006\n",
            "epoch 0 step 1566 loss 0.34477\n",
            "epoch 0 step 1567 loss 0.00297\n",
            "epoch 0 step 1568 loss 0.05809\n",
            "epoch 0 step 1569 loss 0.53087\n",
            "epoch 0 step 1570 loss 0.06603\n",
            "epoch 0 step 1571 loss 0.03064\n",
            "epoch 0 step 1572 loss 0.1066\n",
            "epoch 0 step 1573 loss 7e-05\n",
            "epoch 0 step 1574 loss 0.27529\n",
            "epoch 0 step 1575 loss 0.11415\n",
            "epoch 0 step 1576 loss 0.00615\n",
            "epoch 0 step 1577 loss 0.0085\n",
            "epoch 0 step 1578 loss 0.01907\n",
            "epoch 0 step 1579 loss 0.27464\n",
            "epoch 0 step 1580 loss 0.06304\n",
            "epoch 0 step 1581 loss 0.00291\n",
            "epoch 0 step 1582 loss 0.0794\n",
            "epoch 0 step 1583 loss 0.06643\n",
            "epoch 0 step 1584 loss 0.48599\n",
            "epoch 0 step 1585 loss 0.04105\n",
            "epoch 0 step 1586 loss 0.07396\n",
            "epoch 0 step 1587 loss 0.37769\n",
            "epoch 0 step 1588 loss 0.08434\n",
            "epoch 0 step 1589 loss 0.18686\n",
            "epoch 0 step 1590 loss 0.62395\n",
            "epoch 0 step 1591 loss 0.166\n",
            "epoch 0 step 1592 loss 0.08136\n",
            "epoch 0 step 1593 loss 0.01817\n",
            "epoch 0 step 1594 loss 0.04556\n",
            "epoch 0 step 1595 loss 0.06365\n",
            "epoch 0 step 1596 loss 0.15951\n",
            "epoch 0 step 1597 loss 0.0484\n",
            "epoch 0 step 1598 loss 0.25627\n",
            "epoch 0 step 1599 loss 0.01661\n",
            "epoch 0 step 1600 loss 0.12402\n",
            "epoch 0 step 1601 loss 0.46455\n",
            "epoch 0 step 1602 loss 0.05115\n",
            "epoch 0 step 1603 loss 0.19976\n",
            "epoch 0 step 1604 loss 0.02458\n",
            "epoch 0 step 1605 loss 0.02161\n",
            "epoch 0 step 1606 loss 0.26805\n",
            "epoch 0 step 1607 loss 0.12166\n",
            "epoch 0 step 1608 loss 0.22803\n",
            "epoch 0 step 1609 loss 0.02371\n",
            "epoch 0 step 1610 loss 0.13738\n",
            "epoch 0 step 1611 loss 0.58231\n",
            "epoch 0 step 1612 loss 0.27681\n",
            "epoch 0 step 1613 loss 0.09296\n",
            "epoch 0 step 1614 loss 0.67356\n",
            "epoch 0 step 1615 loss 0.00163\n",
            "epoch 0 step 1616 loss 0.19268\n",
            "epoch 0 step 1617 loss 0.33778\n",
            "epoch 0 step 1618 loss 0.41711\n",
            "epoch 0 step 1619 loss 0.39237\n",
            "epoch 0 step 1620 loss 0.01082\n",
            "epoch 0 step 1621 loss 0.0121\n",
            "epoch 0 step 1622 loss 0.12385\n",
            "epoch 0 step 1623 loss 0.27677\n",
            "epoch 0 step 1624 loss 0.1253\n",
            "epoch 0 step 1625 loss 0.02743\n",
            "epoch 0 step 1626 loss 0.01551\n",
            "epoch 0 step 1627 loss 0.10459\n",
            "epoch 0 step 1628 loss 0.43299\n",
            "epoch 0 step 1629 loss 0.19015\n",
            "epoch 0 step 1630 loss 0.13856\n",
            "epoch 0 step 1631 loss 0.12713\n",
            "epoch 0 step 1632 loss 0.32169\n",
            "epoch 0 step 1633 loss 0.00354\n",
            "epoch 0 step 1634 loss 0.2196\n",
            "epoch 0 step 1635 loss 0.15472\n",
            "epoch 0 step 1636 loss 0.02299\n",
            "epoch 0 step 1637 loss 0.13997\n",
            "epoch 0 step 1638 loss 0.14498\n",
            "epoch 0 step 1639 loss 0.09745\n",
            "epoch 0 step 1640 loss 0.00955\n",
            "epoch 0 step 1641 loss 0.00045\n",
            "epoch 0 step 1642 loss 0.03322\n",
            "epoch 0 step 1643 loss 0.16689\n",
            "epoch 0 step 1644 loss 0.22973\n",
            "epoch 0 step 1645 loss 0.38844\n",
            "epoch 0 step 1646 loss 0.04584\n",
            "epoch 0 step 1647 loss 0.00037\n",
            "epoch 0 step 1648 loss 0.08794\n",
            "epoch 0 step 1649 loss 0.17457\n",
            "epoch 0 step 1650 loss 0.08592\n",
            "epoch 0 step 1651 loss 0.01114\n",
            "epoch 0 step 1652 loss 0.02372\n",
            "epoch 0 step 1653 loss 0.09409\n",
            "epoch 0 step 1654 loss 0.19656\n",
            "epoch 0 step 1655 loss 0.42658\n",
            "epoch 0 step 1656 loss 0.17363\n",
            "epoch 0 step 1657 loss 0.01667\n",
            "epoch 0 step 1658 loss 0.05943\n",
            "epoch 0 step 1659 loss 0.01602\n",
            "epoch 0 step 1660 loss 0.17173\n",
            "epoch 0 step 1661 loss 0.0121\n",
            "epoch 0 step 1662 loss 0.40733\n",
            "epoch 0 step 1663 loss 0.12994\n",
            "epoch 0 step 1664 loss 0.06724\n",
            "epoch 0 step 1665 loss 0.14674\n",
            "epoch 0 step 1666 loss 0.34135\n",
            "epoch 0 step 1667 loss 0.01085\n",
            "epoch 0 step 1668 loss 0.37565\n",
            "epoch 0 step 1669 loss 0.00492\n",
            "epoch 0 step 1670 loss 0.03013\n",
            "epoch 0 step 1671 loss 0.00586\n",
            "epoch 0 step 1672 loss 0.33007\n",
            "epoch 0 step 1673 loss 0.05908\n",
            "epoch 0 step 1674 loss 0.00857\n",
            "epoch 0 step 1675 loss 0.27571\n",
            "epoch 0 step 1676 loss 0.04509\n",
            "epoch 0 step 1677 loss 0.02337\n",
            "epoch 0 step 1678 loss 0.028\n",
            "epoch 0 step 1679 loss 0.02511\n",
            "epoch 0 step 1680 loss 0.05763\n",
            "epoch 0 step 1681 loss 0.03634\n",
            "epoch 0 step 1682 loss 0.35683\n",
            "epoch 0 step 1683 loss 0.08254\n",
            "epoch 0 step 1684 loss 0.10919\n",
            "epoch 0 step 1685 loss 0.09879\n",
            "epoch 0 step 1686 loss 0.28467\n",
            "epoch 0 step 1687 loss 0.00129\n",
            "epoch 0 step 1688 loss 0.10911\n",
            "epoch 0 step 1689 loss 0.01413\n",
            "epoch 0 step 1690 loss 0.27515\n",
            "epoch 0 step 1691 loss 0.11854\n",
            "epoch 0 step 1692 loss 0.06273\n",
            "epoch 0 step 1693 loss 0.00325\n",
            "epoch 0 step 1694 loss 0.47413\n",
            "epoch 0 step 1695 loss 0.37393\n",
            "epoch 0 step 1696 loss 0.00194\n",
            "epoch 0 step 1697 loss 0.02294\n",
            "epoch 0 step 1698 loss 0.001\n",
            "epoch 0 step 1699 loss 0.30945\n",
            "epoch 0 step 1700 loss 0.30405\n",
            "epoch 0 step 1701 loss 0.11186\n",
            "epoch 0 step 1702 loss 0.0582\n",
            "epoch 0 step 1703 loss 0.00454\n",
            "epoch 0 step 1704 loss 0.01811\n",
            "epoch 0 step 1705 loss 0.42952\n",
            "epoch 0 step 1706 loss 0.15288\n",
            "epoch 0 step 1707 loss 0.08053\n",
            "epoch 0 step 1708 loss 0.09506\n",
            "epoch 0 step 1709 loss 0.00437\n",
            "epoch 0 step 1710 loss 0.01356\n",
            "epoch 0 step 1711 loss 0.02906\n",
            "epoch 0 step 1712 loss 0.09553\n",
            "epoch 0 step 1713 loss 0.20021\n",
            "epoch 0 step 1714 loss 0.1527\n",
            "epoch 0 step 1715 loss 0.41071\n",
            "epoch 0 step 1716 loss 0.23885\n",
            "epoch 0 step 1717 loss 0.00019\n",
            "epoch 0 step 1718 loss 0.01367\n",
            "epoch 0 step 1719 loss 0.20099\n",
            "epoch 0 step 1720 loss 0.011\n",
            "epoch 0 step 1721 loss 0.00144\n",
            "epoch 0 step 1722 loss 0.05358\n",
            "epoch 0 step 1723 loss 0.37847\n",
            "epoch 0 step 1724 loss 0.00857\n",
            "epoch 0 step 1725 loss 0.00588\n",
            "epoch 0 step 1726 loss 0.66981\n",
            "epoch 0 step 1727 loss 0.29594\n",
            "epoch 0 step 1728 loss 0.01346\n",
            "epoch 0 step 1729 loss 0.04213\n",
            "epoch 0 step 1730 loss 0.27759\n",
            "epoch 0 step 1731 loss 0.01703\n",
            "epoch 0 step 1732 loss 0.00073\n",
            "epoch 0 step 1733 loss 0.14468\n",
            "epoch 0 step 1734 loss 0.00308\n",
            "epoch 0 step 1735 loss 0.08796\n",
            "epoch 0 step 1736 loss 0.44352\n",
            "epoch 0 step 1737 loss 0.56204\n",
            "epoch 0 step 1738 loss 0.54367\n",
            "epoch 0 step 1739 loss 0.0297\n",
            "epoch 0 step 1740 loss 0.10707\n",
            "epoch 0 step 1741 loss 0.00356\n",
            "epoch 0 step 1742 loss 0.19727\n",
            "epoch 0 step 1743 loss 0.00506\n",
            "epoch 0 step 1744 loss 0.00452\n",
            "epoch 0 step 1745 loss 0.05183\n",
            "epoch 0 step 1746 loss 0.00927\n",
            "epoch 0 step 1747 loss 0.41699\n",
            "epoch 0 step 1748 loss 0.12137\n",
            "epoch 0 step 1749 loss 0.22514\n",
            "epoch 0 step 1750 loss 0.00036\n",
            "epoch 0 step 1751 loss 0.02547\n",
            "epoch 0 step 1752 loss 0.10633\n",
            "epoch 0 step 1753 loss 0.0308\n",
            "epoch 0 step 1754 loss 0.04799\n",
            "epoch 0 step 1755 loss 0.06238\n",
            "epoch 0 step 1756 loss 0.00992\n",
            "epoch 0 step 1757 loss 0.0011\n",
            "epoch 0 step 1758 loss 0.0212\n",
            "epoch 0 step 1759 loss 0.00023\n",
            "epoch 0 step 1760 loss 0.28634\n",
            "epoch 0 step 1761 loss 0.15705\n",
            "epoch 0 step 1762 loss 0.04458\n",
            "epoch 0 step 1763 loss 0.00319\n",
            "epoch 0 step 1764 loss 0.12614\n",
            "epoch 0 step 1765 loss 0.06181\n",
            "epoch 0 step 1766 loss 0.00266\n",
            "epoch 0 step 1767 loss 0.00598\n",
            "epoch 0 step 1768 loss 0.0499\n",
            "epoch 0 step 1769 loss 0.27836\n",
            "epoch 0 step 1770 loss 0.12543\n",
            "epoch 0 step 1771 loss 0.02071\n",
            "epoch 0 step 1772 loss 0.01588\n",
            "epoch 0 step 1773 loss 0.19684\n",
            "epoch 0 step 1774 loss 0.1325\n",
            "epoch 0 step 1775 loss 0.00097\n",
            "epoch 0 step 1776 loss 0.34176\n",
            "epoch 0 step 1777 loss 0.36863\n",
            "epoch 0 step 1778 loss 0.0837\n",
            "epoch 0 step 1779 loss 0.18303\n",
            "epoch 0 step 1780 loss 0.00057\n",
            "epoch 0 step 1781 loss 0.10616\n",
            "epoch 0 step 1782 loss 0.07964\n",
            "epoch 0 step 1783 loss 0.16632\n",
            "epoch 0 step 1784 loss 0.385\n",
            "epoch 0 step 1785 loss 0.29898\n",
            "epoch 0 step 1786 loss 0.13756\n",
            "epoch 0 step 1787 loss 0.07492\n",
            "epoch 0 step 1788 loss 0.51362\n",
            "epoch 0 step 1789 loss 0.18144\n",
            "epoch 0 step 1790 loss 0.62229\n",
            "epoch 0 step 1791 loss 0.00264\n",
            "epoch 0 step 1792 loss 0.04689\n",
            "epoch 0 step 1793 loss 0.17091\n",
            "epoch 0 step 1794 loss 0.19271\n",
            "epoch 0 step 1795 loss 0.01836\n",
            "epoch 0 step 1796 loss 0.04203\n",
            "epoch 0 step 1797 loss 0.32485\n",
            "epoch 0 step 1798 loss 0.02122\n",
            "epoch 0 step 1799 loss 0.11355\n",
            "epoch 0 step 1800 loss 0.20809\n",
            "epoch 0 step 1801 loss 0.01901\n",
            "epoch 0 step 1802 loss 0.00248\n",
            "epoch 0 step 1803 loss 0.05503\n",
            "epoch 0 step 1804 loss 0.00211\n",
            "epoch 0 step 1805 loss 0.14118\n",
            "epoch 0 step 1806 loss 0.02491\n",
            "epoch 0 step 1807 loss 0.30748\n",
            "epoch 0 step 1808 loss 0.01309\n",
            "epoch 0 step 1809 loss 0.0732\n",
            "epoch 0 step 1810 loss 0.1316\n",
            "epoch 0 step 1811 loss 0.23107\n",
            "epoch 0 step 1812 loss 0.06064\n",
            "epoch 0 step 1813 loss 0.15027\n",
            "epoch 0 step 1814 loss 0.04577\n",
            "epoch 0 step 1815 loss 0.00651\n",
            "epoch 0 step 1816 loss 0.05145\n",
            "epoch 0 step 1817 loss 0.02263\n",
            "epoch 0 step 1818 loss 0.1818\n",
            "epoch 0 step 1819 loss 0.04824\n",
            "epoch 0 step 1820 loss 0.00056\n",
            "epoch 0 step 1821 loss 0.14025\n",
            "epoch 0 step 1822 loss 0.18641\n",
            "epoch 0 step 1823 loss 0.10148\n",
            "epoch 0 step 1824 loss 0.14334\n",
            "epoch 0 step 1825 loss 0.14114\n",
            "epoch 0 step 1826 loss 0.15546\n",
            "epoch 0 step 1827 loss 0.14127\n",
            "epoch 0 step 1828 loss 0.00076\n",
            "epoch 0 step 1829 loss 0.07355\n",
            "epoch 0 step 1830 loss 0.2924\n",
            "epoch 0 step 1831 loss 0.12007\n",
            "epoch 0 step 1832 loss 0.41735\n",
            "epoch 0 step 1833 loss 0.00066\n",
            "epoch 0 step 1834 loss 0.25548\n",
            "epoch 0 step 1835 loss 0.08093\n",
            "epoch 0 step 1836 loss 0.28992\n",
            "epoch 0 step 1837 loss 0.11678\n",
            "epoch 0 step 1838 loss 0.00788\n",
            "epoch 0 step 1839 loss 0.00335\n",
            "epoch 0 step 1840 loss 0.17656\n",
            "epoch 0 step 1841 loss 0.01797\n",
            "epoch 0 step 1842 loss 0.10901\n",
            "epoch 0 step 1843 loss 0.02777\n",
            "epoch 0 step 1844 loss 0.0389\n",
            "epoch 0 step 1845 loss 0.20622\n",
            "epoch 0 step 1846 loss 0.18633\n",
            "epoch 0 step 1847 loss 0.03587\n",
            "epoch 0 step 1848 loss 0.24859\n",
            "epoch 0 step 1849 loss 0.02635\n",
            "epoch 0 step 1850 loss 0.28923\n",
            "epoch 0 step 1851 loss 0.0291\n",
            "epoch 0 step 1852 loss 0.00285\n",
            "epoch 0 step 1853 loss 0.23945\n",
            "epoch 0 step 1854 loss 0.00262\n",
            "epoch 0 step 1855 loss 0.01083\n",
            "epoch 0 step 1856 loss 0.04496\n",
            "epoch 0 step 1857 loss 0.08556\n",
            "epoch 0 step 1858 loss 0.09558\n",
            "epoch 0 step 1859 loss 0.05557\n",
            "epoch 0 step 1860 loss 0.28768\n",
            "epoch 0 step 1861 loss 0.23527\n",
            "epoch 0 step 1862 loss 0.40661\n",
            "epoch 0 step 1863 loss 0.00043\n",
            "epoch 0 step 1864 loss 0.00805\n",
            "epoch 0 step 1865 loss 0.00433\n",
            "epoch 0 step 1866 loss 6e-05\n",
            "epoch 0 step 1867 loss 0.25828\n",
            "epoch 0 step 1868 loss 0.00535\n",
            "epoch 0 step 1869 loss 0.02119\n",
            "epoch 0 step 1870 loss 0.47862\n",
            "epoch 0 step 1871 loss 0.12792\n",
            "epoch 0 step 1872 loss 0.12269\n",
            "epoch 0 step 1873 loss 0.0088\n",
            "epoch 0 step 1874 loss 0.04944\n",
            "epoch 0 step 1875 loss 0.14163\n",
            "epoch 0 step 1876 loss 0.03938\n",
            "epoch 0 step 1877 loss 0.06647\n",
            "epoch 0 step 1878 loss 0.04004\n",
            "epoch 0 step 1879 loss 0.01008\n",
            "epoch 0 step 1880 loss 0.17987\n",
            "epoch 0 step 1881 loss 0.22509\n",
            "epoch 0 step 1882 loss 0.06576\n",
            "epoch 0 step 1883 loss 0.02544\n",
            "epoch 0 step 1884 loss 0.17308\n",
            "epoch 0 step 1885 loss 0.39193\n",
            "epoch 0 step 1886 loss 0.00127\n",
            "epoch 0 step 1887 loss 0.0236\n",
            "epoch 0 step 1888 loss 0.38587\n",
            "epoch 0 step 1889 loss 0.05362\n",
            "epoch 0 step 1890 loss 0.16975\n",
            "epoch 0 step 1891 loss 0.02881\n",
            "epoch 0 step 1892 loss 0.00299\n",
            "epoch 0 step 1893 loss 0.00036\n",
            "epoch 0 step 1894 loss 0.00286\n",
            "epoch 0 step 1895 loss 0.36273\n",
            "epoch 0 step 1896 loss 0.00026\n",
            "epoch 0 step 1897 loss 0.07253\n",
            "epoch 0 step 1898 loss 0.05016\n",
            "epoch 0 step 1899 loss 0.04302\n",
            "epoch 0 step 1900 loss 0.07608\n",
            "epoch 0 step 1901 loss 0.11149\n",
            "epoch 0 step 1902 loss 0.16848\n",
            "epoch 0 step 1903 loss 0.18311\n",
            "epoch 0 step 1904 loss 0.11841\n",
            "epoch 0 step 1905 loss 0.2933\n",
            "epoch 0 step 1906 loss 0.00248\n",
            "epoch 0 step 1907 loss 0.08347\n",
            "epoch 0 step 1908 loss 0.08533\n",
            "epoch 0 step 1909 loss 0.33678\n",
            "epoch 0 step 1910 loss 0.40085\n",
            "epoch 0 step 1911 loss 0.04588\n",
            "epoch 0 step 1912 loss 0.01714\n",
            "epoch 0 step 1913 loss 0.01568\n",
            "epoch 0 step 1914 loss 0.58963\n",
            "epoch 0 step 1915 loss 0.31554\n",
            "epoch 0 step 1916 loss 0.14807\n",
            "epoch 0 step 1917 loss 0.10705\n",
            "epoch 0 step 1918 loss 0.10837\n",
            "epoch 0 step 1919 loss 0.24748\n",
            "epoch 0 step 1920 loss 0.0016\n",
            "epoch 0 step 1921 loss 0.31735\n",
            "epoch 0 step 1922 loss 0.17714\n",
            "epoch 0 step 1923 loss 0.30076\n",
            "epoch 0 step 1924 loss 0.00258\n",
            "epoch 0 step 1925 loss 0.00029\n",
            "epoch 0 step 1926 loss 0.01819\n",
            "epoch 0 step 1927 loss 0.11063\n",
            "epoch 0 step 1928 loss 0.0725\n",
            "epoch 0 step 1929 loss 0.07209\n",
            "epoch 0 step 1930 loss 0.01145\n",
            "epoch 0 step 1931 loss 0.06451\n",
            "epoch 0 step 1932 loss 0.14809\n",
            "epoch 0 step 1933 loss 0.01103\n",
            "epoch 0 step 1934 loss 0.07533\n",
            "epoch 0 step 1935 loss 0.1736\n",
            "epoch 0 step 1936 loss 0.01149\n",
            "epoch 0 step 1937 loss 0.19097\n",
            "epoch 0 step 1938 loss 0.00322\n",
            "epoch 0 step 1939 loss 0.00368\n",
            "epoch 0 step 1940 loss 0.07893\n",
            "epoch 0 step 1941 loss 0.43401\n",
            "epoch 0 step 1942 loss 0.42774\n",
            "epoch 0 step 1943 loss 0.31007\n",
            "epoch 0 step 1944 loss 0.18338\n",
            "epoch 0 step 1945 loss 0.17932\n",
            "epoch 0 step 1946 loss 0.2468\n",
            "epoch 0 step 1947 loss 0.0295\n",
            "epoch 0 step 1948 loss 0.22509\n",
            "epoch 0 step 1949 loss 0.00866\n",
            "epoch 0 step 1950 loss 0.45889\n",
            "epoch 0 step 1951 loss 0.30339\n",
            "epoch 0 step 1952 loss 0.01863\n",
            "epoch 0 step 1953 loss 0.09689\n",
            "epoch 0 step 1954 loss 0.20735\n",
            "epoch 0 step 1955 loss 0.08484\n",
            "epoch 0 step 1956 loss 0.13016\n",
            "epoch 0 step 1957 loss 0.00053\n",
            "epoch 0 step 1958 loss 0.01303\n",
            "epoch 0 step 1959 loss 0.33931\n",
            "epoch 0 step 1960 loss 0.3022\n",
            "epoch 0 step 1961 loss 0.00809\n",
            "epoch 0 step 1962 loss 0.0125\n",
            "epoch 0 step 1963 loss 0.26137\n",
            "epoch 0 step 1964 loss 0.02454\n",
            "epoch 0 step 1965 loss 0.1537\n",
            "epoch 0 step 1966 loss 0.26105\n",
            "epoch 0 step 1967 loss 0.24033\n",
            "epoch 0 step 1968 loss 0.01117\n",
            "epoch 0 step 1969 loss 0.01518\n",
            "epoch 0 step 1970 loss 0.0007\n",
            "epoch 0 step 1971 loss 0.01622\n",
            "epoch 0 step 1972 loss 0.15497\n",
            "epoch 0 step 1973 loss 0.00136\n",
            "epoch 0 step 1974 loss 0.25861\n",
            "epoch 0 step 1975 loss 0.04755\n",
            "epoch 0 step 1976 loss 0.26238\n",
            "epoch 0 step 1977 loss 0.02918\n",
            "epoch 0 step 1978 loss 0.12009\n",
            "epoch 0 step 1979 loss 0.12617\n",
            "epoch 0 step 1980 loss 0.04199\n",
            "epoch 0 step 1981 loss 0.18237\n",
            "epoch 0 step 1982 loss 0.14487\n",
            "epoch 0 step 1983 loss 0.17464\n",
            "epoch 0 step 1984 loss 0.44398\n",
            "epoch 0 step 1985 loss 0.09737\n",
            "epoch 0 step 1986 loss 0.01921\n",
            "epoch 0 step 1987 loss 0.10655\n",
            "epoch 0 step 1988 loss 0.00294\n",
            "epoch 0 step 1989 loss 0.10205\n",
            "epoch 0 step 1990 loss 0.06095\n",
            "epoch 0 step 1991 loss 0.05432\n",
            "epoch 0 step 1992 loss 0.12107\n",
            "epoch 0 step 1993 loss 0.00671\n",
            "epoch 0 step 1994 loss 0.15603\n",
            "epoch 0 step 1995 loss 0.19949\n",
            "epoch 0 step 1996 loss 0.19703\n",
            "epoch 0 step 1997 loss 0.00964\n",
            "epoch 0 step 1998 loss 0.28766\n",
            "epoch 0 step 1999 loss 0.01638\n",
            "epoch 0 step 2000 loss 0.01668\n",
            "epoch 0 step 2001 loss 0.32463\n",
            "epoch 0 step 2002 loss 0.14107\n",
            "epoch 0 step 2003 loss 0.20894\n",
            "epoch 0 step 2004 loss 0.19232\n",
            "epoch 0 step 2005 loss 0.12387\n",
            "epoch 0 step 2006 loss 0.07958\n",
            "epoch 0 step 2007 loss 0.21732\n",
            "epoch 0 step 2008 loss 0.01421\n",
            "epoch 0 step 2009 loss 0.08666\n",
            "epoch 0 step 2010 loss 0.00164\n",
            "epoch 0 step 2011 loss 0.45205\n",
            "epoch 0 step 2012 loss 0.35066\n",
            "epoch 0 step 2013 loss 0.00451\n",
            "epoch 0 step 2014 loss 0.14578\n",
            "epoch 0 step 2015 loss 0.00734\n",
            "epoch 0 step 2016 loss 0.08519\n",
            "epoch 0 step 2017 loss 0.10697\n",
            "epoch 0 step 2018 loss 0.20597\n",
            "epoch 0 step 2019 loss 0.00124\n",
            "epoch 0 step 2020 loss 0.29472\n",
            "epoch 0 step 2021 loss 0.08515\n",
            "epoch 0 step 2022 loss 0.16444\n",
            "epoch 0 step 2023 loss 5e-05\n",
            "epoch 0 step 2024 loss 0.0079\n",
            "epoch 0 step 2025 loss 0.02508\n",
            "epoch 0 step 2026 loss 0.37182\n",
            "epoch 0 step 2027 loss 0.00043\n",
            "epoch 0 step 2028 loss 0.08383\n",
            "epoch 0 step 2029 loss 0.30627\n",
            "epoch 0 step 2030 loss 0.00246\n",
            "epoch 0 step 2031 loss 0.39475\n",
            "epoch 0 step 2032 loss 0.01767\n",
            "epoch 0 step 2033 loss 0.07275\n",
            "epoch 0 step 2034 loss 0.01444\n",
            "epoch 0 step 2035 loss 0.13308\n",
            "epoch 0 step 2036 loss 0.09445\n",
            "epoch 0 step 2037 loss 0.00628\n",
            "epoch 0 step 2038 loss 0.43951\n",
            "epoch 0 step 2039 loss 0.00403\n",
            "epoch 0 step 2040 loss 0.135\n",
            "epoch 0 step 2041 loss 0.00284\n",
            "epoch 0 step 2042 loss 0.28698\n",
            "epoch 0 step 2043 loss 0.00013\n",
            "epoch 0 step 2044 loss 0.15037\n",
            "epoch 0 step 2045 loss 0.12746\n",
            "epoch 0 step 2046 loss 0.30615\n",
            "epoch 0 step 2047 loss 0.03957\n",
            "epoch 0 step 2048 loss 0.23094\n",
            "epoch 0 step 2049 loss 0.14299\n",
            "epoch 0 step 2050 loss 0.00073\n",
            "epoch 0 step 2051 loss 0.28126\n",
            "epoch 0 step 2052 loss 0.02238\n",
            "epoch 0 step 2053 loss 0.07948\n",
            "epoch 0 step 2054 loss 0.03103\n",
            "epoch 0 step 2055 loss 0.00597\n",
            "epoch 0 step 2056 loss 0.42738\n",
            "epoch 0 step 2057 loss 0.10786\n",
            "epoch 0 step 2058 loss 0.28104\n",
            "epoch 0 step 2059 loss 0.00034\n",
            "epoch 0 step 2060 loss 0.01244\n",
            "epoch 0 step 2061 loss 0.003\n",
            "epoch 0 step 2062 loss 0.24701\n",
            "epoch 0 step 2063 loss 0.10458\n",
            "epoch 0 step 2064 loss 0.13103\n",
            "epoch 0 step 2065 loss 0.10565\n",
            "epoch 0 step 2066 loss 0.04775\n",
            "epoch 0 step 2067 loss 0.02275\n",
            "epoch 0 step 2068 loss 0.00108\n",
            "epoch 0 step 2069 loss 0.00143\n",
            "epoch 0 step 2070 loss 0.13619\n",
            "epoch 0 step 2071 loss 0.07753\n",
            "epoch 0 step 2072 loss 0.02452\n",
            "epoch 0 step 2073 loss 0.36285\n",
            "epoch 0 step 2074 loss 0.00119\n",
            "epoch 0 step 2075 loss 0.00176\n",
            "epoch 0 step 2076 loss 0.25971\n",
            "epoch 0 step 2077 loss 0.0055\n",
            "epoch 0 step 2078 loss 0.07955\n",
            "epoch 0 step 2079 loss 0.04936\n",
            "epoch 0 step 2080 loss 0.11299\n",
            "epoch 0 step 2081 loss 0.13354\n",
            "epoch 0 step 2082 loss 0.00453\n",
            "epoch 0 step 2083 loss 0.28198\n",
            "epoch 0 step 2084 loss 0.32882\n",
            "epoch 0 step 2085 loss 0.04945\n",
            "epoch 0 step 2086 loss 0.09752\n",
            "epoch 0 step 2087 loss 0.02587\n",
            "epoch 0 step 2088 loss 0.02313\n",
            "epoch 0 step 2089 loss 0.14737\n",
            "epoch 0 step 2090 loss 0.05308\n",
            "epoch 0 step 2091 loss 0.00458\n",
            "epoch 0 step 2092 loss 0.00381\n",
            "epoch 0 step 2093 loss 0.02225\n",
            "epoch 0 step 2094 loss 0.07916\n",
            "epoch 0 step 2095 loss 0.00141\n",
            "epoch 0 step 2096 loss 0.14525\n",
            "epoch 0 step 2097 loss 0.33901\n",
            "epoch 0 step 2098 loss 0.01142\n",
            "epoch 0 step 2099 loss 0.34614\n",
            "epoch 0 step 2100 loss 0.32778\n",
            "epoch 0 step 2101 loss 0.00105\n",
            "epoch 0 step 2102 loss 0.00609\n",
            "epoch 0 step 2103 loss 0.1833\n",
            "epoch 0 step 2104 loss 0.06518\n",
            "epoch 0 step 2105 loss 0.14001\n",
            "epoch 0 step 2106 loss 0.86653\n",
            "epoch 0 step 2107 loss 0.10095\n",
            "epoch 0 step 2108 loss 0.08157\n",
            "epoch 0 step 2109 loss 0.20137\n",
            "epoch 0 step 2110 loss 0.04637\n",
            "epoch 0 step 2111 loss 0.04858\n",
            "epoch 0 step 2112 loss 0.49407\n",
            "epoch 0 step 2113 loss 0.08533\n",
            "epoch 0 step 2114 loss 0.05571\n",
            "epoch 0 step 2115 loss 0.55914\n",
            "epoch 0 step 2116 loss 0.0156\n",
            "epoch 0 step 2117 loss 0.00619\n",
            "epoch 0 step 2118 loss 0.36416\n",
            "epoch 0 step 2119 loss 0.2292\n",
            "epoch 0 step 2120 loss 0.00116\n",
            "epoch 0 step 2121 loss 0.00755\n",
            "epoch 0 step 2122 loss 0.26926\n",
            "epoch 0 step 2123 loss 0.03089\n",
            "epoch 0 step 2124 loss 0.15912\n",
            "epoch 0 step 2125 loss 0.09404\n",
            "epoch 0 step 2126 loss 0.25119\n",
            "epoch 0 step 2127 loss 0.00661\n",
            "epoch 0 step 2128 loss 0.26602\n",
            "epoch 0 step 2129 loss 0.2546\n",
            "epoch 0 step 2130 loss 0.19106\n",
            "epoch 0 step 2131 loss 0.15468\n",
            "epoch 0 step 2132 loss 0.00204\n",
            "epoch 0 step 2133 loss 0.08686\n",
            "epoch 0 step 2134 loss 0.00786\n",
            "epoch 0 step 2135 loss 0.21402\n",
            "epoch 0 step 2136 loss 0.109\n",
            "epoch 0 step 2137 loss 0.00246\n",
            "epoch 0 step 2138 loss 0.15066\n",
            "epoch 0 step 2139 loss 0.30325\n",
            "epoch 0 step 2140 loss 0.0652\n",
            "epoch 0 step 2141 loss 0.07642\n",
            "epoch 0 step 2142 loss 0.14178\n",
            "epoch 0 step 2143 loss 0.09095\n",
            "epoch 0 step 2144 loss 0.12067\n",
            "epoch 0 step 2145 loss 0.55743\n",
            "epoch 0 step 2146 loss 0.0135\n",
            "epoch 0 step 2147 loss 0.01815\n",
            "epoch 0 step 2148 loss 0.03908\n",
            "epoch 0 step 2149 loss 0.00389\n",
            "epoch 0 step 2150 loss 0.00019\n",
            "epoch 0 step 2151 loss 0.01234\n",
            "epoch 0 step 2152 loss 0.14277\n",
            "epoch 0 step 2153 loss 0.01132\n",
            "epoch 0 step 2154 loss 0.17906\n",
            "epoch 0 step 2155 loss 0.01612\n",
            "epoch 0 step 2156 loss 0.0111\n",
            "epoch 0 step 2157 loss 0.60252\n",
            "epoch 0 step 2158 loss 0.02911\n",
            "epoch 0 step 2159 loss 0.00938\n",
            "epoch 0 step 2160 loss 0.02935\n",
            "epoch 0 step 2161 loss 0.15059\n",
            "epoch 0 step 2162 loss 0.38384\n",
            "epoch 0 step 2163 loss 0.01619\n",
            "epoch 0 step 2164 loss 0.05865\n",
            "epoch 0 step 2165 loss 0.21756\n",
            "epoch 0 step 2166 loss 0.00034\n",
            "epoch 0 step 2167 loss 0.00011\n",
            "epoch 0 step 2168 loss 0.28638\n",
            "epoch 0 step 2169 loss 0.04915\n",
            "epoch 0 step 2170 loss 0.2652\n",
            "epoch 0 step 2171 loss 0.00741\n",
            "epoch 0 step 2172 loss 0.04308\n",
            "epoch 0 step 2173 loss 0.0013\n",
            "epoch 0 step 2174 loss 0.11726\n",
            "epoch 0 step 2175 loss 0.05465\n",
            "epoch 0 step 2176 loss 0.08959\n",
            "epoch 0 step 2177 loss 0.03126\n",
            "epoch 0 step 2178 loss 0.03769\n",
            "epoch 0 step 2179 loss 0.02976\n",
            "epoch 0 step 2180 loss 0.05645\n",
            "epoch 0 step 2181 loss 0.01054\n",
            "epoch 0 step 2182 loss 0.00064\n",
            "epoch 0 step 2183 loss 0.1525\n",
            "epoch 0 step 2184 loss 0.11354\n",
            "epoch 0 step 2185 loss 0.05845\n",
            "epoch 0 step 2186 loss 0.07475\n",
            "epoch 0 step 2187 loss 0.0555\n",
            "epoch 0 step 2188 loss 0.00662\n",
            "epoch 0 step 2189 loss 0.02976\n",
            "epoch 0 step 2190 loss 0.06963\n",
            "epoch 0 step 2191 loss 0.09389\n",
            "epoch 0 step 2192 loss 0.23176\n",
            "epoch 0 step 2193 loss 7e-05\n",
            "epoch 0 step 2194 loss 0.03794\n",
            "epoch 0 step 2195 loss 0.04853\n",
            "epoch 0 step 2196 loss 0.09053\n",
            "epoch 0 step 2197 loss 0.00087\n",
            "epoch 0 step 2198 loss 0.00028\n",
            "epoch 0 step 2199 loss 0.19044\n",
            "epoch 0 step 2200 loss 0.03102\n",
            "epoch 0 step 2201 loss 0.00014\n",
            "epoch 0 step 2202 loss 0.57186\n",
            "epoch 0 step 2203 loss 0.24845\n",
            "epoch 0 step 2204 loss 0.0031\n",
            "epoch 0 step 2205 loss 0.36391\n",
            "epoch 0 step 2206 loss 0.10497\n",
            "epoch 0 step 2207 loss 0.00532\n",
            "epoch 0 step 2208 loss 0.29911\n",
            "epoch 0 step 2209 loss 0.00091\n",
            "epoch 0 step 2210 loss 0.1641\n",
            "epoch 0 step 2211 loss 0.02021\n",
            "epoch 0 step 2212 loss 0.27265\n",
            "epoch 0 step 2213 loss 0.00018\n",
            "epoch 0 step 2214 loss 0.13424\n",
            "epoch 0 step 2215 loss 0.00764\n",
            "epoch 0 step 2216 loss 0.01781\n",
            "epoch 0 step 2217 loss 0.00066\n",
            "epoch 0 step 2218 loss 0.00059\n",
            "epoch 0 step 2219 loss 0.02369\n",
            "epoch 0 step 2220 loss 0.15298\n",
            "epoch 0 step 2221 loss 0.00075\n",
            "epoch 0 step 2222 loss 0.02664\n",
            "epoch 0 step 2223 loss 0.33181\n",
            "epoch 0 step 2224 loss 0.00388\n",
            "epoch 0 step 2225 loss 0.52538\n",
            "epoch 0 step 2226 loss 0.13185\n",
            "epoch 0 step 2227 loss 0.03427\n",
            "epoch 0 step 2228 loss 0.02849\n",
            "epoch 0 step 2229 loss 0.32768\n",
            "epoch 0 step 2230 loss 0.10832\n",
            "epoch 0 step 2231 loss 0.24516\n",
            "epoch 0 step 2232 loss 0.44299\n",
            "epoch 0 step 2233 loss 0.27075\n",
            "epoch 0 step 2234 loss 0.00712\n",
            "epoch 0 step 2235 loss 2e-05\n",
            "epoch 0 step 2236 loss 0.05162\n",
            "epoch 0 step 2237 loss 0.00636\n",
            "epoch 0 step 2238 loss 0.17383\n",
            "epoch 0 step 2239 loss 0.24859\n",
            "epoch 0 step 2240 loss 0.17144\n",
            "epoch 0 step 2241 loss 0.00796\n",
            "epoch 0 step 2242 loss 0.12062\n",
            "epoch 0 step 2243 loss 0.35114\n",
            "epoch 0 step 2244 loss 0.12783\n",
            "epoch 0 step 2245 loss 0.00066\n",
            "epoch 0 step 2246 loss 0.28313\n",
            "epoch 0 step 2247 loss 0.01431\n",
            "epoch 0 step 2248 loss 0.20567\n",
            "epoch 0 step 2249 loss 0.02231\n",
            "epoch 0 step 2250 loss 0.0051\n",
            "epoch 0 step 2251 loss 0.22325\n",
            "epoch 0 step 2252 loss 0.01805\n",
            "epoch 0 step 2253 loss 0.06385\n",
            "epoch 0 step 2254 loss 0.0\n",
            "epoch 0 step 2255 loss 0.03649\n",
            "epoch 0 step 2256 loss 0.06763\n",
            "epoch 0 step 2257 loss 0.01141\n",
            "epoch 0 step 2258 loss 0.2839\n",
            "epoch 0 step 2259 loss 0.55323\n",
            "epoch 0 step 2260 loss 0.34822\n",
            "epoch 0 step 2261 loss 0.52931\n",
            "epoch 0 step 2262 loss 0.18581\n",
            "epoch 0 step 2263 loss 1.16224\n",
            "epoch 0 step 2264 loss 0.0804\n",
            "epoch 0 step 2265 loss 0.00127\n",
            "epoch 0 step 2266 loss 0.11757\n",
            "epoch 0 step 2267 loss 0.00361\n",
            "epoch 0 step 2268 loss 0.02389\n",
            "epoch 0 step 2269 loss 0.30973\n",
            "epoch 0 step 2270 loss 0.00075\n",
            "epoch 0 step 2271 loss 0.00209\n",
            "epoch 0 step 2272 loss 0.12629\n",
            "epoch 0 step 2273 loss 0.03597\n",
            "epoch 0 step 2274 loss 0.19846\n",
            "epoch 0 step 2275 loss 0.13933\n",
            "epoch 0 step 2276 loss 0.0637\n",
            "epoch 0 step 2277 loss 0.1177\n",
            "epoch 0 step 2278 loss 0.17666\n",
            "epoch 0 step 2279 loss 0.03142\n",
            "epoch 0 step 2280 loss 0.22491\n",
            "epoch 0 step 2281 loss 0.05313\n",
            "epoch 0 step 2282 loss 0.00082\n",
            "epoch 0 step 2283 loss 0.00604\n",
            "epoch 0 step 2284 loss 0.1441\n",
            "epoch 0 step 2285 loss 0.29995\n",
            "epoch 0 step 2286 loss 0.14007\n",
            "epoch 0 step 2287 loss 0.02095\n",
            "epoch 0 step 2288 loss 0.00073\n",
            "epoch 0 step 2289 loss 0.04966\n",
            "epoch 0 step 2290 loss 0.01647\n",
            "epoch 0 step 2291 loss 0.26505\n",
            "epoch 0 step 2292 loss 0.10372\n",
            "epoch 0 step 2293 loss 0.00913\n",
            "epoch 0 step 2294 loss 0.09627\n",
            "epoch 0 step 2295 loss 0.30286\n",
            "epoch 0 step 2296 loss 0.26852\n",
            "epoch 0 step 2297 loss 0.00525\n",
            "epoch 0 step 2298 loss 0.26378\n",
            "epoch 0 step 2299 loss 0.00183\n",
            "epoch 0 step 2300 loss 0.44624\n",
            "epoch 0 step 2301 loss 0.09006\n",
            "epoch 0 step 2302 loss 0.02876\n",
            "epoch 0 step 2303 loss 0.252\n",
            "epoch 0 step 2304 loss 0.15873\n",
            "epoch 0 step 2305 loss 0.04445\n",
            "epoch 0 step 2306 loss 0.16727\n",
            "epoch 0 step 2307 loss 0.09952\n",
            "epoch 0 step 2308 loss 0.71891\n",
            "epoch 0 step 2309 loss 0.38232\n",
            "epoch 0 step 2310 loss 0.20182\n",
            "epoch 0 step 2311 loss 0.13542\n",
            "epoch 0 step 2312 loss 0.00776\n",
            "epoch 0 step 2313 loss 0.00503\n",
            "epoch 0 step 2314 loss 0.09071\n",
            "epoch 0 step 2315 loss 0.00089\n",
            "epoch 0 step 2316 loss 0.002\n",
            "epoch 0 step 2317 loss 0.10866\n",
            "epoch 0 step 2318 loss 0.0142\n",
            "epoch 0 step 2319 loss 0.10086\n",
            "epoch 0 step 2320 loss 0.1432\n",
            "epoch 0 step 2321 loss 0.006\n",
            "epoch 0 step 2322 loss 0.46884\n",
            "epoch 0 step 2323 loss 0.45769\n",
            "epoch 0 step 2324 loss 0.24873\n",
            "epoch 0 step 2325 loss 0.10582\n",
            "epoch 0 step 2326 loss 0.00804\n",
            "epoch 0 step 2327 loss 0.11956\n",
            "epoch 0 step 2328 loss 0.1067\n",
            "epoch 0 step 2329 loss 0.08248\n",
            "epoch 0 step 2330 loss 0.02104\n",
            "epoch 0 step 2331 loss 0.21401\n",
            "epoch 0 step 2332 loss 0.00165\n",
            "epoch 0 step 2333 loss 0.16761\n",
            "epoch 0 step 2334 loss 0.15955\n",
            "epoch 0 step 2335 loss 0.3448\n",
            "epoch 0 step 2336 loss 0.00427\n",
            "epoch 0 step 2337 loss 0.00096\n",
            "epoch 0 step 2338 loss 0.14864\n",
            "epoch 0 step 2339 loss 0.00467\n",
            "epoch 0 step 2340 loss 0.00317\n",
            "epoch 0 step 2341 loss 0.0024\n",
            "epoch 0 step 2342 loss 0.01197\n",
            "epoch 0 step 2343 loss 0.10553\n",
            "epoch 0 step 2344 loss 0.14574\n",
            "epoch 0 step 2345 loss 0.16498\n",
            "epoch 0 step 2346 loss 0.00168\n",
            "epoch 0 step 2347 loss 0.12482\n",
            "epoch 0 step 2348 loss 0.36505\n",
            "epoch 0 step 2349 loss 0.13412\n",
            "epoch 0 step 2350 loss 0.2575\n",
            "epoch 0 step 2351 loss 0.20217\n",
            "epoch 0 step 2352 loss 0.1678\n",
            "epoch 0 step 2353 loss 0.0742\n",
            "epoch 0 step 2354 loss 0.38746\n",
            "epoch 0 step 2355 loss 0.37575\n",
            "epoch 0 step 2356 loss 0.06343\n",
            "epoch 0 step 2357 loss 0.00193\n",
            "epoch 0 step 2358 loss 0.09552\n",
            "epoch 0 step 2359 loss 0.05715\n",
            "epoch 0 step 2360 loss 0.00607\n",
            "epoch 0 step 2361 loss 0.11816\n",
            "epoch 0 step 2362 loss 0.01585\n",
            "epoch 0 step 2363 loss 0.49198\n",
            "epoch 0 step 2364 loss 0.00249\n",
            "epoch 0 step 2365 loss 0.01271\n",
            "epoch 0 step 2366 loss 0.33352\n",
            "epoch 0 step 2367 loss 0.00402\n",
            "epoch 0 step 2368 loss 0.06423\n",
            "epoch 0 step 2369 loss 0.02318\n",
            "epoch 0 step 2370 loss 0.17967\n",
            "epoch 0 step 2371 loss 0.15649\n",
            "epoch 0 step 2372 loss 0.29419\n",
            "epoch 0 step 2373 loss 0.00017\n",
            "epoch 0 step 2374 loss 0.05016\n",
            "epoch 0 step 2375 loss 0.07231\n",
            "epoch 0 step 2376 loss 0.21783\n",
            "epoch 0 step 2377 loss 0.45046\n",
            "epoch 0 step 2378 loss 0.11711\n",
            "epoch 0 step 2379 loss 0.0747\n",
            "epoch 0 step 2380 loss 0.00063\n",
            "epoch 0 step 2381 loss 0.15325\n",
            "epoch 0 step 2382 loss 0.00067\n",
            "epoch 0 step 2383 loss 0.31408\n",
            "epoch 0 step 2384 loss 0.03835\n",
            "epoch 0 step 2385 loss 0.05864\n",
            "epoch 0 step 2386 loss 0.69845\n",
            "epoch 0 step 2387 loss 0.00158\n",
            "epoch 0 step 2388 loss 0.00091\n",
            "epoch 0 step 2389 loss 0.17985\n",
            "epoch 0 step 2390 loss 0.00862\n",
            "epoch 0 step 2391 loss 0.32728\n",
            "epoch 0 step 2392 loss 0.07446\n",
            "epoch 0 step 2393 loss 0.00893\n",
            "epoch 0 step 2394 loss 0.02379\n",
            "epoch 0 step 2395 loss 0.07223\n",
            "epoch 0 step 2396 loss 0.11049\n",
            "epoch 0 step 2397 loss 0.18006\n",
            "epoch 0 step 2398 loss 0.09888\n",
            "epoch 0 step 2399 loss 0.13736\n",
            "epoch 0 step 2400 loss 0.15271\n",
            "epoch 0 step 2401 loss 0.0575\n",
            "epoch 0 step 2402 loss 0.0656\n",
            "epoch 0 step 2403 loss 0.21607\n",
            "epoch 0 step 2404 loss 0.32481\n",
            "epoch 0 step 2405 loss 0.276\n",
            "epoch 0 step 2406 loss 0.25772\n",
            "epoch 0 step 2407 loss 0.07598\n",
            "epoch 0 step 2408 loss 0.00727\n",
            "epoch 0 step 2409 loss 0.05688\n",
            "epoch 0 step 2410 loss 0.01561\n",
            "epoch 0 step 2411 loss 0.06572\n",
            "epoch 0 step 2412 loss 0.12995\n",
            "epoch 0 step 2413 loss 0.00175\n",
            "epoch 0 step 2414 loss 0.33129\n",
            "epoch 0 step 2415 loss 0.07071\n",
            "epoch 0 step 2416 loss 0.06832\n",
            "epoch 0 step 2417 loss 0.32704\n",
            "epoch 0 step 2418 loss 0.00159\n",
            "epoch 0 step 2419 loss 0.00333\n",
            "epoch 0 step 2420 loss 0.01716\n",
            "epoch 0 step 2421 loss 0.00098\n",
            "epoch 0 step 2422 loss 0.21505\n",
            "epoch 0 step 2423 loss 0.01886\n",
            "epoch 0 step 2424 loss 0.00113\n",
            "epoch 0 step 2425 loss 0.00017\n",
            "epoch 0 step 2426 loss 0.52321\n",
            "epoch 0 step 2427 loss 0.09885\n",
            "epoch 0 step 2428 loss 9e-05\n",
            "epoch 0 step 2429 loss 0.13859"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/CodeBERT/CodeBERT/code2nl/run.py --do_test --model_type roberta --model_name_or_path microsoft/codebert-base --load_model_path data/model/java/checkpoint-best-bleu/pytorch_model.bin --dev_filename /content/data/code2nl/CodeSearchNet/java/valid.jsonl --test_filename /content/data/code2nl/CodeSearchNet/java/test.jsonl --output_dir model/java --max_source_length 256 --max_target_length 128 --beam_size 10 --eval_batch_size 1000"
      ],
      "metadata": {
        "id": "lSlIX6t-JNAc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}