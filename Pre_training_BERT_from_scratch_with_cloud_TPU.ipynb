{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bye23mj/BERT/blob/main/Pre_training_BERT_from_scratch_with_cloud_TPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2XB_l-Hgzq_"
      },
      "source": [
        "# Pre-training BERT from scratch with cloud TPU"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list | grep tensorflow\n",
        "!pip install tensorflow==1.15.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEYyBEOvuWEF",
        "outputId": "ff54deef-50cb-4fe5-e26d-675f63ae500e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensorflow                    2.9.2\n",
            "tensorflow-datasets           4.6.0\n",
            "tensorflow-estimator          2.9.0\n",
            "tensorflow-gcs-config         2.9.1\n",
            "tensorflow-hub                0.12.0\n",
            "tensorflow-io-gcs-filesystem  0.27.0\n",
            "tensorflow-metadata           1.10.0\n",
            "tensorflow-probability        0.16.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==1.15.2\n",
            "  Downloading tensorflow-1.15.2-cp37-cp37m-manylinux2010_x86_64.whl (110.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 110.5 MB 86.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.21.6)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (2.0.1)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 63.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.15.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 55.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.2.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.17.3)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.14.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.3.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.8.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.50.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.37.1)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.2) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.4.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (4.1.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15.2) (1.5.2)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=b3801639dd8746af132d1ccd97f649d260caafcc7b138e0faa3e4fd06134135b\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.2\n",
            "    Uninstalling tensorflow-2.9.2:\n",
            "      Successfully uninstalled tensorflow-2.9.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.2 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.2 tensorflow-estimator-1.15.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list | grep tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0klPds9uaCl",
        "outputId": "698d1272-0e4a-436d-ab55-92a6bcc3f877"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensorflow                    1.15.2\n",
            "tensorflow-datasets           4.6.0\n",
            "tensorflow-estimator          1.15.1\n",
            "tensorflow-gcs-config         2.9.1\n",
            "tensorflow-hub                0.12.0\n",
            "tensorflow-io-gcs-filesystem  0.27.0\n",
            "tensorflow-metadata           1.10.0\n",
            "tensorflow-probability        0.16.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZPgpRl5g2e2"
      },
      "source": [
        "In this experiment, we will be pre-training a state-of-the-art Natural Language Understanding model [BERT](https://arxiv.org/abs/1810.04805.) on arbitrary text data using Google Cloud infrastructure.\n",
        "\n",
        "This guide covers all stages of the procedure, including:\n",
        "\n",
        "1. Setting up the training environment\n",
        "2. Downloading raw text data\n",
        "3. Preprocessing text data\n",
        "4. Learning a new vocabulary\n",
        "5. Creating sharded pre-training data\n",
        "6. Setting up GCS storage for data and model\n",
        "7. Training the model on a cloud TPU\n",
        "\n",
        "For persistent storage of training data and model, you will require a Google Cloud Storage bucket. \n",
        "Please follow the [Google Cloud TPU quickstart](https://cloud.google.com/tpu/docs/quickstart) to create a GCP account and GCS bucket. New Google Cloud users have [$300 free credit](https://cloud.google.com/free/) to get started with any GCP product. \n",
        "\n",
        "Steps 1-5 of this tutorial can be run without a GCS bucket for demonstration purposes. In that case, however, you will not be able to train the model.\n",
        "\n",
        "**Note** \n",
        "The only parameter you *really have to set* is BUCKET_NAME in steps 5 and 6. Everything else has default values which should work for most use-cases.\n",
        "\n",
        "**Note** \n",
        "Pre-training a BERT-Base model on a TPUv2 will take about 54 hours. Google Colab is not designed for executing such long-running jobs and will interrupt the training process every 8 hours or so. For uninterrupted training, consider using a preemptible TPUv2 instance. \n",
        "\n",
        "That said, at the time of writing (09.05.2019), with a Colab TPU, pre-training a BERT model from scratch can be achieved at a negligible cost of storing the said model and data in GCS  (~1 USD).\n",
        "\n",
        "Now, let's get to business."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODimOhBR05yR"
      },
      "source": [
        "MIT License\n",
        "\n",
        "Copyright (c) [2019] [Antyukhov Denis Olegovich]\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjad5jsr9YaM"
      },
      "source": [
        "## Step 1: setting up training environment\n",
        "First and foremost, we get the packages required to train the model. \n",
        "The Jupyter environment allows executing bash commands directly from the notebook by using an exclamation mark ‘!’. I will be exploiting this approach to make use of several other bash commands throughout the experiment.\n",
        "\n",
        "Now, let’s import the packages and authorize ourselves in Google Cloud."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S4CiOh3RzFW",
        "outputId": "b69c6004-94d6-4b10-a54c-5872e3395df4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install sentencepiece\n",
        "!git clone https://github.com/google-research/bert\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import nltk\n",
        "import random\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "import sentencepiece as spm\n",
        "\n",
        "from glob import glob\n",
        "from google.colab import auth, drive\n",
        "from tensorflow.keras.utils import Progbar\n",
        "\n",
        "sys.path.append(\"bert\")\n",
        "\n",
        "from bert import modeling, optimization, tokenization\n",
        "from bert.run_pretraining import input_fn_builder, model_fn_builder\n",
        "\n",
        "auth.authenticate_user()\n",
        "  \n",
        "# configure logging\n",
        "log = logging.getLogger('tensorflow')\n",
        "log.setLevel(logging.INFO)\n",
        "\n",
        "# create formatter and add it to the handlers\n",
        "formatter = logging.Formatter('%(asctime)s :  %(message)s')\n",
        "sh = logging.StreamHandler()\n",
        "sh.setLevel(logging.INFO)\n",
        "sh.setFormatter(formatter)\n",
        "log.handlers = [sh]\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  log.info(\"Using TPU runtime\")\n",
        "  USE_TPU = True\n",
        "  TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "\n",
        "  with tf.Session(TPU_ADDRESS) as session:\n",
        "    log.info('TPU address is ' + TPU_ADDRESS)\n",
        "    # Upload credentials to TPU.\n",
        "    #with open('/content/adc.json', 'r') as f:\n",
        "    #  auth_info = json.load(f)\n",
        "    #tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "    \n",
        "else:\n",
        "  log.warning('Not connected to TPU runtime')\n",
        "  USE_TPU = False"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.97)\n",
            "fatal: destination path 'bert' already exists and is not an empty directory.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-11-02 06:12:56,598 :  Using TPU runtime\n",
            "INFO:tensorflow:Using TPU runtime\n",
            "2022-11-02 06:12:56,603 :  TPU address is grpc://10.7.129.162:8470\n",
            "INFO:tensorflow:TPU address is grpc://10.7.129.162:8470\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvfNt0mVZ33n"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGVXMoC-aMy1"
      },
      "source": [
        "## Step 2: getting the data\n",
        "\n",
        "We begin with obtaining a corpus of raw text data. For this experiment, we will be using the [OpenSubtitles](http://www.opensubtitles.org/) dataset, which is available for 65 languages [here](http://opus.nlpl.eu/OpenSubtitles-v2016.php). \n",
        "\n",
        "Unlike more common text datasets (like Wikipedia) it does not require any complex pre-processing. It also comes pre-formatted with one sentence per line.\n",
        "\n",
        "Feel free to use the dataset for your language instead by changing the language code (en) below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FotFkkshbdvK",
        "outputId": "32be33dd-d30f-4017-f5e8-9c1e3ac37a01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "AVAILABLE =  {'af','ar','bg','bn','br','bs','ca','cs',\n",
        "              'da','de','el','en','eo','es','et','eu',\n",
        "              'fa','fi','fr','gl','he','hi','hr','hu',\n",
        "              'hy','id','is','it','ja','ka','kk','ko',\n",
        "              'lt','lv','mk','ml','ms','nl','no','pl',\n",
        "              'pt','pt_br','ro','ru','si','sk','sl','sq',\n",
        "              'sr','sv','ta','te','th','tl','tr','uk',\n",
        "              'ur','vi','ze_en','ze_zh','zh','zh_cn',\n",
        "              'zh_en','zh_tw','zh_zh'}\n",
        "\n",
        "LANG_CODE = \"en\" #@param {type:\"string\"}\n",
        "\n",
        "assert LANG_CODE in AVAILABLE, \"Invalid language code selected\"\n",
        "\n",
        "!wget http://opus.nlpl.eu/download.php?f=OpenSubtitles/v2016/mono/OpenSubtitles.raw.en.gz -O dataset.txt.gz\n",
        "!gzip -d dataset.txt.gz\n",
        "!tail dataset.txt"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-02 05:40:47--  http://opus.nlpl.eu/download.php?f=OpenSubtitles/v2016/mono/OpenSubtitles.raw.en.gz\n",
            "Resolving opus.nlpl.eu (opus.nlpl.eu)... 193.166.25.9\n",
            "Connecting to opus.nlpl.eu (opus.nlpl.eu)|193.166.25.9|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://opus.nlpl.eu/download.php?f=OpenSubtitles/v2016/mono/OpenSubtitles.raw.en.gz [following]\n",
            "--2022-11-02 05:40:48--  https://opus.nlpl.eu/download.php?f=OpenSubtitles/v2016/mono/OpenSubtitles.raw.en.gz\n",
            "Connecting to opus.nlpl.eu (opus.nlpl.eu)|193.166.25.9|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://object.pouta.csc.fi/OPUS-OpenSubtitles/v2016/mono/en.txt.gz [following]\n",
            "--2022-11-02 05:40:49--  https://object.pouta.csc.fi/OPUS-OpenSubtitles/v2016/mono/en.txt.gz\n",
            "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.18, 86.50.254.19\n",
            "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2906709304 (2.7G) [application/gzip]\n",
            "Saving to: ‘dataset.txt.gz’\n",
            "\n",
            "dataset.txt.gz      100%[===================>]   2.71G  21.5MB/s    in 2m 11s  \n",
            "\n",
            "2022-11-02 05:43:01 (21.2 MB/s) - ‘dataset.txt.gz’ saved [2906709304/2906709304]\n",
            "\n",
            "gzip: dataset.txt already exists; do you wish to overwrite (y or n)? y\n",
            "y\n",
            "The astronomers run at full speed turning around each time they are pressed too closely and reducing the fragile beings to dust.\n",
            "At last, the astronomers have found the shell and quickly shut themselves in the interior.\n",
            "Thanks to the advance, they have succeeded in getting over their adversaries.\n",
            "Only one, the president, has been left behind.\n",
            "He rushes to the rope which hangs from the point of the shell and letting himself slide down the rope he gives it an impetus which causes the shell to fall off the edge of the moon.\n",
            "The shell falls with sickening rapidity.\n",
            "The sea appears.\n",
            "The shell balances and thanks to the hermetically sealed air in its interior rises slowly to the surface.\n",
            "The shell is picked up by a steamer which tows it to port.\n",
            "The Mayor welcomes the astronomers with a speech, and the general ovation awaits their happy return.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nb5TPsPOppn0"
      },
      "source": [
        "For demonstration purposes, we will only use a small fraction of the whole corpus for this experiment. \n",
        "\n",
        "When training the real model, make sure to uncheck the DEMO_MODE checkbox to use a 100x larger dataset.\n",
        "\n",
        "Rest assured, 100M lines are perfectly sufficient to train a reasonably good BERT-base model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDR5Z1MDgB1g"
      },
      "source": [
        "DEMO_MODE = True #@param {type:\"boolean\"}\n",
        "\n",
        "if DEMO_MODE:\n",
        "  CORPUS_SIZE = 1000000\n",
        "else:\n",
        "  CORPUS_SIZE = 1000000 #@param {type: \"integer\"}\n",
        "  \n",
        "!(head -n $CORPUS_SIZE dataset.txt) > subdataset.txt\n",
        "!mv subdataset.txt dataset.txt"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRQd4-v0nQqH"
      },
      "source": [
        "## Step 3: preprocessing text\n",
        "\n",
        "The raw text data we have downloaded contains punсtuation, uppercase letters and non-UTF symbols which we will remove before proceeding. During inference, we will apply the same normalization procedure to new data.\n",
        "\n",
        "If your use-case requires different preprocessing (e.g. if uppercase letters or punctuation are expected during inference), feel free to modify the function below to accomodate for your needs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCi2oSdInRkC"
      },
      "source": [
        "regex_tokenizer = nltk.RegexpTokenizer(\"\\w+\")\n",
        "\n",
        "def normalize_text(text):\n",
        "  # lowercase text\n",
        "  text = str(text).lower()\n",
        "  # remove non-UTF\n",
        "  text = text.encode(\"utf-8\", \"ignore\").decode()\n",
        "  # remove punktuation symbols\n",
        "  text = \" \".join(regex_tokenizer.tokenize(text))\n",
        "  return text\n",
        "\n",
        "def count_lines(filename):\n",
        "  count = 0\n",
        "  with open(filename) as fi:\n",
        "    for line in fi:\n",
        "      count += 1\n",
        "  return count"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYtwtDnesaQQ"
      },
      "source": [
        "Check how that works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gngtEZWqVhY",
        "outputId": "bf707c0d-e7e7-40b0-bf73-38c0afd022b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "normalize_text('Thanks to the advance, they have succeeded in getting over their adversaries.')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'thanks to the advance they have succeeded in getting over their adversaries'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY-Kvnx6sUFS"
      },
      "source": [
        "Apply normalization to the whole dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myjxQe5awo1v",
        "outputId": "f2dda81c-e08b-4229-9663-5e4129d7567e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "RAW_DATA_FPATH = \"dataset.txt\" #@param {type: \"string\"}\n",
        "PRC_DATA_FPATH = \"proc_dataset.txt\" #@param {type: \"string\"}\n",
        "\n",
        "# apply normalization to the dataset\n",
        "# this will take a minute or two\n",
        "\n",
        "total_lines = count_lines(RAW_DATA_FPATH)\n",
        "bar = Progbar(total_lines)\n",
        "\n",
        "with open(RAW_DATA_FPATH,encoding=\"utf-8\") as fi:\n",
        "  with open(PRC_DATA_FPATH, \"w\",encoding=\"utf-8\") as fo:\n",
        "    for l in fi:\n",
        "      fo.write(normalize_text(l)+\"\\n\")\n",
        "      bar.add(1)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000000/1000000 [==============================] - 6s 6us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3A64RZjwo9k"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVO9EnUwrluQ"
      },
      "source": [
        "## Step 4: building the vocabulary\n",
        "\n",
        "For the next step, we will learn a new vocabulary that we will use to represent our dataset. \n",
        "\n",
        "The BERT paper uses a WordPiece tokenizer, which is not available in opensource. Instead, we will be using SentencePiece tokenizer in unigram mode. While it is not directly compatible with BERT, with a small hack we can make it work.\n",
        "\n",
        "SentencePiece requires quite a lot of RAM, so running it on the full dataset in Colab will crash the kernel. To avoid this, we will randomly subsample a fraction of the dataset for building the vocabulary. Another option would be to use a machine with more RAM for this step - that decision is up to you.\n",
        "\n",
        "Also, SentencePiece adds BOS and EOS control symbols to the vocabulary by default. We disable them explicitly by setting their indices to -1.\n",
        "\n",
        "The typical values for VOC_SIZE are somewhere in between 32000 and 128000. We reserve NUM_PLACEHOLDERS tokens in case one wants to update the vocabulary and fine-tune the model after the pre-training phase is finished."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18nn6eW_s-fV"
      },
      "source": [
        "MODEL_PREFIX = \"tokenizer\" #@param {type: \"string\"}\n",
        "VOC_SIZE = 32000 #@param {type:\"integer\"}\n",
        "SUBSAMPLE_SIZE = 12800000 #@param {type:\"integer\"}\n",
        "NUM_PLACEHOLDERS = 256 #@param {type:\"integer\"}\n",
        "\n",
        "SPM_COMMAND = ('--input={} --model_prefix={} '\n",
        "               '--vocab_size={} --input_sentence_size={} '\n",
        "               '--shuffle_input_sentence=true ' \n",
        "               '--bos_id=-1 --eos_id=-1').format(\n",
        "               PRC_DATA_FPATH, MODEL_PREFIX, \n",
        "               VOC_SIZE - NUM_PLACEHOLDERS, SUBSAMPLE_SIZE)\n",
        "\n",
        "spm.SentencePieceTrainer.Train(SPM_COMMAND)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAowCoH2u1iZ"
      },
      "source": [
        "Now let's see how we can make SentencePiece tokenizer work for the BERT model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OLaw7kPW3he"
      },
      "source": [
        "Below is a sentence tokenized using the WordPiece vocabulary from a pretrained English [BERT-base](https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip) model from the official [repo](https://github.com/google-research/bert). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwHAp_Gh5OPK"
      },
      "source": [
        "testcase = \"Colorless geothermal substations are generating furiously\""
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyEGAVl_5YRD"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        ">>> wordpiece.tokenize(\"Colorless geothermal substations are generating furiously\")\n",
        "\n",
        "['color',\n",
        " '##less',\n",
        " 'geo',\n",
        " '##thermal',\n",
        " 'sub',\n",
        " '##station',\n",
        " '##s',\n",
        " 'are',\n",
        " 'generating',\n",
        " 'furiously']\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNiLdWXTh9cj"
      },
      "source": [
        "As we can see, the WordPiece tokenizer prepends the subwords which occur in the middle of words with '##'. The subwords occurring at the beginning of words are unchanged. If the subword occurs both in the beginning and in the middle of words, both versions (with and without '##') are added to the vocabulary.\n",
        "\n",
        "Now let's have a look at the vocabulary that the SentencePiece tokenizer has learned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8_ebLxqTnWu",
        "outputId": "e9db2e2e-adb2-405b-d6a6-20f1eca840a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "adc.json  dataset.txt\t    sample_data      tokenizer.vocab\n",
            "bert\t  proc_dataset.txt  tokenizer.model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYlBqiv5UD-j"
      },
      "source": [
        "SentencePiece has created two files: tokenizer.model and tokenizer.vocab. Let's have a look at the learned vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDJ9QmNMUEQf",
        "outputId": "9743494b-6234-4e9c-ce60-7daa46cf0f1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!head -n 100 tokenizer.vocab"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<unk>\t0\n",
            "▁you\t-3.2342\n",
            "▁i\t-3.2821\n",
            "▁the\t-3.56375\n",
            "▁s\t-3.84955\n",
            "▁to\t-3.87601\n",
            "▁a\t-3.9102\n",
            "▁it\t-3.97593\n",
            "▁t\t-4.25729\n",
            "▁and\t-4.32686\n",
            "▁that\t-4.33991\n",
            "▁of\t-4.57503\n",
            "▁what\t-4.59504\n",
            "▁is\t-4.67144\n",
            "▁me\t-4.7115\n",
            "▁in\t-4.71265\n",
            "▁we\t-4.7231\n",
            "▁he\t-4.88446\n",
            "▁this\t-4.89466\n",
            "▁no\t-4.98519\n",
            "▁on\t-4.99226\n",
            "▁my\t-4.99616\n",
            "▁m\t-5.02169\n",
            "▁your\t-5.02715\n",
            "▁for\t-5.05648\n",
            "▁have\t-5.06695\n",
            "▁don\t-5.09057\n",
            "▁do\t-5.12828\n",
            "▁\t-5.12998\n",
            "▁re\t-5.1371\n",
            "▁can\t-5.20534\n",
            "▁was\t-5.20891\n",
            "▁know\t-5.22494\n",
            "▁be\t-5.23557\n",
            "▁are\t-5.25247\n",
            "▁not\t-5.26685\n",
            "▁all\t-5.29439\n",
            "▁with\t-5.31925\n",
            "▁but\t-5.37188\n",
            "▁so\t-5.42124\n",
            "▁get\t-5.43031\n",
            "▁here\t-5.43446\n",
            "▁just\t-5.43719\n",
            "▁ll\t-5.49322\n",
            "▁like\t-5.5182\n",
            "▁they\t-5.5196\n",
            "▁there\t-5.52012\n",
            "▁up\t-5.55201\n",
            "▁go\t-5.55369\n",
            "▁she\t-5.5887\n",
            "▁right\t-5.61613\n",
            "▁out\t-5.64626\n",
            "▁oh\t-5.68517\n",
            "s\t-5.71583\n",
            "▁come\t-5.73109\n",
            "▁if\t-5.73944\n",
            "▁him\t-5.74635\n",
            "▁one\t-5.75141\n",
            "▁about\t-5.75416\n",
            "▁got\t-5.77074\n",
            "▁at\t-5.78535\n",
            "▁now\t-5.8265\n",
            "▁yeah\t-5.84144\n",
            "▁how\t-5.84948\n",
            "▁her\t-5.87603\n",
            "▁well\t-5.94933\n",
            "▁let\t-5.95131\n",
            "▁good\t-5.98986\n",
            "▁want\t-6.01295\n",
            "▁ve\t-6.02348\n",
            "▁think\t-6.04507\n",
            "▁who\t-6.06409\n",
            "▁did\t-6.09598\n",
            "▁see\t-6.10352\n",
            "▁why\t-6.11416\n",
            "▁will\t-6.12599\n",
            "▁gonna\t-6.17628\n",
            "▁from\t-6.17938\n",
            "▁look\t-6.19767\n",
            "▁as\t-6.20588\n",
            "▁yes\t-6.21377\n",
            "▁back\t-6.22989\n",
            "▁d\t-6.23892\n",
            "▁his\t-6.24827\n",
            "n\t-6.25315\n",
            "▁man\t-6.25455\n",
            "▁when\t-6.26481\n",
            "▁okay\t-6.27249\n",
            "▁time\t-6.27638\n",
            "▁could\t-6.28472\n",
            "▁take\t-6.30417\n",
            "▁hey\t-6.31565\n",
            "▁say\t-6.35372\n",
            "▁had\t-6.37539\n",
            "▁an\t-6.37643\n",
            "▁us\t-6.38685\n",
            "▁or\t-6.38714\n",
            "▁were\t-6.38932\n",
            "▁some\t-6.39201\n",
            "▁where\t-6.39465\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBsURk_h5jw4",
        "outputId": "21a7b454-2d32-49e9-c9ae-1d5de94d4ac2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def read_sentencepiece_vocab(filepath):\n",
        "  voc = []\n",
        "  with open(filepath, encoding='utf-8') as fi:\n",
        "    for line in fi:\n",
        "      voc.append(line.split(\"\\t\")[0])\n",
        "  # skip the first <unk> token\n",
        "  voc = voc[1:]\n",
        "  return voc\n",
        "\n",
        "snt_vocab = read_sentencepiece_vocab(\"{}.vocab\".format(MODEL_PREFIX))\n",
        "print(\"Learnt vocab size: {}\".format(len(snt_vocab)))\n",
        "print(\"Sample tokens: {}\".format(random.sample(snt_vocab, 10)))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learnt vocab size: 31743\n",
            "Sample tokens: ['▁desert', '▁ieast', '▁theresa', '▁supervise', '▁dagger', 'cited', '▁expectations', '▁neither', 'stad', 'ministering']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJPtxrtz5470"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YjqHVRpmlKq"
      },
      "source": [
        "As we may observe, SentencePiece does quite the opposite to WordPiece. From the [documentation](https://github.com/google/sentencepiece/blob/master/README.md):\n",
        "\n",
        "\n",
        "SentencePiece first escapes the whitespace with a meta-symbol \"▁\" (U+2581) as follows:\n",
        "\n",
        "`Hello▁World`.\n",
        "\n",
        "Then, this text is segmented into small pieces, for example:\n",
        "\n",
        "`[Hello] [▁Wor] [ld] [.]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OD-28l_0p0PQ"
      },
      "source": [
        "Subwords which occur after whitespace (which are also those that most words begin with) are prepended with '▁', while others are unchanged. This excludes subwords which only occur at the beginning of sentences and nowhere else. These cases should be quite rare, however. \n",
        "\n",
        "So, in order to obtain a vocabulary analogous to WordPiece, we need to perform a simple conversion, removing \"▁\" from the tokens that contain it and adding \"##\"  to the ones that don't."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QJGFjzOMbfN"
      },
      "source": [
        "def parse_sentencepiece_token(token):\n",
        "    if token.startswith(\"▁\"):\n",
        "        return token[1:]\n",
        "    else:\n",
        "        return \"##\" + token"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64dcVgD98S28"
      },
      "source": [
        "bert_vocab = list(map(parse_sentencepiece_token, snt_vocab))"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL9ZR3RN9IMA"
      },
      "source": [
        "We also add some special control symbols which are required by the BERT architecture. By convention, we put those at the beginning of the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdTlXDPL8cHE"
      },
      "source": [
        "ctrl_symbols = [\"[PAD]\",\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[MASK]\"]\n",
        "bert_vocab = ctrl_symbols + bert_vocab"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ry1jvEr9EIdd"
      },
      "source": [
        "We also append some placeholder tokens to the vocabulary. Those are useful if one wishes to update the pre-trained model with new, task-specific tokens. \n",
        "\n",
        "In that case, the placeholder tokens are replaced with new real ones, the pre-training data is re-generated, and the model is fine-tuned on new data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLYSTil4E0Dm",
        "outputId": "3d9b02e6-6ac1-4f95-c21a-c84fde06b980",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "bert_vocab += [\"[UNUSED_{}]\".format(i) for i in range(VOC_SIZE - len(bert_vocab))]\n",
        "print(len(bert_vocab))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJKk_7JI-MtW"
      },
      "source": [
        "Finally, we write the obtained vocabulary to file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8G1jg0cj9Duf"
      },
      "source": [
        "VOC_FNAME = \"vocab.txt\" #@param {type:\"string\"}\n",
        "\n",
        "with open(VOC_FNAME, \"w\") as fo:\n",
        "  for token in bert_vocab:\n",
        "    fo.write(token+\"\\n\")"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5fs7H049nB8"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MqXZnc3FCuY"
      },
      "source": [
        "Now let's see how the new vocabulary works in practice:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsSOnEnC-jG1",
        "outputId": "b2cb11e6-9542-4d8c-dd4c-f5068636ca4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "bert_tokenizer = tokenization.FullTokenizer(VOC_FNAME)\n",
        "bert_tokenizer.tokenize(testcase)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-11-02 05:45:59,814 :  From /content/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['color',\n",
              " '##less',\n",
              " 'geo',\n",
              " '##ther',\n",
              " '##mal',\n",
              " 'subs',\n",
              " '##tation',\n",
              " '##s',\n",
              " 'are',\n",
              " 'generat',\n",
              " '##ing',\n",
              " 'furious',\n",
              " '##ly']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN8xDNfF0Q2q"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DauD8ndhEA-z"
      },
      "source": [
        "Looking good!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwFtStCo__QX"
      },
      "source": [
        "## Step 5: generating pre-training data\n",
        "\n",
        "With the vocabulary at hand, we are ready to generate pre-training data for the BERT model. Since our dataset might be quite large, we will split it into shards:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyN1nI04-uKV",
        "outputId": "70d759fe-2e48-4176-aa44-1782fb908157",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!mkdir ./shards\n",
        "!split -a 4 -l 256000 -d $PRC_DATA_FPATH ./shards/shard_\n",
        "!ls ./shards/"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shard_0000  shard_0001\tshard_0002  shard_0003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-FSq3zNFvLs"
      },
      "source": [
        "Before we start generating, we need to set some model-specific parameters.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnZcD0yIBGPd"
      },
      "source": [
        "MAX_SEQ_LENGTH = 128 #@param {type:\"integer\"}\n",
        "MASKED_LM_PROB = 0.15 #@param\n",
        "MAX_PREDICTIONS = 20 #@param {type:\"integer\"}\n",
        "DO_LOWER_CASE = True #@param {type:\"boolean\"}\n",
        "PROCESSES = 2 #@param {type:\"integer\"}\n",
        "PRETRAINING_DIR = \"pretraining_data\" #@param {type:\"string\"}"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-MibOBkFam2"
      },
      "source": [
        "Now, for each shard we need to call *create_pretraining_data.py* script. To that end, we will employ the  *xargs* command. \n",
        "\n",
        "Running this might take quite some time depending on the size of your dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbZjIeVP0T36"
      },
      "source": [
        "XARGS_CMD = (\"ls ./shards/ | \"\n",
        "             \"xargs -n 1 -P {} -I{} \"\n",
        "             \"python3 bert/create_pretraining_data.py \"\n",
        "             \"--input_file=./shards/{} \"\n",
        "             \"--output_file={}/{}.tfrecord \"\n",
        "             \"--vocab_file={} \"\n",
        "             \"--do_lower_case={} \"\n",
        "             \"--max_predictions_per_seq={} \"\n",
        "             \"--max_seq_length={} \"\n",
        "             \"--masked_lm_prob={} \"\n",
        "             \"--random_seed=34 \"\n",
        "             \"--dupe_factor=5\")\n",
        "\n",
        "XARGS_CMD = XARGS_CMD.format(PROCESSES, '{}', '{}', PRETRAINING_DIR, '{}', \n",
        "                             VOC_FNAME, DO_LOWER_CASE, \n",
        "                             MAX_PREDICTIONS, MAX_SEQ_LENGTH, MASKED_LM_PROB)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fyo9_LcQ0pla"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5XzaY8xCdiV"
      },
      "source": [
        "tf.gfile.MkDir(PRETRAINING_DIR)\n",
        "!$XARGS_CMD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKnW_hVxPoP6"
      },
      "source": [
        "## Step 6: setting up persistent storage\n",
        "\n",
        "To preserve our hard-earned assets, we will persist them to Google Cloud Storage. Provided that you have created the GCS bucket, this should be simple.\n",
        "\n",
        "We will create two directories in GCS, one for the data and one for the model.\n",
        "In the model directory, we will put the model vocabulary and configuration file.\n",
        "\n",
        "**Configure your BUCKET_NAME variable here before proceeding, otherwise the model and data will not be saved.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDtrt68QQIHs"
      },
      "source": [
        "BUCKET_NAME = \"bert_sdw\" #@param {type:\"string\"}\n",
        "MODEL_DIR = \"bert_model\" #@param {type:\"string\"}\n",
        "tf.gfile.MkDir(MODEL_DIR)\n",
        "\n",
        "if not BUCKET_NAME:\n",
        "  log.warning(\"WARNING: BUCKET_NAME is not set. \"\n",
        "              \"You will not be able to train the model.\")"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YigCcV-hSHVH"
      },
      "source": [
        "Below is the sample hyperparameter configuration for BERT-base. Change at your own risk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEpSGpUKReKW"
      },
      "source": [
        "# use this for BERT-base\n",
        "\n",
        "bert_base_config = {\n",
        "  \"attention_probs_dropout_prob\": 0.1, \n",
        "  \"directionality\": \"bidi\", \n",
        "  \"hidden_act\": \"gelu\", \n",
        "  \"hidden_dropout_prob\": 0.1, \n",
        "  \"hidden_size\": 768, \n",
        "  \"initializer_range\": 0.02, \n",
        "  \"intermediate_size\": 3072, \n",
        "  \"max_position_embeddings\": 512, \n",
        "  \"num_attention_heads\": 12, \n",
        "  \"num_hidden_layers\": 12, \n",
        "  \"pooler_fc_size\": 768, \n",
        "  \"pooler_num_attention_heads\": 12, \n",
        "  \"pooler_num_fc_layers\": 3, \n",
        "  \"pooler_size_per_head\": 128, \n",
        "  \"pooler_type\": \"first_token_transform\", \n",
        "  \"type_vocab_size\": 2, \n",
        "  \"vocab_size\": VOC_SIZE\n",
        "}\n",
        "\n",
        "with open(\"{}/bert_config.json\".format(MODEL_DIR), \"w\") as fo:\n",
        "  json.dump(bert_base_config, fo, indent=2)\n",
        "  \n",
        "with open(\"{}/{}\".format(MODEL_DIR, VOC_FNAME), \"w\") as fo:\n",
        "  for token in bert_vocab:\n",
        "    fo.write(token+\"\\n\")"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2O3QxTD_MUC",
        "outputId": "8a78b4c5-35c9-4d8d-a680-a03cd690641d"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are attempting to perform an operation that requires a project id, with none configured. Please re-run gsutil config and make sure to follow the instructions for finding and entering your default project id.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txgrEDugRG48",
        "outputId": "a402fb88-ca44-4926-885e-da1dc64944d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "if BUCKET_NAME:\n",
        "  !gsutil -m cp -r $MODEL_DIR $PRETRAINING_DIR gs://$BUCKET_NAME"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying file://bert_model/bert_config.json [Content-Type=application/json]...\n",
            "/ [0/8 files][    0.0 B/294.7 MiB]   0% Done                                    \rCopying file://bert_model/graph.pbtxt [Content-Type=application/octet-stream]...\n",
            "Copying file://bert_model/events.out.tfevents.1667368510.b570fee5ae9a [Content-Type=application/octet-stream]...\n",
            "/ [0/8 files][    0.0 B/294.7 MiB]   0% Done                                    \r/ [0/8 files][    0.0 B/294.7 MiB]   0% Done                                    \rCopying file://bert_model/vocab.txt [Content-Type=text/plain]...\n",
            "/ [0/8 files][    0.0 B/294.7 MiB]   0% Done                                    \rCopying file://pretraining_data/shard_0003.tfrecord [Content-Type=application/octet-stream]...\n",
            "/ [0/8 files][    0.0 B/294.7 MiB]   0% Done                                    \rCopying file://pretraining_data/shard_0001.tfrecord [Content-Type=application/octet-stream]...\n",
            "Copying file://pretraining_data/shard_0002.tfrecord [Content-Type=application/octet-stream]...\n",
            "Copying file://pretraining_data/shard_0000.tfrecord [Content-Type=application/octet-stream]...\n",
            "- [8/8 files][294.7 MiB/294.7 MiB] 100% Done                                    \n",
            "Operation completed over 8 objects/294.7 MiB.                                    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DL6xuCAYPrp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "64b3405e-7dea-4642-b8fd-1e28c0ae4de8"
      },
      "source": [
        "pwd"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gdQEOzhYmSh"
      },
      "source": [
        "## Step 7: training the model\n",
        "\n",
        "We are almost ready to begin training our model. If you wish  to continue an interrupted training run, you may skip steps 2-6 and proceed from here.\n",
        "\n",
        "**Make sure that you have set the BUCKET_NAME here as well.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXAuzsJfYrio",
        "outputId": "569efa47-58dd-443b-a496-a76c8491e5fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#BUCKET_NAME = \"bert_sdw\" #@param {type:\"string\"}\n",
        "MODEL_DIR = \"bert_model\" #@param {type:\"string\"}\n",
        "PRETRAINING_DIR = \"pretraining_data\" #@param {type:\"string\"}\n",
        "VOC_FNAME = \"vocab.txt\" #@param {type:\"string\"}\n",
        "\n",
        "# Input data pipeline config\n",
        "TRAIN_BATCH_SIZE = 128 #@param {type:\"integer\"}\n",
        "MAX_PREDICTIONS = 20 #@param {type:\"integer\"}\n",
        "MAX_SEQ_LENGTH = 128 #@param {type:\"integer\"}\n",
        "MASKED_LM_PROB = 0.15 #@param\n",
        "\n",
        "# Training procedure config\n",
        "EVAL_BATCH_SIZE = 64\n",
        "LEARNING_RATE = 2e-5\n",
        "TRAIN_STEPS = 1000000 #@param {type:\"integer\"}\n",
        "SAVE_CHECKPOINTS_STEPS = 2500 #@param {type:\"integer\"}\n",
        "NUM_TPU_CORES = 8\n",
        "\n",
        "if BUCKET_NAME:\n",
        "  BUCKET_PATH = \"gs://{}\".format(BUCKET_NAME)\n",
        "else:\n",
        "  BUCKET_PATH = \".\"\n",
        "\n",
        "BERT_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, MODEL_DIR)\n",
        "DATA_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, PRETRAINING_DIR)\n",
        "\n",
        "VOCAB_FILE = os.path.join(BERT_GCS_DIR, VOC_FNAME)\n",
        "CONFIG_FILE = os.path.join(BERT_GCS_DIR, \"bert_config.json\")\n",
        "\n",
        "INIT_CHECKPOINT = tf.train.latest_checkpoint(BERT_GCS_DIR)\n",
        "\n",
        "bert_config = modeling.BertConfig.from_json_file(CONFIG_FILE)\n",
        "input_files = tf.gfile.Glob(os.path.join(DATA_GCS_DIR,'*tfrecord'))\n",
        "\n",
        "log.info(\"Using checkpoint: {}\".format(INIT_CHECKPOINT))\n",
        "log.info(\"Using {} data shards\".format(len(input_files)))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-11-02 07:57:39,061 :  Using checkpoint: None\n",
            "INFO:tensorflow:Using checkpoint: None\n",
            "2022-11-02 07:57:39,064 :  Using 4 data shards\n",
            "INFO:tensorflow:Using 4 data shards\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwwF-WqcZHUG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "77b1224a-298b-41fe-aa87-17f74ee93903"
      },
      "source": [
        "pwd"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQTFjITdd53F"
      },
      "source": [
        "Prepare the training run configuration, build the estimator and input function, power up the bass cannon."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMahsqUnZ55z",
        "outputId": "63d2b948-d7db-4700-ca38-e92311504369",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model_fn = model_fn_builder(\n",
        "      bert_config=bert_config,\n",
        "      init_checkpoint=INIT_CHECKPOINT,\n",
        "      learning_rate=LEARNING_RATE,\n",
        "      num_train_steps=TRAIN_STEPS,\n",
        "      num_warmup_steps=10,\n",
        "      use_tpu=USE_TPU,\n",
        "      use_one_hot_embeddings=True)\n",
        "\n",
        "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "\n",
        "run_config = tf.contrib.tpu.RunConfig(\n",
        "    cluster=tpu_cluster_resolver,\n",
        "    model_dir=BERT_GCS_DIR,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "        iterations_per_loop=SAVE_CHECKPOINTS_STEPS,\n",
        "        num_shards=NUM_TPU_CORES,\n",
        "        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
        "\n",
        "estimator = tf.contrib.tpu.TPUEstimator(\n",
        "    use_tpu=USE_TPU,\n",
        "    model_fn=model_fn,\n",
        "    config=run_config,\n",
        "    train_batch_size=TRAIN_BATCH_SIZE,\n",
        "    eval_batch_size=EVAL_BATCH_SIZE)\n",
        "  \n",
        "train_input_fn = input_fn_builder(\n",
        "        input_files=input_files,\n",
        "        max_seq_length=MAX_SEQ_LENGTH,\n",
        "        max_predictions_per_seq=MAX_PREDICTIONS,\n",
        "        is_training=True)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-11-02 07:57:41,951 :  Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7fa988e9e7a0>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7fa988e9e7a0>) includes params argument, but params are not passed to Estimator.\n",
            "2022-11-02 07:57:41,954 :  Using config: {'_model_dir': './bert_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 2500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.7.129.162:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "isolate_session_state: true\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fa9909a3950>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.7.129.162:8470', '_evaluation_master': 'grpc://10.7.129.162:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=2500, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7fa97cad39d0>}\n",
            "INFO:tensorflow:Using config: {'_model_dir': './bert_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 2500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.7.129.162:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "isolate_session_state: true\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fa9909a3950>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.7.129.162:8470', '_evaluation_master': 'grpc://10.7.129.162:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=2500, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7fa97cad39d0>}\n",
            "2022-11-02 07:57:41,957 :  _TPUContext: eval_on_tpu True\n",
            "INFO:tensorflow:_TPUContext: eval_on_tpu True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNt5ykopeIYB"
      },
      "source": [
        "Fire!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrCuEbr6dv8U",
        "outputId": "ac40c4d5-d168-45b6-fdd2-9026aede56e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "estimator.train(input_fn=train_input_fn, max_steps=TRAIN_STEPS)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-11-02 07:57:46,772 :  Querying Tensorflow master (grpc://10.7.129.162:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.7.129.162:8470) for TPU system metadata.\n",
            "2022-11-02 07:57:46,782 :  Found TPU system:\n",
            "INFO:tensorflow:Found TPU system:\n",
            "2022-11-02 07:57:46,784 :  *** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "2022-11-02 07:57:46,787 :  *** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "2022-11-02 07:57:46,789 :  *** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "2022-11-02 07:57:46,791 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 14277562530083811006)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 14277562530083811006)\n",
            "2022-11-02 07:57:46,794 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 6701184566704306979)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 6701184566704306979)\n",
            "2022-11-02 07:57:46,796 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 3197101256674069165)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 3197101256674069165)\n",
            "2022-11-02 07:57:46,798 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 2908672141413093767)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 2908672141413093767)\n",
            "2022-11-02 07:57:46,801 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 1361745471776189489)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 1361745471776189489)\n",
            "2022-11-02 07:57:46,803 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 3457321559882673536)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 3457321559882673536)\n",
            "2022-11-02 07:57:46,806 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 3482352651018717341)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 3482352651018717341)\n",
            "2022-11-02 07:57:46,809 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 1570110160920763062)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 1570110160920763062)\n",
            "2022-11-02 07:57:46,811 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 16414735746516353220)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 16414735746516353220)\n",
            "2022-11-02 07:57:46,814 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 7192132182305868920)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 7192132182305868920)\n",
            "2022-11-02 07:57:46,816 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7024324821964570085)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7024324821964570085)\n",
            "2022-11-02 07:57:46,849 :  Calling model_fn.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "2022-11-02 07:57:46,928 :  Found small feature: next_sentence_labels [16, 1]\n",
            "INFO:tensorflow:Found small feature: next_sentence_labels [16, 1]\n",
            "2022-11-02 07:57:46,934 :  Found small feature: next_sentence_labels [16, 1]\n",
            "INFO:tensorflow:Found small feature: next_sentence_labels [16, 1]\n",
            "2022-11-02 07:57:46,941 :  Found small feature: next_sentence_labels [16, 1]\n",
            "INFO:tensorflow:Found small feature: next_sentence_labels [16, 1]\n",
            "2022-11-02 07:57:46,948 :  Found small feature: next_sentence_labels [16, 1]\n",
            "INFO:tensorflow:Found small feature: next_sentence_labels [16, 1]\n",
            "2022-11-02 07:57:46,955 :  Found small feature: next_sentence_labels [16, 1]\n",
            "INFO:tensorflow:Found small feature: next_sentence_labels [16, 1]\n",
            "2022-11-02 07:57:46,961 :  Found small feature: next_sentence_labels [16, 1]\n",
            "INFO:tensorflow:Found small feature: next_sentence_labels [16, 1]\n",
            "2022-11-02 07:57:46,968 :  Found small feature: next_sentence_labels [16, 1]\n",
            "INFO:tensorflow:Found small feature: next_sentence_labels [16, 1]\n",
            "2022-11-02 07:57:46,974 :  Found small feature: next_sentence_labels [16, 1]\n",
            "INFO:tensorflow:Found small feature: next_sentence_labels [16, 1]\n",
            "2022-11-02 07:57:47,023 :  *** Features ***\n",
            "INFO:tensorflow:*** Features ***\n",
            "2022-11-02 07:57:47,025 :    name = input_ids, shape = (16, 128)\n",
            "INFO:tensorflow:  name = input_ids, shape = (16, 128)\n",
            "2022-11-02 07:57:47,028 :    name = input_mask, shape = (16, 128)\n",
            "INFO:tensorflow:  name = input_mask, shape = (16, 128)\n",
            "2022-11-02 07:57:47,030 :    name = masked_lm_ids, shape = (16, 20)\n",
            "INFO:tensorflow:  name = masked_lm_ids, shape = (16, 20)\n",
            "2022-11-02 07:57:47,033 :    name = masked_lm_positions, shape = (16, 20)\n",
            "INFO:tensorflow:  name = masked_lm_positions, shape = (16, 20)\n",
            "2022-11-02 07:57:47,036 :    name = masked_lm_weights, shape = (16, 20)\n",
            "INFO:tensorflow:  name = masked_lm_weights, shape = (16, 20)\n",
            "2022-11-02 07:57:47,038 :    name = next_sentence_labels, shape = (16, 1)\n",
            "INFO:tensorflow:  name = next_sentence_labels, shape = (16, 1)\n",
            "2022-11-02 07:57:47,041 :    name = segment_ids, shape = (16, 128)\n",
            "INFO:tensorflow:  name = segment_ids, shape = (16, 128)\n",
            "2022-11-02 07:57:51,646 :  **** Trainable Variables ****\n",
            "INFO:tensorflow:**** Trainable Variables ****\n",
            "2022-11-02 07:57:51,649 :    name = bert/embeddings/word_embeddings:0, shape = (32000, 768)\n",
            "INFO:tensorflow:  name = bert/embeddings/word_embeddings:0, shape = (32000, 768)\n",
            "2022-11-02 07:57:51,652 :    name = bert/embeddings/token_type_embeddings:0, shape = (2, 768)\n",
            "INFO:tensorflow:  name = bert/embeddings/token_type_embeddings:0, shape = (2, 768)\n",
            "2022-11-02 07:57:51,654 :    name = bert/embeddings/position_embeddings:0, shape = (512, 768)\n",
            "INFO:tensorflow:  name = bert/embeddings/position_embeddings:0, shape = (512, 768)\n",
            "2022-11-02 07:57:51,657 :    name = bert/embeddings/LayerNorm/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/embeddings/LayerNorm/beta:0, shape = (768,)\n",
            "2022-11-02 07:57:51,660 :    name = bert/embeddings/LayerNorm/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/embeddings/LayerNorm/gamma:0, shape = (768,)\n",
            "2022-11-02 07:57:51,662 :    name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:51,664 :    name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:51,666 :    name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:51,669 :    name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:51,671 :    name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:51,673 :    name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:51,676 :    name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:51,678 :    name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:51,681 :    name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "2022-11-02 07:57:51,683 :    name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2022-11-02 07:57:51,686 :    name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "2022-11-02 07:57:51,688 :    name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,)\n",
            "2022-11-02 07:57:51,690 :    name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768)\n",
            "2022-11-02 07:57:51,693 :    name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:51,695 :    name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,)\n",
            "2022-11-02 07:57:51,698 :    name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2022-11-02 07:57:51,702 :    name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:51,705 :    name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:51,708 :    name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:51,710 :    name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:51,712 :    name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:51,714 :    name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:51,717 :    name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:51,720 :    name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:51,722 :    name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "2022-11-02 07:57:51,725 :    name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2022-11-02 07:57:51,727 :    name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "2022-11-02 07:57:51,729 :    name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,)\n",
            "2022-11-02 07:57:51,732 :    name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768)\n",
            "2022-11-02 07:57:51,734 :    name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:51,736 :    name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,)\n",
            "2022-11-02 07:57:51,739 :    name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2022-11-02 07:57:51,742 :    name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:51,744 :    name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:51,748 :    name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:51,750 :    name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:51,753 :    name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:51,756 :    name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:51,759 :    name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:51,761 :    name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:51,764 :    name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "2022-11-02 07:57:51,766 :    name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2022-11-02 07:57:51,769 :    name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "2022-11-02 07:57:51,771 :    name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,)\n",
            "2022-11-02 07:57:51,774 :    name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768)\n",
            "2022-11-02 07:57:51,776 :    name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:51,779 :    name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,)\n",
            "2022-11-02 07:57:51,781 :    name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2022-11-02 07:57:51,784 :    name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:51,787 :    name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:51,790 :    name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:51,793 :    name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:51,795 :    name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:51,797 :    name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:51,800 :    name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:51,802 :    name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:51,804 :    name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "2022-11-02 07:57:51,807 :    name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2022-11-02 07:57:51,810 :    name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "2022-11-02 07:57:51,812 :    name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,)\n",
            "2022-11-02 07:57:51,817 :    name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768)\n",
            "2022-11-02 07:57:51,822 :    name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:51,829 :    name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,)\n",
            "2022-11-02 07:57:51,833 :    name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2022-11-02 07:57:51,837 :    name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:51,839 :    name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:51,842 :    name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:51,845 :    name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:51,848 :    name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:51,850 :    name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:51,852 :    name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:51,855 :    name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:51,857 :    name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "2022-11-02 07:57:51,860 :    name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2022-11-02 07:57:51,862 :    name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "2022-11-02 07:57:51,865 :    name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,)\n",
            "2022-11-02 07:57:51,868 :    name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768)\n",
            "2022-11-02 07:57:51,870 :    name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:51,872 :    name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,)\n",
            "2022-11-02 07:57:51,876 :    name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2022-11-02 07:57:51,880 :    name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:51,882 :    name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:51,884 :    name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:51,887 :    name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:51,890 :    name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:51,893 :    name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:51,896 :    name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:51,899 :    name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:51,901 :    name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "2022-11-02 07:57:51,904 :    name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2022-11-02 07:57:51,906 :    name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "2022-11-02 07:57:51,909 :    name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,)\n",
            "2022-11-02 07:57:51,912 :    name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768)\n",
            "2022-11-02 07:57:51,914 :    name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:51,917 :    name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,)\n",
            "2022-11-02 07:57:51,920 :    name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2022-11-02 07:57:51,923 :    name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:51,925 :    name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:51,928 :    name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:51,930 :    name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:51,933 :    name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:51,936 :    name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:51,938 :    name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:51,941 :    name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:51,943 :    name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "2022-11-02 07:57:51,947 :    name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2022-11-02 07:57:51,950 :    name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "2022-11-02 07:57:51,953 :    name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,)\n",
            "2022-11-02 07:57:51,956 :    name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768)\n",
            "2022-11-02 07:57:51,959 :    name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:51,962 :    name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,)\n",
            "2022-11-02 07:57:51,965 :    name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2022-11-02 07:57:51,968 :    name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:51,970 :    name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:51,974 :    name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:51,977 :    name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:51,979 :    name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:51,984 :    name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:51,986 :    name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:51,990 :    name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:51,993 :    name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "2022-11-02 07:57:51,996 :    name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2022-11-02 07:57:51,999 :    name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "2022-11-02 07:57:52,001 :    name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,)\n",
            "2022-11-02 07:57:52,005 :    name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768)\n",
            "2022-11-02 07:57:52,007 :    name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:52,010 :    name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,)\n",
            "2022-11-02 07:57:52,014 :    name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2022-11-02 07:57:52,017 :    name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:52,020 :    name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:52,024 :    name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:52,026 :    name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:52,028 :    name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:52,032 :    name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:52,034 :    name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:52,037 :    name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:52,041 :    name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "2022-11-02 07:57:52,043 :    name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2022-11-02 07:57:52,045 :    name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "2022-11-02 07:57:52,050 :    name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,)\n",
            "2022-11-02 07:57:52,053 :    name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768)\n",
            "2022-11-02 07:57:52,056 :    name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:52,059 :    name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,)\n",
            "2022-11-02 07:57:52,062 :    name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2022-11-02 07:57:52,065 :    name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:52,068 :    name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:52,071 :    name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:52,074 :    name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:52,076 :    name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:52,079 :    name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:52,081 :    name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:52,086 :    name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:52,090 :    name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "2022-11-02 07:57:52,093 :    name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2022-11-02 07:57:52,096 :    name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "2022-11-02 07:57:52,099 :    name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,)\n",
            "2022-11-02 07:57:52,102 :    name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768)\n",
            "2022-11-02 07:57:52,105 :    name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:52,108 :    name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,)\n",
            "2022-11-02 07:57:52,111 :    name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2022-11-02 07:57:52,113 :    name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:52,116 :    name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:52,119 :    name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:52,123 :    name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:52,126 :    name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:52,128 :    name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:52,131 :    name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:52,133 :    name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:52,137 :    name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "2022-11-02 07:57:52,140 :    name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2022-11-02 07:57:52,143 :    name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "2022-11-02 07:57:52,146 :    name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,)\n",
            "2022-11-02 07:57:52,150 :    name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768)\n",
            "2022-11-02 07:57:52,154 :    name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:52,157 :    name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,)\n",
            "2022-11-02 07:57:52,160 :    name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2022-11-02 07:57:52,162 :    name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:52,165 :    name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:52,169 :    name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:52,171 :    name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:52,174 :    name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:52,176 :    name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:52,179 :    name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:52,182 :    name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:52,184 :    name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "2022-11-02 07:57:52,186 :    name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2022-11-02 07:57:52,189 :    name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "2022-11-02 07:57:52,192 :    name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,)\n",
            "2022-11-02 07:57:52,195 :    name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768)\n",
            "2022-11-02 07:57:52,198 :    name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:52,201 :    name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,)\n",
            "2022-11-02 07:57:52,205 :    name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2022-11-02 07:57:52,208 :    name = bert/pooler/dense/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/pooler/dense/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:52,210 :    name = bert/pooler/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/pooler/dense/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:52,213 :    name = cls/predictions/transform/dense/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = cls/predictions/transform/dense/kernel:0, shape = (768, 768)\n",
            "2022-11-02 07:57:52,217 :    name = cls/predictions/transform/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = cls/predictions/transform/dense/bias:0, shape = (768,)\n",
            "2022-11-02 07:57:52,219 :    name = cls/predictions/transform/LayerNorm/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = cls/predictions/transform/LayerNorm/beta:0, shape = (768,)\n",
            "2022-11-02 07:57:52,222 :    name = cls/predictions/transform/LayerNorm/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = cls/predictions/transform/LayerNorm/gamma:0, shape = (768,)\n",
            "2022-11-02 07:57:52,225 :    name = cls/predictions/output_bias:0, shape = (32000,)\n",
            "INFO:tensorflow:  name = cls/predictions/output_bias:0, shape = (32000,)\n",
            "2022-11-02 07:57:52,228 :    name = cls/seq_relationship/output_weights:0, shape = (2, 768)\n",
            "INFO:tensorflow:  name = cls/seq_relationship/output_weights:0, shape = (2, 768)\n",
            "2022-11-02 07:57:52,231 :    name = cls/seq_relationship/output_bias:0, shape = (2,)\n",
            "INFO:tensorflow:  name = cls/seq_relationship/output_bias:0, shape = (2,)\n",
            "2022-11-02 07:58:04,103 :  Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "2022-11-02 07:58:04,501 :  Done calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "2022-11-02 07:58:04,504 :  TPU job name worker\n",
            "INFO:tensorflow:TPU job name worker\n",
            "2022-11-02 07:58:05,944 :  Graph was finalized.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "2022-11-02 07:58:12,479 :  Running local_init_op.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "2022-11-02 07:58:13,348 :  Done running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "2022-11-02 07:58:21,915 :  Saving checkpoints for 0 into ./bert_model/model.ckpt.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into ./bert_model/model.ckpt.\n",
            "2022-11-02 07:58:24,741 :  Error recorded from training_loop: From /job:worker/replica:0/task:0:\n",
            "File system scheme '[local]' not implemented (file: './bert_model/model.ckpt-0_temp_b8a8247393784c068d17c27b7b8d2ff7/part-00000-of-00001')\n",
            "\t [[node save/SaveV2 (defined at /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]\n",
            "\n",
            "Original stack trace for 'save/SaveV2':\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py\", line 612, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tornado/ioloop.py\", line 758, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 1233, in inner\n",
            "    self.run()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 1147, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 365, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 326, in wrapper\n",
            "    yielded = next(result)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 326, in wrapper\n",
            "    yielded = next(result)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 545, in execute_request\n",
            "    user_expressions, allow_stdin,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 326, in wrapper\n",
            "    yielded = next(result)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py\", line 306, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2855, in run_cell\n",
            "    raw_cell, store_history, silent, shell_futures)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2881, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3058, in run_cell_async\n",
            "    interactivity=interactivity, compiler=compiler, result=result)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3249, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-80-fc1a0ca3c649>\", line 1, in <module>\n",
            "    estimator.train(input_fn=train_input_fn, max_steps=TRAIN_STEPS)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 3030, in train\n",
            "    saving_listeners=saving_listeners)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 370, in train\n",
            "    loss = self._train_model(input_fn, hooks, saving_listeners)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1161, in _train_model\n",
            "    return self._train_model_default(input_fn, hooks, saving_listeners)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1195, in _train_model_default\n",
            "    saving_listeners)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1490, in _train_with_estimator_spec\n",
            "    log_step_count_steps=log_step_count_steps) as mon_sess:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 584, in MonitoredTrainingSession\n",
            "    stop_grace_period_secs=stop_grace_period_secs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 1014, in __init__\n",
            "    stop_grace_period_secs=stop_grace_period_secs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 725, in __init__\n",
            "    self._sess = _RecoverableSession(self._coordinated_creator)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 1207, in __init__\n",
            "    _WrappedSession.__init__(self, self._create_session())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 1212, in _create_session\n",
            "    return self._sess_creator.create_session()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 878, in create_session\n",
            "    self.tf_sess = self._session_creator.create_session()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 638, in create_session\n",
            "    self._scaffold.finalize()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 237, in finalize\n",
            "    self._saver.build()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/saver.py\", line 840, in build\n",
            "    self._build(self._filename, build_save=True, build_restore=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/saver.py\", line 878, in _build\n",
            "    build_restore=build_restore)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/saver.py\", line 499, in _build_internal\n",
            "    save_tensor = self._AddShardedSaveOps(filename_tensor, per_device)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/saver.py\", line 291, in _AddShardedSaveOps\n",
            "    return self._AddShardedSaveOpsForV2(filename_tensor, per_device)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/saver.py\", line 265, in _AddShardedSaveOpsForV2\n",
            "    sharded_saves.append(self._AddSaveOps(sharded_filename, saveables))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/saver.py\", line 206, in _AddSaveOps\n",
            "    save = self.save_op(filename_tensor, saveables)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/saver.py\", line 122, in save_op\n",
            "    tensors)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/gen_io_ops.py\", line 1946, in save_v2\n",
            "    name=name)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n",
            "    op_def=op_def)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n",
            "    attrs, op_def, compute_device)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n",
            "    op_def=op_def)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n",
            "    self._traceback = tf_stack.extract_stack()\n",
            "\n",
            "ERROR:tensorflow:Error recorded from training_loop: From /job:worker/replica:0/task:0:\n",
            "File system scheme '[local]' not implemented (file: './bert_model/model.ckpt-0_temp_b8a8247393784c068d17c27b7b8d2ff7/part-00000-of-00001')\n",
            "\t [[node save/SaveV2 (defined at /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]\n",
            "\n",
            "Original stack trace for 'save/SaveV2':\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py\", line 612, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tornado/ioloop.py\", line 758, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 1233, in inner\n",
            "    self.run()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 1147, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 365, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 326, in wrapper\n",
            "    yielded = next(result)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 326, in wrapper\n",
            "    yielded = next(result)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 545, in execute_request\n",
            "    user_expressions, allow_stdin,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 326, in wrapper\n",
            "    yielded = next(result)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py\", line 306, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2855, in run_cell\n",
            "    raw_cell, store_history, silent, shell_futures)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2881, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3058, in run_cell_async\n",
            "    interactivity=interactivity, compiler=compiler, result=result)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3249, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-80-fc1a0ca3c649>\", line 1, in <module>\n",
            "    estimator.train(input_fn=train_input_fn, max_steps=TRAIN_STEPS)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 3030, in train\n",
            "    saving_listeners=saving_listeners)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 370, in train\n",
            "    loss = self._train_model(input_fn, hooks, saving_listeners)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1161, in _train_model\n",
            "    return self._train_model_default(input_fn, hooks, saving_listeners)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1195, in _train_model_default\n",
            "    saving_listeners)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1490, in _train_with_estimator_spec\n",
            "    log_step_count_steps=log_step_count_steps) as mon_sess:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 584, in MonitoredTrainingSession\n",
            "    stop_grace_period_secs=stop_grace_period_secs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 1014, in __init__\n",
            "    stop_grace_period_secs=stop_grace_period_secs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 725, in __init__\n",
            "    self._sess = _RecoverableSession(self._coordinated_creator)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 1207, in __init__\n",
            "    _WrappedSession.__init__(self, self._create_session())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 1212, in _create_session\n",
            "    return self._sess_creator.create_session()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 878, in create_session\n",
            "    self.tf_sess = self._session_creator.create_session()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 638, in create_session\n",
            "    self._scaffold.finalize()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 237, in finalize\n",
            "    self._saver.build()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/saver.py\", line 840, in build\n",
            "    self._build(self._filename, build_save=True, build_restore=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/saver.py\", line 878, in _build\n",
            "    build_restore=build_restore)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/saver.py\", line 499, in _build_internal\n",
            "    save_tensor = self._AddShardedSaveOps(filename_tensor, per_device)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/saver.py\", line 291, in _AddShardedSaveOps\n",
            "    return self._AddShardedSaveOpsForV2(filename_tensor, per_device)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/saver.py\", line 265, in _AddShardedSaveOpsForV2\n",
            "    sharded_saves.append(self._AddSaveOps(sharded_filename, saveables))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/saver.py\", line 206, in _AddSaveOps\n",
            "    save = self.save_op(filename_tensor, saveables)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/saver.py\", line 122, in save_op\n",
            "    tensors)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/gen_io_ops.py\", line 1946, in save_v2\n",
            "    name=name)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n",
            "    op_def=op_def)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n",
            "    attrs, op_def, compute_device)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n",
            "    op_def=op_def)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n",
            "    self._traceback = tf_stack.extract_stack()\n",
            "\n",
            "2022-11-02 07:58:24,749 :  training_loop marked as finished\n",
            "INFO:tensorflow:training_loop marked as finished\n",
            "2022-11-02 07:58:24,751 :  Reraising captured error\n",
            "WARNING:tensorflow:Reraising captured error\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnimplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnimplementedError\u001b[0m: From /job:worker/replica:0/task:0:\nFile system scheme '[local]' not implemented (file: './bert_model/model.ckpt-0_temp_b8a8247393784c068d17c27b7b8d2ff7/part-00000-of-00001')\n\t [[{{node save/SaveV2}}]]",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-fc1a0ca3c649>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTRAIN_STEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m   3033\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3034\u001b[0m       \u001b[0mrendezvous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training_loop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3035\u001b[0;31m       \u001b[0mrendezvous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3037\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py\u001b[0m in \u001b[0;36mraise_errors\u001b[0;34m(self, timeout_sec)\u001b[0m\n\u001b[1;32m    134\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Reraising captured error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkept_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m   3028\u001b[0m           \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3029\u001b[0m           \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3030\u001b[0;31m           saving_listeners=saving_listeners)\n\u001b[0m\u001b[1;32m   3031\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3032\u001b[0m       \u001b[0mrendezvous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training_loop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1159\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1193\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n\u001b[1;32m   1194\u001b[0m                                              \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m                                              saving_listeners)\n\u001b[0m\u001b[1;32m   1196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_with_estimator_spec\u001b[0;34m(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\u001b[0m\n\u001b[1;32m   1488\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1489\u001b[0m         \u001b[0mmax_wait_secs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession_creation_timeout_secs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1490\u001b[0;31m         log_step_count_steps=log_step_count_steps) as mon_sess:\n\u001b[0m\u001b[1;32m   1491\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1492\u001b[0m       \u001b[0many_step_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/monitored_session.py\u001b[0m in \u001b[0;36mMonitoredTrainingSession\u001b[0;34m(master, is_chief, checkpoint_dir, scaffold, hooks, chief_only_hooks, save_checkpoint_secs, save_summaries_steps, save_summaries_secs, config, stop_grace_period_secs, log_step_count_steps, max_wait_secs, save_checkpoint_steps, summary_dir)\u001b[0m\n\u001b[1;32m    582\u001b[0m       \u001b[0msession_creator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession_creator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m       \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_hooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m       stop_grace_period_secs=stop_grace_period_secs)\n\u001b[0m\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session_creator, hooks, stop_grace_period_secs)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0mshould_recover\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m         stop_grace_period_secs=stop_grace_period_secs)\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session_creator, hooks, should_recover, stop_grace_period_secs)\u001b[0m\n\u001b[1;32m    723\u001b[0m         stop_grace_period_secs=stop_grace_period_secs)\n\u001b[1;32m    724\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshould_recover\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_RecoverableSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinated_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinated_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sess_creator)\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \"\"\"\n\u001b[1;32m   1206\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess_creator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess_creator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1207\u001b[0;31m     \u001b[0m_WrappedSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/monitored_session.py\u001b[0m in \u001b[0;36m_create_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1210\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1212\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1213\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m         logging.info(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/monitored_session.py\u001b[0m in \u001b[0;36mcreate_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    883\u001b[0m       \u001b[0;31m# Inform the hooks that a new session has been created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mhook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_create_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_sess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m       return _CoordinatedSession(\n\u001b[1;32m    887\u001b[0m           \u001b[0m_HookedSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_sess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/basic_session_run_hooks.py\u001b[0m in \u001b[0;36mafter_create_session\u001b[0;34m(self, session, coord)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_summary_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;31m# The checkpoint saved here is the state at step \"global_step\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_last_triggered_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/basic_session_run_hooks.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self, session, step)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_saver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     self._summary_writer.add_session_log(\n\u001b[1;32m    613\u001b[0m         SessionLog(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs, save_debug_info)\u001b[0m\n\u001b[1;32m   1174\u001b[0m           model_checkpoint_path = sess.run(\n\u001b[1;32m   1175\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m               {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0mmodel_checkpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                     \u001b[0;34m'\\nsession_config.graph_options.rewrite_options.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[0;32m-> 1384\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnimplementedError\u001b[0m: From /job:worker/replica:0/task:0:\nFile system scheme '[local]' not implemented (file: './bert_model/model.ckpt-0_temp_b8a8247393784c068d17c27b7b8d2ff7/part-00000-of-00001')\n\t [[node save/SaveV2 (defined at /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]\n\nOriginal stack trace for 'save/SaveV2':\n  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py\", line 612, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 545, in execute_request\n    user_expressions, allow_stdin,\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py\", line 306, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2855, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2881, in _run_cell\n    return runner(coro)\n  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3058, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3249, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-80-fc1a0ca3c649>\", line 1, in <module>\n    estimator.train(input_fn=train_input_fn, max_steps=TRAIN_STEPS)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 3030, in train\n    saving_listeners=saving_listeners)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 370, in train\n    loss = self._train_model(input_fn, hooks, saving_listeners)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1161, in _train_model\n    return self._train_model_default(input_fn, hooks, saving_listeners)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1195, in _train_model_default\n    saving_listeners)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1490, in _train_with_estimator_spec\n    log_step_count_steps=log_step_count_steps) as mon_sess:\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 584, in MonitoredTrainingSession\n    stop_grace_period_secs=stop_grace_period_secs)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 1014, in __init__\n    stop_grace_period_secs=stop_grace_period_secs)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 725, in __init__\n    self._sess = _RecoverableSession(self._coordinated_creator)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 1207, in __init__\n    _WrappedSession.__init__(self, self._create_session())\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 1212, in _create_session\n    return self._sess_creator.create_session()\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 878, in create_session\n    self.tf_sess = self._session_creator.create_session()\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 638, in create_session\n    self._scaffold.finalize()\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 237, in finalize\n    self._saver.build()\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/saver.py\", line 840, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/saver.py\", line 878, in _build\n    build_restore=build_restore)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/saver.py\", line 499, in _build_internal\n    save_tensor = self._AddShardedSaveOps(filename_tensor, per_device)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/saver.py\", line 291, in _AddShardedSaveOps\n    return self._AddShardedSaveOpsForV2(filename_tensor, per_device)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/saver.py\", line 265, in _AddShardedSaveOpsForV2\n    sharded_saves.append(self._AddSaveOps(sharded_filename, saveables))\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/saver.py\", line 206, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/saver.py\", line 122, in save_op\n    tensors)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/gen_io_ops.py\", line 1946, in save_v2\n    name=name)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_OeXod-fHMT"
      },
      "source": [
        "Training the model with the default parameters for 1 million steps will take ~53 hours. \n",
        "\n",
        "In case the kernel is restarted, you may always continue training from the latest checkpoint. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77ZIAAATfzdF"
      },
      "source": [
        "This concludes the guide to pre-training BERT from scratch on a cloud TPU. However, the really fun stuff is still  to come, so stay tuned.\n",
        "\n",
        "Keep learning!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiFH_9Lbze5f"
      },
      "source": [
        "!gsutil cp gs://hermes_assets/russian_uncased_L-12_H-768_A-12.zip gs://bert_resourses/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PP7_2pKWzfiZ"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}